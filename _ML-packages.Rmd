---
title: "ML Packages"
output:
  pdf_document: default
  html_document: default
date: "2023-12-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Makine öğrenimi alanındaki kütüphaneler irdelenmiştir.

# abess
En iyi alt küme seçimi problemini çözmek için son derece etkili araç seti. Paket, doğrusal model için polinom zamanlarında tam destek kurtarmayı ve küresel olarak en uygun çözümü garanti etmek için yeni bir sıralama ve birleştirme tekniğinden yararlanan şekilde tasarlanan algoritmaları uygular ve genelleştirir. Ayrıca lojistik regresyon, Poisson regresyon, Cox orantılı risk modeli, Gama regresyon, çoklu yanıt regresyon, çok terimli lojistik regresyon, sıralı regresyon, (sıralı) temel bileşen analizi ve sağlam temel bileşen analizi için en iyi alt küme seçimini destekler. Grup seçiminin en iyi alt kümesi ve kesin bağımsızlık taraması gibi diğer değerli özellikler de sağlanmaktadır.
```{r}
library('abess')

# Adaptive best subset selection (for generalized linear model)
abess(x,y,family = c("gaussian", "binomial", "poisson", "cox", "mgaussian",
                     "multinomial","gamma", "ordinal"),
      tune.path = c("sequence", "gsection"),tune.type = c("gic", "ebic", "bic", "aic", "cv"),
      weight = NULL,normalize = NULL,fit.intercept = TRUE,beta.low = -.Machine$double.xmax,
      beta.high = .Machine$double.xmax,c.max = 2,support.size = NULL,gs.range = NULL,
      lambda = 0,always.include = NULL,group.index = NULL,init.active.set = NULL,
      splicing.type = 2,max.splicing.iter = 20,screening.num = NULL,important.search = NULL,
      warm.start = TRUE,nfolds = 5,foldid = NULL,cov.update = FALSE,
      newton = c("exact", "approx"),newton.thresh = 1e-06,max.newton.iter = NULL,
      early.stop = FALSE,ic.scale = 1,num.threads = 0,seed = 1,...)

# Adaptive best subset selection for principal component analysis
abesspca(x,type = c("predictor", "gram"),sparse.type = c("fpc", "kpc"),cor = FALSE,
         kpc.num = NULL,support.size = NULL,gs.range = NULL,
         tune.path = c("sequence", "gsection"),tune.type = c("gic", "aic", "bic", "ebic", "cv"),
         nfolds = 5,foldid = NULL,ic.scale = 1,c.max = NULL,always.include = NULL,
         group.index = NULL,screening.num = NULL,splicing.type = 1,
         max.splicing.iter = 20,warm.start = TRUE,num.threads = 0,...)

#Adaptive best subset selection for robust principal component analysis
abessrpca(x,rank,support.size = NULL,tune.path = c("sequence", "gsection"),
          gs.range = NULL,tune.type = c("gic", "aic", "bic", "ebic"),
          ic.scale = 1,lambda = 0,always.include = NULL,group.index = NULL,
          c.max = NULL,splicing.type = 2,max.splicing.iter = 1,warm.start = TRUE,
          important.search = NULL,max.newton.iter = 1,newton.thresh = 0.001,
          num.threads = 0,seed = 1,...)

predict()
```


# e1071
Gizli sınıf analizine yönelik işlevler, kısa zamanlı Fourier dönüşümü, bulanık kümeleme, destek vektör makineleri, en kısa yol hesaplaması, torbalı kümeleme, saf Bayes sınıflandırıcısı, genelleştirilmiş k-en yakın komşu.
```{r}
library('e1071')

#gknn is an implementation of the k-nearest neighbours algorithm making use of
#general distance measures.
gknn(formula, data = NULL, ..., subset, na.action = na.pass, scale = TRUE)
gknn(x, y, k = 1, method = NULL,scale = TRUE, use_all = TRUE,FUN = mean, ...)

#Computes the conditional a-posterior probabilities of a categorical class 
#variable given independent predictor variables using the Bayes rule.
naiveBayes(formula, data, laplace = 0, ..., subset, na.action = na.pass)
naiveBayes(x, y, laplace = 0, ...)

#svm is used to train a support vector machine.
svm(formula, data = NULL, ..., subset, na.action =na.omit, scale = TRUE)
svm(x, y = NULL, scale = TRUE, type = NULL, kernel ="radial", degree = 3,
    gamma = if (is.vector(x)) 1 else 1 / ncol(x), coef0 = 0, cost = 1, nu = 0.5,
    class.weights = NULL, cachesize = 40, tolerance = 0.001, epsilon = 0.1,
    shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,...,
    subset, na.action = na.omit)

#bclust Bagged Clustering
bclust(x, centers=2, iter.base=10, minsize=0,dist.method="euclidean",
       hclust.method="average", base.method="kmeans",base.centers=20,
       verbose=TRUE,final.kmeans=FALSE, docmdscale=FALSE,resample=TRUE,
       weights=NULL, maxcluster=base.centers, ...)
hclust.bclust(object, x, centers, dist.method=object$dist.method,
              hclust.method=object$hclust.method, final.kmeans=FALSE,
              docmdscale = FALSE, maxcluster=object$maxcluster)
#cmeans Fuzzy C-Means Clustering
cmeans(x, centers, iter.max = 100, verbose = FALSE,dist = "euclidean",
       method = "cmeans", m = 2,rate.par = NULL, weights = 1, control = list())

#cshell Fuzzy C-Shell Clustering
cshell(x, centers, iter.max=100, verbose=FALSE, dist="euclidean",method="cshell",
       m=2, radius = NULL)

predict()
```

# gbm
Freund ve Schapire'nin AdaBoost algoritmasının ve Friedman'ın gradyan artırma makinesinin uzantılarının bir uygulaması. En küçük kareler, mutlak kayıp, t-dağılımı kaybı, niceliksel regresyon, lojistik, çok terimli lojistik, Poisson, Cox orantılı tehlikeler kısmi olasılığı, AdaBoost üstel kaybı, Huberleştirilmiş menteşe kaybı ve Sıralamayı Öğrenme ölçümleri için regresyon yöntemlerini içerir
```{r}
library('gbm')
#Fits generalized boosted regression models
gbm(formula = formula(data),distribution = "bernoulli",data = list(),weights,
    var.monotone = NULL,n.trees = 100,interaction.depth = 1,n.minobsinnode = 10,
    shrinkage = 0.1,bag.fraction = 0.5,train.fraction = 1,cv.folds = 0,
    keep.data = TRUE,verbose = FALSE,class.stratify.cv = NULL,n.cores = NULL)

methods<-c("OOB","test","cv")
for (i in methods) {
  best.iter <- gbm.perf(gbm1, method = i)
  print(i," :",best.iter)
}

#Bir GBM nesnesi için en uygun yükseltme yineleme sayısını tahmin eder ve isteğe bağlı olarak çeşitli performans önlemlerini çizer
gbm.perf(object, plot.it = TRUE, oobag.curve = FALSE, overlay = TRUE, method)

predict(object, newdata, n.trees, type = "link", single.tree = FALSE, ...)
```
DISTRIBUTION:"gaussian" (squared error), "laplace" (absolute loss), "tdist" (t-distribution loss), "bernoulli" (logistic regression for 0-1 outcomes), "huberized" (huberized hinge loss for 0-1 outcomes), classes,
"adaboost" (the AdaBoost exponential loss for 0-1 outcomes), "poisson" (count outcomes), "coxph" (right censored observations), "quantile", or "pairwise" (ranking measure using the LambdaMart algorithm).

# kernlab
Sınıflandırma, regresyon, kümeleme, yenilik tespiti, niceliksel regresyon ve boyutluluğun azaltılması için çekirdek tabanlı makine öğrenimi yöntemleri. Diğer yöntemlerin yanı sıra 'kernlab', Destek Vektör Makinelerini, Spektral Kümelemeyi, Çekirdek PCA'yı, Gauss Süreçlerini ve bir QP çözücüyü içerir.
```{r}
library('kernlab')

K <- as.kernelMatrix(x)

#Gausspr, sınıflandırma ve regresyon için Gauss süreçlerinin bir uygulamasıdır.
gausspr(x, y, scaled = TRUE, type= NULL, kernel="rbfdot", #type->"classification" or "regression"
  kpar="automatic", var=1, variance.model = FALSE, tol=0.0005,
  cross=0, fit=TRUE, ... , subset, na.action = na.omit)

#Çekirdek Özellik Analizi algoritması, muhtemelen yüksek boyutlu veri kümelerinden yapı çıkarmaya yönelik bir algoritmadır. Kpca'ya benzer şekilde veriler için yeni bir temel bulundu. Veriler daha sonra yeni temelde yansıtılabilir
kha(x, kernel = "rbfdot", kpar = list(sigma = 0.1), features = 5,
  eta = 0.005, th = 1e-4, maxiter = 10000, verbose = FALSE,
  na.action = na.omit, ...)

#Ünlü k-ortalamalar algoritmasının ağırlıklı çekirdek versiyonu.
kkmeans(x, centers, kernel = "rbfdot", kpar = "automatic",
  alg="kkmeans", p=1, na.action = na.omit, ...)

#Çekirdek Maksimum Ortalama Farklılığı kmmd, parametrik olmayan bir dağıtım testi gerçekleştirir.
kmmd(x, y, kernel="rbfdot",kpar="automatic", alpha = 0.05,
asymptotic = FALSE, replace = TRUE, ntimes = 150, frac = 1, ...)

#Çekirdek Temel Bileşenler Analizi, temel bileşen analizinin doğrusal olmayan bir şeklidir.
kpca(x, kernel = "rbfdot", kpar = list(sigma = 0.1),
  features = 0, th = 1e-4, na.action = na.omit, ...)

#Çekirdek Kantil Regresyon algoritması kqr, parametrik olmayan Kantil Regresyon gerçekleştirir.
kqr(x, y, scaled = TRUE, tau = 0.5, C = 0.1, kernel = "rbfdot",
  kpar = "automatic", reduced = FALSE, rank = dim(x)[1]/6,
  fit = TRUE, cross = 0, na.action = na.omit)

#ksvm, iyi bilinen C-svc, nu-svc, (sınıflandırma) tek sınıf-svc (yenilik) eps-svr, nu-svr (regresyon) formülasyonlarının yanı sıra yerel çok sınıflı sınıflandırma formülasyonlarını ve sınırlı kısıtlama SVM formülasyonlarını destekler.
ksvm(x, y = NULL, scaled = TRUE, type = NULL,kernel ="rbfdot", kpar = "automatic",
     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,class.weights = NULL,
     cross = 0, fit = TRUE, cache = 40,tol = 0.001, shrinking = TRUE, ...,
     subset, na.action = na.omit)

#lssvm, csi işlevi tarafından hesaplanan çekirdek matrisinin ayrıştırılmasını kullanan En Küçük Kareler SVM'nin küçültülmüş bir sürümünü içerir.
lssvm(x, y, scaled = TRUE, kernel = "rbfdot", kpar = "automatic",type = NULL,
      tau = 0.01, reduced = TRUE, tol = 0.0001,rank = floor(dim(x)[1]/3),
      delta = 40, cross = 0, fit = TRUE,..., subset, na.action = na.omit)

#İlgililik Vektör Makinesi, aynı fonksiyonel formun destek vektör makinesine regresyonu ve sınıflandırılması için bir Bayes modelidir. rvm işlevi şu anda yalnızca regresyonu desteklemektedir.
rvm(x, y, type="regression",kernel="rbfdot", kpar="automatic",alpha= ncol(as.matrix(x)),
    var=0.1, var.fix=FALSE, iterations=100,verbosity = 0, tol = .Machine$double.eps,
    minmaxdiff = 1e-3,cross = 0, fit = TRUE, ... , subset, na.action = na.omit)

#Bir spektral kümeleme algoritması. Kümeleme, verilerin bir ilgi matrisinin özvektörlerinin alt uzayına yerleştirilmesiyle gerçekleştirilir.
specc(x, centers,kernel = "rbfdot", kpar = "automatic",nystrom.red = FALSE,
      nystrom.sample = dim(x)[1]/6,iterations = 200, mod.sample = 0.75, na.action = na.omit, ...)

predict(mdl,ndata,...)
```
KERNEL
• rbfdot Radial Basis kernel function "Gaussian"
• polydot Polynomial kernel function
• vanilladot Linear kernel function
• tanhdot Hyperbolic tangent kernel function
• laplacedot Laplacian kernel function
• besseldot Bessel kernel function
• anovadot ANOVA RBF kernel function
• splinedot Spline kernel

KPAR
• sigma inverse kernel width for the Radial Basis kernel function "rbfdot"
and the Laplacian kernel "laplacedot".
• degree, scale, offset for the Polynomial kernel "polydot"
• scale, offset for the Hyperbolic tangent kernel function "tanhdot"
• sigma, order, degree for the Bessel kernel "besseldot".
• sigma, degree for the ANOVA kernel "anovadot".


# mboost
Genelleştirilmiş doğrusal, toplamsal ve etkileşim modellerini potansiyel olarak yüksek boyutlu verilere uydurmak için temel öğreniciler olarak bileşen bazında (cezalandırılmış) en küçük kareler tahminlerini veya regresyon ağaçlarını kullanan genel risk fonksiyonlarını optimize etmeye yönelik fonksiyonel gradyan iniş algoritması (artırma).
```{r}
library('mboost')

#gamboost for boosted (generalized) additive models,
mboost(formula, data = list(), na.action = na.omit, weights = NULL,offset = NULL,
       family = Gaussian(), control = boost_control(),oobweights = NULL,
       baselearner = c("bbs", "bols", "btree", "bss", "bns"),
       ...)

gamboost(formula, data = list(), na.action = na.omit, weights = NULL,offset = NULL,
         family = Gaussian(), control = boost_control(),oobweights = NULL,
         baselearner = c("bbs", "bols", "btree", "bss", "bns"),dfbase = 4, ...)

varimp(object, ...)
plot(x, percent = TRUE, type = c("variable", "blearner"),
     blorder = c("importance", "alphabetical", "rev_alphabetical", "formula"),
     nbars = 10L, maxchar = 20L, xlab = NULL, ylab = NULL, xlim, auto.key, ...)

#glmboost for boosted linear models and
glmboost(x, y, center = TRUE, weights = NULL,offset = NULL, family = Gaussian(),
         na.action = na.pass, control = boost_control(), oobweights = NULL, ...)

#blackboost for boosted trees.
blackboost(formula, data = list(),weights = NULL, na.action = na.pass,offset = NULL,
           family = Gaussian(),control = boost_control(),oobweights = NULL,
           tree_controls = partykit::ctree_control(
             teststat = "quad",testtype = "Teststatistic",mincriterion = 0,
             minsplit = 10,minbucket = 4,maxdepth = 2,saveinfo = FALSE),
           ...)

print()
coef()
predict()
```


# randomForest
Breiman'a dayalı, rastgele girdiler kullanan bir ağaç ormanına dayalı sınıflandırma ve regresyon.
```{r}
library('randomForest')

randomForest(x, y=NULL, xtest=NULL, ytest=NULL, ntree=500,
             mtry=if (!is.null(y) && !is.factor(y))
               max(floor(ncol(x)/3), 1) else floor(sqrt(ncol(x))),
             weights=NULL,replace=TRUE, classwt=NULL, cutoff, strata,
             sampsize = if (replace) nrow(x) else ceiling(.632*nrow(x)),
             nodesize = if (!is.null(y) && !is.factor(y)) 5 else 1,maxnodes = NULL,
             importance=FALSE, localImp=FALSE, nPerm=1,proximity, oob.prox=proximity,
             norm.votes=TRUE, do.trace=FALSE,keep.forest=!is.null(y) && is.null(xtest),
             corr.bias=FALSE,keep.inbag=FALSE, ...)

predict(object, newdata, type="response",norm.votes=TRUE, predict.all=FALSE,
        proximity=FALSE, nodes=FALSE,cutoff, ...)

# shows the cross-validated prediction performance
rfcv(trainx, trainy, cv.fold=5, scale="log", step=0.5,
     mtry=function(p) max(1, floor(sqrt(p))), recursive=FALSE, ...)

getTree(rfobj, k=1, labelVar=FALSE)#extract the structure of a tree from a randomForest object
importance(x, type=NULL, class=NULL, scale=TRUE, ...)# extractor function for variable importance measures
```


# rpart
Sınıflandırma, regresyon ve hayatta kalma ağaçları için yinelemeli bölümleme
```{r}
library('rpart')

rpart(formula, data, weights, subset, na.action = na.rpart, method,
      model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)

predict(object, newdata,type = c("vector", "prob", "class", "matrix"),
        na.action = na.pass, ...)
```


# rms
ikili veya sıralı regresyon modelleri, Cox regresyonu, hızlandırılmış başarısızlık süresi modelleri, sıradan doğrusal modeller, Buckley-James modeli, seri veya uzamsal korelasyon için genelleştirilmiş en küçük kareler ile çalışmak üzere yazılmıştır.
```{r}
library('rms')

#bj, Buckley-James dağıtımından bağımsız en küçük kareler çoklu regresyon modeline muhtemelen sağdan sansürlü bir yanıt değişkenine uyar
bj(formula, data=environment(formula), subset, na.action=na.delete,link="log",
   control, method='fit', x=FALSE, y=FALSE,time.inc)

#rms Version of glm
Glm(formula,family = gaussian,data = environment(formula),weights,subset,
    na.action = na.delete,start = NULL,offset = NULL,control = glm.control(...),
    model = TRUE,method = "glm.fit",x = FALSE,y = TRUE,contrasts = NULL,...)

#Fit Linear Model Using Generalized Least Squares
Gls(model, data, correlation, weights, subset, method, na.action=na.omit,
    control, verbose, B=0, dupCluster=FALSE, pr=FALSE, x=FALSE)

#Logistic Regression Model
lrm(formula, data=environment(formula),subset, na.action=na.delete, method="lrm.fit",
    model=FALSE, x=FALSE, y=FALSE, linear.predictors=TRUE, se.fit=FALSE,penalty=0,
    penalty.matrix, tol=1e-7,strata.penalty=0, var.penalty=c('simple','sandwich'),
    weights, normwt, scale=FALSE, ...)

#Parametric Survival Model
psm(formula,data=environment(formula), weights,subset, na.action=na.delete,
    dist="weibull",init=NULL, scale=0,control=survreg.control(),parms=NULL,
    model=FALSE, x=FALSE, y=TRUE, time.inc, ...)

#Cox Proportional Hazards Model and Extensions
cph(formula = formula(data), data=environment(formula),weights, subset,
    na.action=na.delete,method=c("efron","breslow","exact","model.frame","model.matrix"),
    singular.ok=FALSE, robust=FALSE,model=FALSE, x=FALSE, y=FALSE, se.fit=FALSE,
    linear.predictors=TRUE, residuals=TRUE, nonames=FALSE,eps=1e-4, init, iter.max=10,
    tol=1e-9, surv=FALSE, time.inc,type=NULL, vartype=NULL, debug=FALSE, ...)

#Cox Survival Estimates
survest(fit, newdata, linear.predictors, x, times,fun, loglog=FALSE, conf.int=0.95,
        type, vartype,conf.type=c("log", "log-log", "plain", "none"), se.fit=TRUE,
        what=c('survival','parallel'),individual=FALSE, ...)

#Cox Predicted Survival
survfit(formula, newdata, se.fit=TRUE, conf.int=0.95,individual=FALSE, type=NULL,
        vartype=NULL,conf.type=c('log', "log-log", "plain", "none"), id, ...)

Predict(object, ..., fun=NULL, funint=TRUE,type = c("predictions", "model.frame", "x"),
        np = 200, conf.int = 0.95,conf.type = c("mean", "individual","simultaneous"),
        usebootcoef=TRUE, boot.type=c("percentile", "bca", "basic"),
        posterior.summary=c('mean', 'median', 'mode'),adj.zero = FALSE, ref.zero = FALSE,
        kint=NULL, ycut=NULL, time = NULL, loglog = FALSE, digits=4, name,factors=NULL, offset=NULL)
```

# CatBoost
karar ağaçlarında gradyan artırma kullanan bir makine öğrenimi algoritmasıdır.
```{r}
library(catboost)
catboost.load_pool(data,
                   label = NULL,
                   cat_features = NULL,
                   column_description = NULL,
                   pairs = NULL,
                   delimiter = "\t",
                   has_header = FALSE,
                   weight = NULL,
                   group_id = NULL,
                   group_weight = NULL,
                   subgroup_id = NULL,
                   pairs_weight = NULL,
                   baseline = NULL,
                   feature_names = NULL,
                   thread_count = -1)
catboost.save_pool(data,
                   label = NULL,
                   weight = NULL,
                   baseline = NULL,
                   pool_path = "data.pool",
                   cd_path = "cd.pool")
catboost.train(learn_pool,
               test_pool = NULL,
               params = list())
catboost.predict(model,
                 pool,
                 verbose=FALSE,
                 prediction_type=None,
                 ntree_start=0,
                 ntree_end=0,
                 thread_count=-1) #(the number of threads is equal to the number of processor cores)
catboost.get_feature_importance(model,
                                pool = NULL,
                                type = 'FeatureImportance',
                                thread_count = -1)
catboost.get_object_importance(model,
                               pool,
                               train_pool,
                               top_size = -1,
                               type = 'Average',
                               update_method = 'SinglePoint',
                               thread_count = -1)

#------------------------------------


library(caret)#fit control and grid için

titanic_train=read.csv("./ML_GlobalAI/DecisionTrees_titanic.csv", sep = ",")
x<-titanic_train[,-1]
y<-titanic_train[,1]

fit_control <- trainControl(method = "cv",
                            number = 4,
                            classProbs = TRUE)

grid <- expand.grid(depth = c(4, 6, 8),
                    learning_rate = 0.1,
                    iterations = 100,
                    l2_leaf_reg = 1e-3,
                    rsm = 0.95,
                    border_count = 64)

report <- train(x, as.factor(make.names(y)),
                method = catboost.caret,
                logging_level = 'Verbose', preProc = NULL,
                tuneGrid = grid, trControl = fit_control)

print(report)

importance <- varImp(report, scale = FALSE)
print(importance)
```

# nlme
Gauss doğrusal ve doğrusal olmayan karma etki modellerini yerleştirin ve karşılaştırın.
```{r}
library('nlme')
#Bu fonksiyon genelleştirilmiş en küçük kareler kullanan doğrusal bir modele uyar.
gls(model, data, correlation, weights, subset, method, na.action,control, verbose)

#Bu fonksiyon genelleştirilmiş en küçük kareler kullanan doğrusal olmayan bir modele uyar.
gnls(model, data, params, start, correlation, weights, subset,na.action, naPattern,
     control, verbose)

#Doğrusal Karma Efekt Modelleri
lme(fixed, data, random, correlation, weights, subset, method,na.action, control,
    contrasts = NULL, keep.data = TRUE)

#Doğrusal Olmayan Karma Etki Modelleri
nlme(model, data, fixed, random, groups, start, correlation, weights,subset, method,
     na.action, naPattern, control, verbose)



plot()
predict()
qqnorm()
```

# survival
Aşağıdakiler dahil temel hayatta kalma analizi rutinlerini içerir:
Surv nesnelerinin tanımı, Kaplan-Meier ve Aalen-Johansen (çok durumlu) eğrileri, Cox modelleri,
ve parametrik hızlandırılmış arıza süresi modelleri
```{r}
library('survival')

#hayatta kalma nesnesi oluşturun.
Surv(time, time2, event,
     type=c('right', 'left', 'interval', 'counting', 'interval2', 'mstate'),origin=0)

#Aalen'in sansürlenmiş veriler için eklemeli regresyon modeli
aareg(formula, data, weights, subset, na.action,qrtol=1e-07, nmin, dfbeta=FALSE,
      taper=1,test = c('aalen', 'variance', 'nrisk'), cluster,model=FALSE, x=FALSE, y=FALSE)

#Koşullu lojistik regresyon
clogit(formula, data, weights, subset, na.action,
       method=c("exact", "approximate", "efron", "breslow"))

#Orantılı Tehlikeler Regresyon Modeline Uygun
coxph(formula, data=, weights, subset,na.action, init, control,
      ties=c("efron","breslow","exact"),singular.ok=TRUE, robust,model=FALSE,
      x=FALSE, y=TRUE, tt, method=ties,id, cluster, istate, statedata, nocenter=c(-1, 0, 1))

#Parametrik Hayatta Kalma Modeli için Regresyon
survreg(formula, data, weights, subset,na.action, dist="weibull", init=NULL, scale=0,
        control,parms=NULL,model=FALSE, x=FALSE,y=TRUE, robust=FALSE, cluster, score=FALSE, ...)

predict()
```

# mice
Tam Koşullu Belirtim (FCS) kullanılarak çoklu atama MICE algoritması tarafından uygulanmıştır. Sürekli veriler (tahminli ortalama eşleştirme, normal), ikili veriler (lojistik regresyon), sırasız kategorik veriler (çok kategorili lojistik regresyon) ve sıralı kategorik veriler (orantılı oranlar) için yerleşik atama modelleri sağlanır. MICE ayrıca sürekli iki seviyeli verileri (normal model, pan, ikinci seviye değişkenler) atayabilir.
Değişkenler arasındaki tutarlılığı korumak için pasif atama kullanılabilir.
```{r}
library('mice')

#Simülasyon amacıyla eksik verileri oluşturun
ampute(data,prop = 0.5,patterns = NULL,freq = NULL,mech = "MAR",weights = NULL,
       std = TRUE,cont = TRUE,type = NULL,odds = NULL,bycases = TRUE,run = TRUE)

#MICE algoritması sürekli, ikili, sırasız kategorik ve sıralı kategorik verilerin karışımlarını atayabilir.
mice(data,m = 5,method = NULL,predictorMatrix,ignore = NULL,where = NULL,
     blocks,visitSequence = NULL,formulas,blots = NULL,post = NULL,
     defaultMethod = c("pmm", "logreg", "polyreg", "polr"),maxit = 5,
     printFlag = TRUE,seed = NA,data.init = NULL) -> imp
complete(imp)

```
METHOD:
pmm             any Predictive mean matching
midastouch      any Weighted predictive mean matching
sample          any Random sample from observed values
cart            any Classification and regression trees
rf              any Random forest imputations
mean            numeric Unconditional mean imputation
norm            numeric Bayesian linear regression
norm.nob        numeric Linear regression ignoring model error
norm.boot       numeric Linear regression using bootstrap
norm.predict    numeric Linear regression, predicted values
lasso.norm      numeric Lasso linear regression
lasso.select.norm numeric Lasso select + linear regression
quadratic       numeric Imputation of quadratic terms
ri              numeric Random indicator for nonignorable data
logreg          binary Logistic regression
logreg.boot     binary Logistic regression with bootstrap
lasso.logreg    binary Lasso logistic regression
lasso.select.logreg binary Lasso select + logistic regression
polr            ordered Proportional odds model
polyreg         unordered Polytomous logistic regression
lda             unordered Linear discriminant analysis
2l.norm         numeric Level-1 normal heteroscedastic
2l.lmer         numeric Level-1 normal homoscedastic, lmer
2l.pan          numeric Level-1 normal homoscedastic, pan
2l.bin          binary Level-1 logistic, glmer
2lonly.mean     numeric Level-2 class mean
2lonly.norm     numeric Level-2 class normal
2lonly.pmm      any Level-2 class predictive mean matching


# adabag
Applies Multiclass AdaBoost.M1, SAMME and Bagging
```{r}
library('adabag')
#Torbalama algoritmasını bir veri kümesine uygular
bagging(formula, data, mfinal = 100, control, par=FALSE,...)

#AdaBoost.M1 ve SAMME algoritmalarını bir veri kümesine uygular
boosting(formula, data, boos = TRUE, mfinal = 100, coeflearn = 'Breiman',control,...)

predict()
```

# arules
İşlem verilerini ve modellerini (sık öğe kümeleri ve birliktelik kuralları) temsil etmek, değiştirmek ve analiz etmek için altyapı sağlar.
Ayrıca Apriori ve Eclat ilişkilendirme madenciliği algoritmalarının C uygulamalarını sağlar.
```{r}
library('arules')
#Apriori algoritmasını kullanarak sık öğe kümelerini, birliktelik kurallarını veya birliktelik hiper kenarlarını kazın.
apriori(data, parameter = NULL, appearance = NULL, control = NULL, ...)

predict()

#Bir dosyadan işlem verilerini okur ve bir işlem nesnesi oluşturur.
read.transactions(file,format = c("basket", "single"),header = FALSE,sep = "",
cols = NULL,rm.duplicates = FALSE,quote = "\"'",skip = 0,encoding = "unknown")

#Eclat algoritmasıyla sık görülen öğe kümelerini bulun. Bu uygulama, ağırlıklı birliktelik kuralı madenciliği (WARM) uygulamak için optimize edilmiş işlem kimliği listesi birleştirmelerini ve işlem ağırlıklarını kullanır.
weclat(data, parameter = NULL, control = NULL)
```


# BART
Bayesian Toplamalı Regresyon Ağaçları (BART), sürekli, ikili, kategorik ve olaya kadar geçen süre sonuçları için ortak değişkenlerin esnek parametrik olmayan modellemesini sağlar.
```{r}
library('BART')

#AFT BART for time-to-event outcomes
abart(x.train, times, delta,x.test=matrix(0,0,0), K=100,type='abart', ntype=1,
      sparse=FALSE, theta=0, omega=1,a=0.5, b=1, augment=FALSE, rho=NULL,
      xinfo=matrix(0,0,0), usequants=FALSE,rm.const=TRUE,sigest=NA, sigdf=3,
      sigquant=0.90,k=2, power=2, base=0.95,lambda=NA, tau.num=c(NA, 3, 6)[ntype],
      offset=NULL, w=rep(1, length(times)),ntree=c(200L, 50L, 50L)[ntype], numcut=100L,
      ndpost=1000L, nskip=100L,keepevery=c(1L, 10L, 10L)[ntype],printevery=100L,
      transposed=FALSE,
      mc.cores = 1L,nice = 19L,seed = 99L ## mc.abart only
      )

predict()

#gbart :Sürekli ve ikili sonuçlar için genelleştirilmiş BART
#lbart :Lojistik latentlerle ikili sonuçlar için Logit BART
#mbart :Daha az kategoriye sahip kategorik sonuçlar için çok terimli BART
#mbart2 :Daha fazla kategoriye sahip kategorik sonuçlar için çok terimli BART
#pbart :Normal latentlerle ikili sonuçlar için Probit BART
#surv.bart :BART ile hayatta kalma analizi
#wbart :Sürekli sonuçlar için BART
```

# bartMachine
Bayesian Toplamalı Regresyon Ağaçlarının veri analizi ve görselleştirmeye yönelik genişletilmiş özelliklere sahip gelişmiş bir uygulaması
```{r}
library('bartMachine')
#Builds a BART model for regression or classification.
bartMachine(X = NULL, y = NULL, Xy = NULL,num_trees = 50,num_burn_in = 250,
            num_iterations_after_burn_in = 1000,alpha = 0.95, beta = 2, k = 2,
            q = 0.9, nu = 3,prob_rule_class = 0.5,mh_prob_steps = c(2.5, 2.5, 4)/9,
            debug_log = FALSE,run_in_sample = TRUE,s_sq_y = "mse",sig_sq_est = NULL,
            print_tree_illustrations = FALSE,cov_prior_vec = NULL,interaction_constraints = NULL,
            use_missing_data = FALSE,covariates_to_permute = NULL,num_rand_samps_in_library = 10000,
            use_missing_data_dummies_as_covars = FALSE,replace_missing_data_with_x_j_bar = FALSE,
            impute_missingness_with_rf_impute = FALSE,impute_missingness_with_x_j_bar_for_lm = TRUE,
            mem_cache_for_speed = TRUE,flush_indices_to_save_RAM = TRUE,serialize = FALSE,
            seed = NULL,verbose = TRUE)

build_bart_machine(X = NULL, y = NULL, Xy = NULL,num_trees = 50,num_burn_in = 250,
                   num_iterations_after_burn_in = 1000,alpha = 0.95, beta = 2, k = 2,
                   q = 0.9, nu = 3,prob_rule_class = 0.5,mh_prob_steps = c(2.5, 2.5, 4)/9,
                   debug_log = FALSE,run_in_sample = TRUE,s_sq_y = "mse",sig_sq_est = NULL,
                   print_tree_illustrations = FALSE,cov_prior_vec = NULL,
                   interaction_constraints = NULL,use_missing_data = FALSE,
                   covariates_to_permute = NULL,num_rand_samps_in_library = 10000,
                   use_missing_data_dummies_as_covars = FALSE,
                   replace_missing_data_with_x_j_bar = FALSE,
                   impute_missingness_with_rf_impute = FALSE,
                   impute_missingness_with_x_j_bar_for_lm = TRUE,
                   mem_cache_for_speed = TRUE,flush_indices_to_save_RAM = TRUE,
                   serialize = FALSE,seed = NULL,verbose = TRUE)

predict()
plot()
```

# BayesTree
Bu BART:Bayesian Katkısal Regresyon Ağaçlarının bir uygulamasıdır
```{r}
library('BayesTree')
bart(x.train, y.train, x.test=matrix(0.0,0,0),sigest=NA, sigdf=3, sigquant=.90,
     k=2.0,power=2.0, base=.95,binaryOffset=0,ntree=200,ndpost=1000, nskip=100,
     printevery=100, keepevery=1, keeptrainfits=TRUE,usequants=FALSE, numcut=100,
     printcutoffs=0,verbose=TRUE)

#pdbart :Partial Dependence Plots for BART
```

# bst
Temel öğreniciler olarak bileşen bazında doğrusal, düzleştirici eğriler ve ağaç modelleri ile kayıp fonksiyonlarını optimize etmek için gradyan artırma.
```{r}
library('bst')
bst(x, y, cost = 0.5, family = c("gaussian", "hinge", "hinge2", "binom", "expo",
                                 "poisson", "tgaussianDC", "thingeDC", "tbinomDC",
                                 "binomdDC", "texpoDC", "tpoissonDC","huber",
                                 "thuberDC", "clossR", "clossRMM", "closs",
                                 "gloss", "qloss", "clossMM","glossMM", "qlossMM", "lar"),
    ctrl = bst_control(), control.tree = list(maxdepth = 1),
    learner = c("ls", "sm", "tree"))

predict(object, newdata=NULL, newy=NULL, mstop=NULL,
        type=c("response", "all.res", "class", "loss", "error"), ...)

plot(x, type = c("step", "norm"),...)
coef(object, which=object$ctrl$mstop, ...)
fpartial(object, mstop=NULL, newdata=NULL)

#mada :One-vs-all multi-class AdaBoost
#mbst :Boosting for Multi-Classification
#mhingebst :Boosting for Multi-class Classification
#mhingeova :Multi-class HingeBoost
#rbst :Robust Boosting for Robust Loss Functions
```

# C50
Quinlan'ın C5.0 algoritmasını kullanarak sınıflandırma ağacı modellerine veya kural tabanlı modellere uyun
```{r}
library('C50')
C5.0(x,y,trials = 1,rules = FALSE,weights = NULL,control = C5.0Control(),costs = NULL)

predict()

mod1 <- C5.0(Species ~ ., data = iris)
plot(mod1)
plot(mod1, subtree = 3)
```

# caret
Sınıflandırma ve regresyon modellerini eğitmek ve çizmek için çeşitli işlevler.
```{r}
library('caret')
#Model Ortalamasını Kullanan Sinir Ağları
avNNet(formula,data,weights,...,repeats = 5,bag = FALSE,allowParallel = TRUE,
       seeds = sample.int(1e+05, repeats),subset,na.action,contrasts = NULL)

predict()

#bagging-torbalama   ((ldaBag,plsBag,nbBag,ctreeBag,svmBag,nnetBag))
bag(x, y, B = 10, vars = ncol(x), bagControl = NULL, ...)

bagControl(fit = NULL,predict = NULL,aggregate = NULL,downSample = FALSE,
           oob = TRUE,allowParallel = TRUE)

predict()

#bagEarth :Bagged Earth
#bagFDA :Bagged FDA

confusionMatrix()
createDataPartition()
nearZeroVar()
varImp(object, ...)#Calculation of variable importance for regression and classification
models

#knn3 :k-Nearest Neighbour Classification
#knnreg :k-Nearest Neighbour Regression
#pcaNNet :Neural Networks with a Principal Component Step
#plsda :Partial Least Squares and Sparse Partial Least Squares Discriminant Analysis

train( #Tahmine Dayalı Modelleri Farklı Ayarlama Parametrelerine Uydur
  x,
  y,
  method = "rf",
  preProcess = NULL,
  ...,
  weights = NULL,
  metric = ifelse(is.factor(y), "Accuracy", "RMSE"),
  maximize = ifelse(metric %in% c("RMSE", "logLoss", "MAE", "logLoss"), FALSE, TRUE),
  trControl = trainControl(),
  tuneGrid = NULL,
  tuneLength = ifelse(trControl$method == "none", 1, 3)
)

trainControl( #Control parameters for train
  method = "boot",
  number = ifelse(grepl("cv", method), 10, 25),
  repeats = ifelse(grepl("[d_]cv$", method), 1, NA),
  p = 0.75,
  search = "grid",
  initialWindow = NULL,
  horizon = 1,
  fixedWindow = TRUE,
  skip = 0,
  verboseIter = FALSE,
  returnData = TRUE,
  returnResamp = "final",
  savePredictions = FALSE,
  classProbs = FALSE,
  summaryFunction = defaultSummary,
  selectionFunction = "best",
  preProcOptions = list(thresh = 0.95, ICAcomp = 3, k = 5, freqCut = 95/5, uniqueCut =
  10, cutoff = 0.9),
  sampling = NULL,
  index = NULL,
  indexOut = NULL,
  indexFinal = NULL,
  timingSamps = 0,
  predictionBounds = rep(FALSE, 2),
  seeds = NA,
  adaptive = list(min = 5, alpha = 0.05, method = "gls", complete = TRUE),
  trim = FALSE,
  allowParallel = TRUE
)

```

# CORElearn
Tahmine dayalı modeller, örneğin isteğe bağlı yapıcı tümevarımlı sınıflandırma ve regresyon ağaçlarını ve yapraklardaki modelleri, rastgele ormanları, kNN'yi, naiveBayes'i ve yerel ağırlıklı regresyonu içerir.
```{r}
library('CORElearn')
#Build a classification or regression model
CoreModel(formula, data,model=c("rf","rfNear","tree","knn","knnKernel","bayes","regTree"),
          costMatrix=NULL,...)

predict()
```

# cubist
Bu işlev, eğitim setindeki en yakın komşulara dayalı ek düzeltmelerle birlikte Quinlan (1992) (diğer adıyla M5)'te açıklanan kural tabanlı modele uyar.
```{r}
library('Cubist')
cubist(x, y, committees = 1, control = cubistControl(), weights = NULL, ...)
predict()
```



# dipm
DIPM yöntemi, belirli bir tedavi grubunda özellikle zayıf veya güçlü performansa sahip alt grupları arayan bir sınıflandırma ağacıdır. Depth Importance in Precision Medicine (DIPM)
```{r}
library('dipm')
dipm(formula,data,types = NULL,nmin = 5,nmin2 = 5,ntree = NULL,mtry = Inf,
     maxdepth = Inf,maxdepth2 = Inf,print = TRUE,dataframe = FALSE,prune = FALSE)

spmtree(formula,data,types = NULL,nmin = 5,maxdepth = Inf,print = TRUE,dataframe = FALSE,
        prune = FALSE)
```

# earth
Friedman'ın "Çok Değişkenli Uyarlanabilir Regresyon Splineları" ve "Hızlı MARS" makalelerindeki teknikleri kullanarak bir regresyon modeli oluşturun
```{r}
library('earth')
earth(formula = stop("no 'formula' argument"), data = NULL,  weights = NULL, wp = NULL,
      subset = NULL,na.action = na.fail,
      pmethod = c("backward", "none", "exhaustive", "forward", "seqrep", "cv"),
      keepxy = FALSE, trace = 0, glm = NULL, degree = 1, nprune = NULL,nfold=0,
      ncross=1, stratify=TRUE,varmod.method = "none", varmod.exponent = 1,
      varmod.conv = 1, varmod.clamp = .1, varmod.minspan = -3,Scale.y = NULL, ...)

predict(object = stop("no 'object' argument"), newdata = NULL,
        type = c("link", "response", "earth", "class", "terms"),
        interval = "none", level = .95,thresh = .5, trace = FALSE, ...)
```

# elasticnet
Elastic-Net'in tüm çözüm yolunu uydurmak için işlevler sağlar ve ayrıca seyrek PCA yapmak için işlevler sağlar. LARS-EN algoritması sıfırdan başlayarak tüm katsayılar ve uyum dizisini sağlar.
```{r}
library('elasticnet')
enet(x, y, lambda, max.steps, normalize=TRUE, intercept=TRUE,trace = FALSE,
     eps = .Machine$double.eps)

predict(object, newx, s, type = c("fit", "coefficients"),
        mode = c("step","fraction", "norm", "penalty"),naive=FALSE, ...)

#Sparse Principal Components Analysis
spca(x, K, para, type=c("predictor","Gram"),sparse=c("penalty","varnum"),
     use.corr=FALSE, lambda=1e-6,max.iter=200, trace=FALSE, eps.conv=1e-3)
```

# evclass
Dempster-Shafer kütle fonksiyonları biçiminde çıktılar sağlayan farklı kanıtsal sınıflandırıcılar. Yöntemler şunlardır: kanıtsal K-en yakın komşu kuralı, kanıtsal sinir ağı, radyal temel fonksiyonlu sinir ağları, lojistik regresyon, ileri beslemeli sinir ağları.
```{r}
library('evclass')
#Ana işlevler şunlardır:
#  EK-NN sınıflandırıcısının başlatılması, eğitimi ve değerlendirilmesi için EkNNinit, EkNNfit ve #EkNNval;
#  kanıtsal sinir ağı sınıflandırıcısı için proDSinit, proDSfit ve proDSval;
#  karar verme kararı; RBF sınıflandırıcısı için RBFinit, RBFfit ve RBFval;
#  Eğitimli lojistik regresyon veya çok katmanlı sınıflandırıcılardan çıktı kütle fonksiyonlarını #hesaplamak için calcAB ve calcm
#EkNNfit :Training of the EkNN classifier
EkNNfit(x,y,K,param = NULL,alpha = 0.95,lambda = 1/max(as.numeric(y)),optimize = TRUE,
        options = list(maxiter = 300, eta = 0.1, gain_min = 1e-06, disp = TRUE))
#EkNNinit :Initialization of parameters for the EkNN classifier
#EkNNval :Classification of a test set by the EkNN classifier
proDSfit(x,y,param,lambda = 1/max(as.numeric(y)),mu = 0,optimProto = TRUE,
         options = list(maxiter = 500, eta = 0.1, gain_min = 1e-04, disp = 10))
RBFfit(x,y,param,lambda = 0,control = list(fnscale = -1, trace = 2, maxit = 1000),
       optimProto = TRUE)
calcm(x, A, B)
calcAB(W, mu = NULL)
```

# evtree
optimum sınıflandırma ve regresyon ağaçlarını öğrenmek için evrimsel bir algoritma uygular, CART alternatifidir
```{r}
library('evtree')
evtree(formula, data, subset, na.action, weights, control = evtree.control(...), ...)
```

# frbs
Sınıflandırma ve regresyon görevleriyle ilgilenmek için bulanık kural tabanlı sistemlere (FRBS'ler) dayalı çeşitli öğrenme algoritmalarının uygulanması.
```{r}
library('frbs')
# the adaptive-network-based fuzzy inference system model building
ANFIS(data.train, num.labels, max.iter = 10, step.size = 0.01,type.tnorm = "MIN",
      type.snorm = "MAX",type.implication.func = "ZADEH")

# dynamic evolving neural-fuzzy inference system model building
DENFIS(data.train, range.data.ori, Dthr = 0.1, max.iter = 100,step.size = 0.01, d = 2)

#Ishibuchi’s method based on hybridization of genetic cooperative-competitive learning (GCCL) and Pittsburg
FH.GBML(data.train, popu.size = 10, max.num.rule = 5,persen_cross = 0.6, persen_mutant = 0.3,
        max.gen = 10, num.class,range.data.input, p.dcare = 0.5, p.gccl = 0.5)

#FIR.DM FIR.DM model building
#FRBCS.CHI FRBCS.CHI model building
#FRBCS.W FRBCS.W model building
#FS.HGD FS.HGD model building
#GFS.FR.MOGUL GFS.FR.MOGUL model building
#GFS.GCCL GFS.GCCL model building
#GFS.Thrift GFS.Thrift model building
#HyFIS HyFIS model building
#WM WM model building
frbsPMML(model, model.name = "frbs_model", app.name = "frbs",description = NULL,
         copyright = NULL,algorithm.name = model$method.type, ...)

```

# glmertree
(Genelleştirilmiş) doğrusal karma modellere dayalı özyinelemeli bölümleme
(GLMM'ler) 'lme4'ten lmer()/glmer() ve 'partykit'ten lmtree()/glmtree()'yi birleştiriyor.
```{r}
library('glmertree')
betamertree(formula, data, family = NULL, weights = NULL, cluster = NULL,
    ranefstart = NULL, offset = NULL, REML = TRUE, joint = TRUE,
    abstol = 0.001, maxit = 100, dfsplit = TRUE, verbose = FALSE,
    plot = FALSE, glmmTMB.control = glmmTMB::glmmTMBControl(), ...)

lmertree(formula, data, weights = NULL, cluster = NULL,
    ranefstart = NULL, offset = NULL, joint = TRUE,
    abstol = 0.001, maxit = 100, dfsplit = TRUE, verbose = FALSE,
    plot = FALSE, REML = TRUE, lmer.control = lmerControl(), ...)

glmertree(formula, data, family = "binomial", weights = NULL,
    cluster = NULL, ranefstart = NULL, offset = NULL, joint = TRUE,
    abstol = 0.001, maxit = 100, dfsplit = TRUE, verbose = FALSE,
    plot = FALSE, nAGQ = 1L, glmer.control = glmerControl(), ...)

```

# glmnet
Doğrusal regresyon, lojistik ve çok terimli regresyon modelleri, Poisson regresyonu, Cox modeli, çok yanıtlı Gaussian ve gruplandırılmış çok terimli regresyon için tüm kement veya elastik ağ düzenleme yolunu uydurmak için son derece etkili prosedürler. 
```{r}
library('glmnet')
#Fit a Cox regression model with elastic net regularization for a single value of lambda
cox.fit(x,y,weights,lambda,alpha = 1,offset = rep(0, nobs),thresh = 1e-10,
        maxit = 1e+05,penalty.factor = rep(1, nvars),exclude = c(),
        lower.limits = -Inf,upper.limits = Inf,warm = NULL,from.cox.path = FALSE,
        save.fit = FALSE,trace.it = 0)

#elnet.fit :Solve weighted least squares (WLS) problem for a single lambda value

#fit a GLM with lasso or elasticnet regularization
glmnet(x,y,
       family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian"),
       weights = NULL,offset = NULL,alpha = 1,nlambda = 100,
       lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),lambda = NULL,
       standardize = TRUE,intercept = TRUE,thresh = 1e-07,dfmax = nvars + 1,
       pmax = min(dfmax * 2 + 20, nvars),exclude = NULL,penalty.factor = rep(1, nvars),
       lower.limits = -Inf,upper.limits = Inf,maxit = 1e+05,
       type.gaussian = ifelse(nvars < 500, "covariance", "naive"),
       type.logistic = c("Newton", "modified.Newton"),standardize.response = FALSE,
       type.multinomial = c("ungrouped", "grouped"),relax = FALSE,trace.it = 0)

predict()
```

# glmpath
L1 düzenli genelleştirilmiş doğrusal modeller ve Cox orantılı tehlike modeli için yol izleyen bir algoritma.
```{r}
library('glmpath')
#Fits the entire L1 regularization path for Cox proportional hazards model
coxpath(data, nopenalty.subset = NULL, method = c("breslow", "efron"),
  lambda2 = 1e-5, max.steps = 10 * min(n, m), max.norm = 100 * m,
  min.lambda = (if (m >= n) 1e-3 else 0), max.vars = Inf,
  max.arclength = Inf, frac.arclength = 1, add.newvars = 1,
  bshoot.threshold = 0.1, relax.lambda = 1e-7,
  approx.Gram = FALSE, standardize = TRUE,
  eps = .Machine$double.eps, trace = FALSE)
predict(object, data, s, type = c("coefficients", "loglik",
  "lp", "risk", "coxph"), mode = c("step",
  "norm.fraction", "norm", "lambda.fraction", "lambda"),
  eps = .Machine$double.eps, ...)

#Fits the entire L1 regularization path for generalized linear models
glmpath(x, y, data, nopenalty.subset = NULL, family = binomial,
  weight = rep(1, n), offset = rep(0, n), lambda2 = 1e-5,
  max.steps = 10 * min(n, m), max.norm = 100 * m,
  min.lambda = (if (m >= n) 1e-6 else 0), max.vars = Inf,
  max.arclength = Inf, frac.arclength = 1, add.newvars = 1,
  bshoot.threshold = 0.1, relax.lambda = 1e-8,
  standardize = TRUE, eps = .Machine$double.eps,
  trace = FALSE)
predict(object, newx, newy, s, type = c("link", "response",
  "loglik", "coefficients"), mode = c("step",
  "norm.fraction", "norm", "lambda.fraction", "lambda"),
  weight = NULL, offset = NULL,
  eps = .Machine$double.eps, ...)

```


# grf
Ormana dayalı istatistiksel tahmin ve çıkarım.
GRF, tümü eksik ortak değişkenleri destekleyen, heterojen tedavi etkileri tahmini (isteğe bağlı olarak sağ sansürlü sonuçlar, çoklu tedavi kolları veya sonuçları veya araçsal değişkenler kullanılarak) ve ayrıca en küçük kareler regresyonu, niceliksel regresyon ve hayatta kalma regresyonu için parametrik olmayan yöntemler sağlar.
```{r}
library('grf')
boosted_regression_forest(X,Y,num.trees = 2000,sample.weights = NULL,clusters = NULL,
                          equalize.cluster.weights = FALSE,sample.fraction = 0.5,
                          mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
                          min.node.size = 5,honesty = TRUE,honesty.fraction = 0.5,
                          honesty.prune.leaves = TRUE,alpha = 0.05,imbalance.penalty = 0,
                          ci.group.size = 2,tune.parameters = "none",tune.num.trees = 10,
                          tune.num.reps = 100,tune.num.draws = 1000,boost.steps = NULL,
                          boost.error.reduction = 0.97,boost.max.steps = 5,
                          boost.trees.tune = 10,num.threads = NULL,
                          seed = runif(1, 0, .Machine$integer.max))
predict()
#causal_forest :Causal forest
#causal_survival_forest :Causal survival forest
#instrumental_forest :Intrumental forest
#ll_regression_forest :Local linear forest
#lm_forest :LM Forest
#multi_arm_causal_forest :Multi-arm/multi-outcome causal forest
#multi_regression_forest :Multi-task regression forest
#quantile_forest :Quantile forest
#regression_forest :Regression forest
#survival_forest :Survival forest

```

# grplasso
Grup lasso cezasına sahip, kullanıcı tarafından belirlenen (GLM-) modellere uyar
```{r}
library('grplasso')
grplasso(x, y, index, weights = rep(1, length(y)), offset = rep(0,length(y)),
         lambda, coef.init = rep(0, ncol(x)),penscale = sqrt, model = LogReg(),
         center = TRUE,standardize = TRUE, control = grpl.control(), ...)
predict()
```

# grpreg
Gruplandırılmış cezalarla doğrusal regresyon, GLM ve Cox regresyon modellerinin düzenlileştirme yolunu uydurmak için etkili algoritmalar. Bu, grup kementi, grup MCP ve grup SCAD gibi grup seçim yöntemlerinin yanı sıra grup üstel kement, bileşik MCP ve grup köprüsü gibi iki düzeyli seçim yöntemlerini içerir.
```{r}
library('grpreg')
#gBridge :Fit a group bridge regression path
gBridge(X, y, group=1:ncol(X), family=c("gaussian", "binomial",
  "poisson"), nlambda=100, lambda, lambda.min={if (nrow(X) > ncol(X)) .001
  else .05}, lambda.max, alpha=1, eps=.001, delta=1e-7, max.iter=10000,
  gamma=0.5, group.multiplier, warn=TRUE, returnX=FALSE, ...)
#grpreg :Fit a group penalized regression path
grpreg(X, y, group=1:ncol(X), penalty=c("grLasso", "grMCP", "grSCAD",
  "gel", "cMCP"), family=c("gaussian", "binomial", "poisson"),
  nlambda=100, lambda, lambda.min={if (nrow(X) > ncol(X)) 1e-4 else .05},
  log.lambda = TRUE, alpha=1, eps=1e-4, max.iter=10000, dfmax=p,
  gmax=length(unique(group)), gamma=ifelse(penalty == "grSCAD", 4, 3),
  tau = 1/3, group.multiplier, warn=TRUE, returnX = FALSE, ...)
#grpsurv Fit an group penalized survival model
grpsurv(X, y, group=1:ncol(X), penalty=c("grLasso", "grMCP", "grSCAD",
  "gel", "cMCP"), gamma=ifelse(penalty=="grSCAD", 4, 3), alpha=1,
  nlambda=100, lambda, lambda.min={if (nrow(X) > ncol(X)) 0.001 else .05},
  eps=.001, max.iter=10000, dfmax=p, gmax=length(unique(group)), tau=1/3,
  group.multiplier, warn=TRUE, returnX=FALSE, ...)

predict()

```

# ipred
Dolaylı sınıflandırma yoluyla geliştirilmiş tahmin modelleri ve sınıflandırma, regresyon ve hayatta kalma sorunları için torbalamanın yanı sıra tahmin hatasının yeniden örneklemeye dayalı tahmin edicileri.
```{r}
library('ipred')
#bagging :Bagging Classification, Regression and Survival Trees
bagging(formula, data, subset, na.action=na.rpart, ...)

#inbagg :Indirect Bagging
inbagg(formula, data, pFUN=NULL,cFUN=list(model = NULL, predict = NULL,training.set = NULL),
       nbagg = 25, ns = 0.5, replace = FALSE, ...)

#ipredknn :k-Nearest Neighbour Classification
ipredknn(formula, data, subset, na.action, k=5, ...)

#slda Stabilised Linear Discriminant Analysis
slda(formula, data, subset, na.action=na.rpart, ...)

predict()
```

# islasso
Model katsayıları üzerinde tahmin ve çıkarım yapılmasına olanak sağlamak için kement düzenleme modellerine yönelik uyarılmış yumuşatma (IS) fikrinin uygulanması (şu anda yalnızca hipotez testi). Çeşitli bağlantı fonksiyonlarıyla doğrusal, lojistik, Poisson ve gama regresyonları uygulanır.
```{r}
library('islasso')
islasso(formula, family = gaussian, lambda, alpha = 1, data, weights, subset,
        offset, unpenalized, contrasts = NULL, control = is.control())
predict()
```

# joinet
R paket birleşimi, yığılmış genellemeyi kullanarak çok değişkenli sırt ve kement regresyonunu uygular. Bu çok değişkenli regresyon, ilişkili sonuçları tahmin etmede tipik olarak tek değişkenli regresyondan daha iyi performans gösterir. Yüksek boyutlu ortamlarda tahmine dayalı ve yorumlanabilir modeller sağlar.
```{r}
library('joinet')
#Multivariate Elastic Net Regression
joinet(Y,X,family = "gaussian",nfolds = 10,foldid = NULL,type.measure = "deviance",
       alpha.base = 1,alpha.meta = 1,weight = NULL,sign = NULL,)
predict()
```

# klaR
Sınıflandırma ve görselleştirmeye yönelik çeşitli işlevler, örn. düzenli diskriminant analizi, sknn() çekirdek yoğunluğu saf Bayes, denetimli sınıflandırma için 'svmlight' ve stepclass() sarmalayıcı değişken seçimi için bir arayüz, sınıflandırma kurallarının partimat() görselleştirilmesi ve küme sonuçlarının shardsplot() yanı sıra kmodes() için bir arayüz kategorik veriler için kümeleme, corclust() değişken kümeleme, farklı değişken kümeleme modellerinden değişken çıkarma ve kanıt ağırlığı ön işleme.
```{r}
library('klaR')
#kmodes K-Modes Clustering
kmodes(data, modes, iter.max = 10, weighted = FALSE, fast = TRUE)

#loclda Localized Linear Discriminant Analysis (LocLDA)
loclda(formula, data, ..., subset, na.action)

#locpvs Pairwise variable selection for classification in local models
locpvs(x, subclasses, subclass.labels, prior=NULL, method="lda",
       vs.method = c("ks.test", "stepclass", "greedy.wilks"),niveau=0.05, fold=10,
       impr=0.1, direct="backward", out=FALSE, ...)

#meclight.default Minimal Error Classification
meclight(formula, data = NULL, ..., subset, na.action = na.fail)

#NaiveBayes Naive Bayes Classifier
NaiveBayes(formula, data, ..., subset, na.action = na.pass)

#nm Nearest Mean Classification
nm(x, grouping, gamma = 0, ...)

#pvs Pairwise variable selection for classification
pvs(x, grouping, prior=NULL, method="lda",vs.method=c("ks.test","stepclass","greedy.wilks"),
    niveau=0.05,fold=10, impr=0.1, direct="backward", out=FALSE, ...)

#rda Regularized Discriminant Analysis (RDA)
rda(x, grouping = NULL, prior = NULL, gamma = NA,lambda = NA,
    regularization = c(gamma = gamma, lambda = lambda),crossval = TRUE, fold = 10,
    train.fraction = 0.5,estimate.error = TRUE, output = FALSE, startsimplex = NULL,
    max.iter = 100, trafo = TRUE, simAnn = FALSE, schedule = 2,T.start = 0.1,
    halflife = 50, zero.temp = 0.01, alpha = 2,K = 100, ...)

#sknn Simple k nearest Neighbours
sknn(x, grouping, kn = 3, gamma=0, ...)

predict()
```

# lars
Tek bir en küçük kareler uyumu maliyetiyle tüm bir kement dizisinin yerleştirilmesi için etkili prosedürler. En küçük açı regresyonu ve sonsuz küçük ileri aşamalı regresyon, aşağıdaki makalede açıklandığı gibi kementle ilgilidir.
```{r}
library('lars')
lars(x, y, type = c("lasso", "lar", "forward.stagewise", "stepwise"),
     trace = FALSE, normalize = TRUE, intercept = TRUE, Gram, eps = 1e-12,
     max.steps, use.Gram = TRUE)

predict(object, newx, s, type = c("fit", "coefficients"),
        mode = c("step","fraction", "norm", "lambda"), ...)
```

# LiblineaR
Büyük ölçekli düzenli doğrusal sınıflandırma ve regresyonu çözmek için basit bir kütüphane. Şu anda L2-düzenlenmiş sınıflandırmanın (lojistik regresyon, L2-kayıp doğrusal SVM ve L1-kayıp doğrusal SVM gibi) yanı sıra L1-düzenlileştirilmiş sınıflandırmayı (L2-kayıp doğrusal SVM ve lojistik regresyon gibi) ve L2-düzenlenmiş destek vektörünü desteklemektedir. regresyon (L1- veya L2 kaybıyla). LiblineaR'ın ana özellikleri arasında çok sınıflı sınıflandırma (bire karşı geri kalanı ve Crammer & Singer yöntemi), model seçimi için çapraz doğrulama, olasılık tahminleri (yalnızca lojistik regresyon) veya dengesiz veriler için ağırlıklar yer alır.
```{r}
library('LiblineaR')
LiblineaR(data,target,type = 0,cost = 1,epsilon = 0.01,svr_eps = NULL,bias = 1,
          wi = NULL,cross = 0,verbose = FALSE,findC = FALSE,useInitC = TRUE)
predict()
```

# lightgbm
Yüksek Verimli Gradyan Artırma Karar Ağacı
```{r}
library('lightgbm')
lightgbm(data,label = NULL,weights = NULL,params = list(),nrounds = 100L,
         verbose = 1L,eval_freq = 1L,early_stopping_rounds = NULL,init_model = NULL,
         callbacks = list(),serializable = TRUE,objective = "auto",init_score = NULL,
         num_threads = NULL)
predict()
```

# mlpack
C++ ile yazılmış, en ileri makine öğrenimi algoritmalarının hızlı, genişletilebilir uygulamalarını sağlamayı amaçlayan hızlı, esnek bir makine öğrenimi kitaplığı. 33 methods
```{r}
library('mlpack')
adaboost(input_model = NA,iterations = NA,labels = NA,test = NA,tolerance = NA,
         training = NA,verbose = FALSE,weak_learner = NA)

bayesian_linear_regression(center = FALSE,input = NA,input_model = NA,responses = NA,
                           scale = FALSE,test = NA,verbose = FALSE)

dbscan(input,epsilon = NA,min_size = NA,naive = FALSE,selection_type = NA,
       single_mode = FALSE,tree_type = NA,verbose = FALSE)

decision_tree(input_model = NA,labels = NA,maximum_depth = NA,minimum_gain_split = NA,
              minimum_leaf_size = NA,print_training_accuracy = FALSE,print_training_error = FALSE,
              test = NA,test_labels = NA,training = NA,verbose = FALSE,weights = NA)

gmm_train(gaussians,input,diagonal_covariance = FALSE,input_model = NA,kmeans_max_iterations = NA,
          max_iterations = NA,no_force_positive = FALSE,noise = NA,percentage = NA,
          refined_start = FALSE,samplings = NA,seed = NA,tolerance = NA,trials = NA,
          verbose = FALSE)

hmm_train(input_file,batch = FALSE,gaussians = NA,input_model = NA,labels_file = NA,
          seed = NA,states = NA,tolerance = NA,type = NA,verbose = FALSE)

kernel_pca(input,kernel,bandwidth = NA,center = FALSE,degree = NA,kernel_scale = NA,
           new_dimensionality = NA,nystroem_method = FALSE,offset = NA,sampling = NA,
           verbose = FALSE)

kmeans(clusters,input,algorithm = NA,allow_empty_clusters = FALSE,in_place = FALSE,
       initial_centroids = NA,kill_empty_clusters = FALSE,kmeans_plus_plus = FALSE,
       labels_only = FALSE,max_iterations = NA,percentage = NA,refined_start = FALSE,
       samplings = NA,seed = NA,verbose = FALSE)

knn(algorithm = NA,epsilon = NA,input_model = NA,k = NA,leaf_size = NA,query = NA,
    random_basis = FALSE,reference = NA,rho = NA,seed = NA,tau = NA,tree_type = NA,
    true_distances = NA,true_neighbors = NA,verbose = FALSE)

lars(input = NA,input_model = NA,lambda1 = NA,lambda2 = NA,no_intercept = FALSE,
     no_normalize = FALSE,responses = NA,test = NA,use_cholesky = FALSE,verbose = FALSE)

linear_regression(input_model = NA,lambda = NA,test = NA,training = NA,
                  training_responses = NA,verbose = FALSE)

linear_svm(delta = NA,epochs = NA,input_model = NA,labels = NA,lambda = NA,
           max_iterations = NA,no_intercept = FALSE,num_classes = NA,optimizer = NA,
           seed = NA,shuffle = FALSE,step_size = NA,test = NA,test_labels = NA,
           tolerance = NA,training = NA,verbose = FALSE)

lmnn(input,batch_size = NA,center = FALSE,distance = NA,k = NA,labels = NA,
     linear_scan = FALSE,max_iterations = NA,normalize = FALSE,optimizer = NA,
     passes = NA,print_accuracy = FALSE,range = NA,rank = NA,regularization = NA,
     seed = NA,step_size = NA,tolerance = NA,verbose = FALSE)

logistic_regression(batch_size = NA,decision_boundary = NA,input_model = NA,
                    labels = NA,lambda = NA,max_iterations = NA,optimizer = NA,
                    step_size = NA,test = NA,tolerance = NA,training = NA,
                    verbose = FALSE)

mean_shift(input,force_convergence = FALSE,in_place = FALSE,labels_only = FALSE,
           max_iterations = NA,radius = NA,verbose = FALSE)

nbc(incremental_variance = FALSE,input_model = NA,labels = NA,test = NA,training = NA,
    verbose = FALSE)#Parametric Naive Bayes Classifier

nca(input,armijo_constant = NA,batch_size = NA,labels = NA,linear_scan = FALSE,
    max_iterations = NA,max_line_search_trials = NA,max_step = NA,min_step = NA,
    normalize = FALSE,num_basis = NA,optimizer = NA,seed = NA,step_size = NA,
    tolerance = NA,verbose = FALSE,wolfe = NA)#Neighborhood Components Analysis

nmf(input,rank,initial_h = NA,initial_w = NA,max_iterations = NA,min_residue = NA,
    seed = NA,update_rules = NA,verbose = FALSE)#Non-negative Matrix Factorization

pca(input,decomposition_method = NA,new_dimensionality = NA,scale = FALSE,
    var_to_retain = NA,verbose = FALSE)

#---
perceptron(input_model = NA,labels = NA,max_iterations = NA,test = NA,training = NA,
           verbose = FALSE)

random_forest(input_model = NA,labels = NA,maximum_depth = NA,minimum_gain_split = NA,
              minimum_leaf_size = NA,num_trees = NA,print_training_accuracy = FALSE,
              seed = NA,subspace_dim = NA,test = NA,test_labels = NA,training = NA,
              verbose = FALSE,warm_start = FALSE)

softmax_regression(input_model = NA,labels = NA,lambda = NA,max_iterations = NA,
                   no_intercept = FALSE,number_of_classes = NA,test = NA,
                   test_labels = NA,training = NA,verbose = FALSE)

#approx_kfn :Yaklaşık en uzak komşu araması
#bayesian_linear_regression :BayesianLinearRegression
#cf :Tavsiye sistemleri için çeşitli işbirlikçi filtreleme (CF) tekniklerinin uygulanması.
#det :Yoğunluk Tahmin Ağaçları ile Yoğunluk Tahmini
#emst :Hızlı Öklid Minimum Yayılan Ağaç
#fastmks :Tek ağaçlı ve çift ağaçlı hızlı maksimum çekirdek arama (FastMKS) algoritmasının bir uygulaması.
#hoeffding_tree :Sınıflandırmaya yönelik bir akış karar ağacı biçimi olan Hoeffding ağaçlarının bir uygulaması.
#kde :Çift ağaç algoritmalarıyla çekirdek yoğunluğu tahmininin bir uygulaması.
#kfn :Tek ağaçlı ve çift ağaçlı algoritmaları kullanan k-en uzak komşu aramasının bir uygulaması.
#krann :Tek ağaç ve çift ağaç algoritmalarını kullanan, yaklaşık k-en yakın komşu aramasının (kRANN) bir uygulaması.
#lsh :Yerelliğe duyarlı karma (LSH) ile yaklaşık k-en yakın komşu aramasının bir uygulaması.
#Bir dizi referans noktası ve bir dizi sorgu noktası verildiğinde, bu, referans kümesindeki her sorgu noktasının yaklaşık k en yakın komşusunu hesaplayacaktır


#image_converter:Bir görüntüyü veya görüntü kümesini daha sonra diğer mlpack yöntemleri ve yardımcı programları tarafından kullanılabilecek tek bir veri kümesine yüklemeye yönelik bir yardımcı program.
```


# mpath
Uygulamalar, sağlam (cezalandırılmış) genelleştirilmiş doğrusal modelleri ve sağlam destek vektör makinelerini içerir.
```{r}
library('mpath')
#glmreg :fit a GLM with lasso (or elastic net), snet or mnet regularization
glmreg(formula, data, weights, offset=NULL, contrasts=NULL,x.keep=FALSE, y.keep=TRUE, ...)

#glmregNB :fit a negative binomial model with lasso (or elastic net), snet and mnet regularization
#irglm :fit a robust generalized linear models
#irglmreg :Fit a robust penalized generalized linear models
#irsvm :fit case weighted support vector machines with robust loss functions
#loss2 :Composite Loss Value
#loss3 :Composite Loss Value for GLM
#ncl :fit a nonconvex loss based robust linear model
#zipath :Fit zero-inflated count data linear model with lasso (or elastic net),snet or mnet regularization

predict()
```

# naivebayes
Naive Bayes sınıflandırıcısının bu uygulamasında aşağıdaki sınıf koşullu dağılımları mevcuttur: Bernoulli, Kategorik, Gaussian, Poisson ve Çekirdek Yoğunluğu Tahmini yoluyla tahmin edilen sınıf koşullu yoğunluğunun parametrik olmayan temsili. Uygulanan sınıflandırıcılar eksik verileri yönetir ve seyrek verilerden yararlanabilir.
```{r}
library('naivebayes')
bernoulli_naive_bayes(x, y, prior = NULL, laplace = 0, ...)
gaussian_naive_bayes(x, y, prior = NULL, ...)
multinomial_naive_bayes(x, y, prior = NULL, laplace = 0.5, ...)
naive_bayes(x, y, prior = NULL, laplace = 0,usekernel = FALSE, usepoisson = FALSE, ...)
nonparametric_naive_bayes(x, y, prior = NULL, ...)
poisson_naive_bayes(x, y, prior = NULL, laplace = 0, ...)

predict()
```

# ncvreg
Kement veya dışbükey olmayan cezalar kullanan doğrusal regresyon, GLM ve Cox regresyon modelleri için düzenleme yollarına uyar; özellikle minimaks içbükey ceza (MCP) ve yumuşak bir şekilde kırpılmış mutlak sapma (SCAD) cezası ve ilave L2 cezaları seçenekleri ("elastik ağ" fikri).
```{r}
library('ncvreg')
#ncvfit Direct interface for nonconvex penalized regression (non-pathwise)
ncvfit(X,y,init = rep(0, ncol(X)),r,xtx,penalty = c("MCP", "SCAD", "lasso"),
       gamma = switch(penalty, SCAD = 3.7, 3),alpha = 1,lambda,eps = 1e-05,
       max.iter = 1000,penalty.factor = rep(1, ncol(X)),warn = TRUE)

#ncvreg Fit an MCP- or SCAD-penalized regression path
ncvreg(X,y,family = c("gaussian", "binomial", "poisson"),
       penalty = c("MCP", "SCAD", "lasso"),gamma = switch(penalty, SCAD = 3.7, 3),
       alpha = 1,lambda.min = ifelse(n > p, 0.001, 0.05),nlambda = 100,lambda,
       eps = 1e-04,max.iter = 10000,convex = TRUE,dfmax = p + 1,
       penalty.factor = rep(1, ncol(X)),warn = TRUE,returnX)

#ncvsurv Fit an MCP- or SCAD-penalized survival model

predict()
```

# OneR
İyileştirmelerle Tek Kurallı Makine Öğrenimi Sınıflandırma Algoritması
```{r}
library('OneR')
OneR(formula, data, ties.method = c("first", "chisq"),verbose = FALSE, ...)

optbin(formula, data, method = c("logreg", "infogain","naive"), na.omit = TRUE, ...)# for data

predict()
```

# party
Özyinelemeli bölümleme için hesaplamalı bir araç kutusu. Paketin özü, ağaç yapılı regresyon modellerini iyi tanımlanmış bir koşullu regresyon teorisine yerleştiren koşullu çıkarım ağaçlarının bir uygulaması olan çıkarım prosedürleri ctree()'dir.Bu parametrik olmayan regresyon ağaçları sınıfı, nominal, sıralı, sayısal, sansürlü ve ayrıca çok değişkenli yanıt değişkenleri ve ortak değişkenlerin keyfi ölçüm ölçekleri dahil olmak üzere her türlü regresyon problemine uygulanabilir.
```{r}
library('party')
#randomforest
cforest(formula, data = list(), subset = NULL, weights = NULL,
        controls = cforest_unbiased(),xtrafo = ptrafo, ytrafo = ptrafo, scores = NULL)

#Conditional Inference Trees
ctree(formula, data, subset = NULL, weights = NULL,
      controls = ctree_control(), xtrafo = ptrafo, ytrafo = ptrafo,scores = NULL)

#mob Model-based Recursive Partitioning
mob(formula, weights, data = list(), na.action = na.omit, model = glinearModel,
    control = mob_control(), ...)

Preict()
```

# partykit
şunları içerir ('rpart', 'RWeka', 'PMML','party':cforest,ctree,mob)
```{r}
library('partykit')
cforest(formula, data, weights, subset, offset, cluster, strata,na.action = na.pass,
        control = ctree_control(teststat = "quad", testtype = "Univ",mincriterion = 0,
                                saveinfo = FALSE, ...),
        ytrafo = NULL, scores = NULL, ntree = 500L,
        perturb = list(replace = FALSE, fraction = 0.632),
        mtry = ceiling(sqrt(nvar)), applyfun = NULL, cores = NULL,trace = FALSE, ...)

ctree(formula, data, subset, weights, na.action = na.pass, offset, cluster,
      control = ctree_control(...), ytrafo = NULL,converged = NULL, scores = NULL,
      doFit = TRUE, ...)

glmtree(formula, data, subset, na.action, weights, offset, cluster,family = gaussian,
        epsilon = 1e-8, maxit = 25, method = "glm.fit", ...)

lmtree(formula, data, subset, na.action, weights, offset, cluster, ...)

mob(formula, data, subset, na.action, weights, offset, cluster,fit, control = mob_control())

predict()
```

# penalizedLDA
Bu paket, özellik sayısının (p) gözlem sayısını (n) aştığı yüksek boyutlu ortam için tasarlanmış, cezalandırılmış doğrusal diskriminant analizi gerçekleştirir.
```{r}
library('penalizedLDA')
PenalizedLDA(x, y, xte=NULL, type = "standard", lambda, K = 2, chrom =NULL,
             lambda2 = NULL, standardized = FALSE, wcsd.x = NULL, ymat = NULL,
             maxiter = 20, trace=FALSE)
predict()

```

# picasso
Bu paket, genelleştirilmiş doğrusal modeli dışbükey ve dışbükey olmayan cezaya uydurmak için hesaplama açısından verimli araçlar sağlar. Kullanıcılar, l1 ve ridge gibi dışbükey cezalara kıyasla önemli ölçüde daha az tahmin hatası ve fazla uyum sağlayan SCAD ve MCP gibi dışbükey olmayan cezaların üstün istatistiksel özelliğinden yararlanabilirler. Hesaplama, çok aşamalı dışbükey gevşeme ve sıcak başlangıç başlatma, aktif küme güncelleme ve hesaplamayı artırmak için koordinat ön seçimi için güçlü kuraldan yararlanan ve benzersiz bir seyrek yerel optimuma doğrusal bir yakınsama elde eden Yol Yönünde Kalibre Edilmiş Seyrek Çekim algoritması (PICASSO) tarafından gerçekleştirilir. Optimum istatistiksel özelliklere sahip.
Hesaplama, seyrek matris çıktısı kullanılarak bellek açısından optimize edilmiştir.
```{r}
library('picasso')
picasso(X, Y, lambda = NULL, nlambda = 100, lambda.min.ratio =0.05, family = "gaussian",
        method = "l1",type.gaussian = "naive", gamma = 3, df = NULL,standardize = TRUE,
        intercept = TRUE, prec = 1e-07,max.ite = 1000, verbose = FALSE)

predict()
```

# pre
Tahmin kuralı topluluklarını (PRE'ler) türetir. Büyük ölçüde, Friedman ve Popescu (2008)'da açıklanan PRE'lerin türetilmesi prosedürünü, ayarlamalar ve iyileştirmelerle takip etmektedir. Ana işlev pre(), sürekli, ikili, sayım, çok terimli ve çok değişkenli sürekli yanıtlar için kurallardan ve/veya doğrusal terimlerden oluşan tahmin kural topluluklarını türetir. gpe() işlevi, tahmin değişkenlerinin kurallarından, menteşe ve doğrusal işlevlerinden oluşan genelleştirilmiş tahmin topluluklarını türetir.
```{r}
library('pre')
#gpe Derive a General Prediction Ensemble (gpe)
gpe(formula,data,base_learners = list(gpe_trees(), gpe_linear()),
    weights = rep(1, times = nrow(data)),sample_func = gpe_sample(),verbose = FALSE,
    penalized_trainer = gpe_cv.glmnet(),model = TRUE)

#pre Derive a prediction rule ensemble
pre(formula,data,family = gaussian,use.grad = TRUE,weights,type = "both",sampfrac = 0.5,
    maxdepth = 3L,learnrate = 0.01,mtry = Inf,ntrees = 500,confirmatory = NULL,
    singleconditions = FALSE,winsfrac = 0.025,normalize = TRUE,standardize = FALSE,
    ordinal = TRUE,nfolds = 10L,tree.control,tree.unbiased = TRUE,removecomplements = TRUE,
    removeduplicates = TRUE,verbose = FALSE,par.init = FALSE,par.final = FALSE,
    sparse = FALSE)

predict()
```

# quantregForest
Nicelik Regresyon Ormanları, koşullu niceliklerin tahminine yönelik ağaç tabanlı bir topluluk yöntemidir. Özellikle yüksek boyutlu veriler için çok uygundur. Karma sınıfların tahmin değişkenleri ele alınabilir. Paket, 'randomForest' paketine bağlıdır.
```{r}
library('quantregForest')
quantregForest(x,y, nthreads=1, keep.inbag=FALSE, ...)
predict()
```

# randomForestSRC
Tek değişkenli, çok değişkenli, denetimsiz, hayatta kalma, rekabet eden riskler, sınıf dengesizliği sınıflandırması ve niceliksel regresyon için Breiman'ın rastgele ormanlarının hızlı OpenMP paralel hesaplanması. Yeni Mahalanobis'in ilişkili sonuçlar için bölünmesi. Aşırı rastgele ormanlar ve rastgele bölme. Eksik veriler için atama yöntemleri paketi. Alt örneklemeyi kullanan hızlı rastgele ormanlar. Değişken önemi için güven bölgeleri ve standart hatalar. Yeni geliştirilmiş bekletme önemi. Duruma özel önem. Minimum derinlik değişkeninin önemi.
```{r}
library('randomForestSRC')

#quantreg.rfsrc Quantile Regression Forests
quantreg(formula, data, object, newdata,method = "local", splitrule = NULL,
         prob = NULL, prob.epsilon = NULL,oob = TRUE, fast = FALSE, maxn = 1e3)

#rfsrc Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
rfsrc(formula, data, ntree = 500,mtry = NULL, ytry = NULL,nodesize = NULL,
      nodedepth = NULL,splitrule = NULL, nsplit = NULL,
      importance = c(FALSE, TRUE, "none", "anti", "permute", "random"),
      block.size = if (any(is.element(as.character(importance),
                                      c("none", "FALSE")))) NULL else 10,
      bootstrap = c("by.root", "none", "by.user"),samptype = c("swor", "swr"),
      samp = NULL, membership = FALSE,
      sampsize = if (samptype == "swor") function(x){x * .632} else function(x){x},
      na.action = c("na.omit", "na.impute"), nimpute = 1,ntime = 150, cause,
      perf.type = NULL,proximity = FALSE, distance = FALSE, forest.wt = FALSE,
      xvar.wt = NULL, yvar.wt = NULL, split.wt = NULL, case.wt = NULL,forest = TRUE,
      save.memory = FALSE,var.used = c(FALSE, "all.trees", "by.tree"),
      split.depth = c(FALSE, "all.trees", "by.tree"),seed = NULL,
      do.trace = FALSE,statistics = FALSE,...)

#sidClustering.rfsrc sidClustering using SID (Staggered Interaction Data) for Unsupervised Clusterin
sidClustering(data,method = "sid",k = NULL,reduce = TRUE,ntree = 500,
              ntree.reduce = function(p, vtry){100 * p / vtry},fast = FALSE,
              x.no.sid = NULL,use.sid.for.x = TRUE,x.only = NULL, y.only = NULL,
              dist.sharpen = TRUE, ...)

```

# ranger
Özellikle yüksek boyutlu veriler için uygun olan Rastgele Ormanların hızlı bir uygulaması. Sınıflandırma, regresyon, hayatta kalma ve olasılık tahmin ağaçlarından oluşan topluluklar desteklenmektedir. Genom çapında ilişkilendirme çalışmalarından elde edilen veriler verimli bir şekilde analiz edilebilir.
```{r}
library('ranger')
ranger(formula = NULL,data = NULL,num.trees = 500,mtry = NULL,importance = "none",
       write.forest = TRUE,probability = FALSE,min.node.size = NULL,min.bucket = NULL,
       max.depth = NULL,replace = TRUE,sample.fraction = ifelse(replace, 1, 0.632),
       case.weights = NULL,class.weights = NULL,splitrule = NULL,num.random.splits = 1,
       alpha = 0.5,minprop = 0.1,split.select.weights = NULL,always.split.variables = NULL,
       respect.unordered.factors = NULL,scale.permutation.importance = FALSE,
       local.importance = FALSE,regularization.factor = 1,regularization.usedepth = FALSE,
       keep.inbag = FALSE,inbag = NULL,holdout = FALSE,quantreg = FALSE,
       time.interest = NULL,oob.error = TRUE,num.threads = NULL,save.memory = FALSE,
       verbose = TRUE,node.stats = FALSE,seed = NULL,dependent.variable.name = NULL,
       status.variable.name = NULL,classification = NULL,x = NULL,y = NULL,...)

predict()
```

# Rborist
Breiman tarafından tanımlandığı gibi sınıflandırma ve regresyon ormanlarının ölçeklenebilir uygulaması
```{r}
library('Rborist')
# eski komut Rborist(x, y, ...)

#rfArb:Rastgele Orman algoritmasının hızlandırılmış uygulaması. Çok çekirdekli ve GPU donanımı için ayarlandı.
rfArb(x,y,autoCompress = 0.25,ctgCensus = "votes",classWeight = NULL,impPermute = 0,
      indexing = FALSE,maxLeaf = 0,minInfo = 0.01,minNode = if (is.factor(y)) 2 else 3,
      nLevel = 0,nSamp = 0,nThread = 0,nTree = 500,noValidate = FALSE,predFixed = 0,
      predProb = 0.0,predWeight = NULL,quantVec = NULL,quantiles = !is.null(quantVec),
      regMono = NULL,rowWeight = NULL,splitQuant = NULL,thinLeaves = is.factor(y) && !indexing,
      trapUnobserved = FALSE,treeBlock = 1,verbose = FALSE,withRepl = TRUE,...)

predict()
```

# RLT
Regresyon, sınıflandırma ve hayatta kalma analizi için çeşitli ek özelliklere sahip rastgele orman
```{r}
library('RLT')
#Reinforcement Learning Trees
RLT(x,y,censor = NULL,model = "regression",print.summary = 0,use.cores = 1,
    ntrees = if (reinforcement) 100 else 500,mtry = max(1, as.integer(ncol(x)/3)),
    nmin = max(1, as.integer(log(nrow(x)))),alpha = 0.4,split.gen = "random",
    nsplit = 1,resample.prob = 0.9,replacement = TRUE,npermute = 1,
    select.method = "var",subject.weight = NULL,variable.weight = NULL,
    track.obs = FALSE,importance = TRUE,reinforcement = FALSE,muting = -1,
    muting.percent = if (reinforcement) MuteRate(nrow(x), ncol(x), speed = "aggressive",
                                                 info = FALSE) else 0,
    protect = as.integer(log(ncol(x))),combsplit = 1,combsplit.th = 0.25,
    random.select = 0,embed.n.th = 4 * nmin,
    embed.ntrees = max(1, -atan(0.01 * (ncol(x) - 500))/pi * 100 + 50),
    embed.resample.prob = 0.8,embed.mtry = 1/2,embed.nmin = as.integer(nrow(x)^(1/3)),
    embed.split.gen = "random",embed.nsplit = 1)

predict()

```

# rminer
Kısa ve tutarlı bir işlevler kümesi sunarak sınıflandırma ve regresyon (zaman serisi tahmini dahil) görevlerinde veri madenciliği algoritmalarının kullanımını kolaylaştırır. fonksiyonları tamamen diğer paketlerden çağırır.
```{r}
library('rminer')
fit(x, data = NULL, model = "default", task = "default",search = "heuristic",
    mpar = NULL, feature = "none",scale = "default", transform = "none",
    created = NULL, fdebug = FALSE, ...)

predict()
```
MODEL: naive, ctree,cv.glm.net,rpart,kknn,knn,ksvm,lssvm,mlp,mlpe,randomForest,xgboost,
bagging,boosting,lda,multinom(lr),naiveBayes,qda,cubist,lm,mr,mars,pcr,plsr,cppls,rvm

# RPMM
Beta ve Gauss Karışımları için Yinelemeli Bölümlenmiş Karışım Modeli.
Bu, hiyerarşik kümelemeye benzer ancak aynı zamanda sonlu karışım modellerine benzer şekilde sınıfların hiyerarşisini döndüren model tabanlı bir kümeleme algoritmasıdır.
```{r}
library('RPMM')
#Fits a beta mixture model for any number of classes
blc(Y, w, maxiter = 25, tol = 1e-06, weights = NULL, verbose = TRUE)
#Performs beta latent class modeling using recursively-partitioned mixture model
blcTree(x, initFunctions = list(blcInitializeSplitFanny()),weight = NULL, index = NULL,
        wthresh = 1e-08, nodename = "root",maxlevel = Inf, verbose = 2, nthresh = 5,
        level = 0, env = NULL,unsplit = NULL, splitCriterion = blcSplitCriterionBIC)

#Fits a Gaussian mixture model for any number of classes
glc(Y, w, maxiter = 100, tol = 1e-06, weights = NULL, verbose = TRUE)
#Performs Gaussian latent class modeling using recursively-partitioned mixture model
glcTree(x, initFunctions = list(glcInitializeSplitFanny(nu=1.5)),weight = NULL,
        index = NULL, wthresh = 1e-08,nodename = "root", maxlevel = Inf, verbose = 2,
        nthresh = 5, level = 0,env = NULL, unsplit = NULL, splitCriterion = glcSplitCriterionBIC)

predict()
```

# RWeka
Weka, Java'da yazılmış, veri ön işlemeye yönelik araçlar içeren, veri madenciliği görevleri için bir makine öğrenme algoritmaları koleksiyonudur.
sınıflandırma, regresyon, kümeleme, birliktelik kuralları ve görselleştirme.
```{r}
library('RWeka')
#Weka_associators R/Weka Associators
Apriori(x, control = NULL)
Tertius(x, control = NULL)

#R interfaces to Weka regression and classification function learners.
LinearRegression(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
Logistic(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
SMO(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)

#R interfaces to Weka lazy learners.
IBk(formula, data, subset, na.action, #k-nn
  control = Weka_control(), options = NULL)
LBR(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)

#R interfaces to Weka meta learners.
AdaBoostM1(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
Bagging(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
LogitBoost(formula, data, subset, na.action,# additive logistic regression
  control = Weka_control(), options = NULL)
MultiBoostAB(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
Stacking(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
CostSensitiveClassifier(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)

#R interfaces to Weka rule learners.
JRip(formula, data, subset, na.action,#"Repeated Incremental Pruning to Produce
                                      #Error Reduction” (RIPPER)
  control = Weka_control(), options = NULL)
M5Rules(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
OneR(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
PART(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)

#R interfaces to Weka regression and classification tree learners.
J48(formula, data, subset, na.action,#unpruned or pruned C4.5 decision trees
  control = Weka_control(), options = NULL)
LMT(formula, data, subset, na.action,#“Logistic Model Trees”
  control = Weka_control(), options = NULL)
M5P(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)
DecisionStump(formula, data, subset, na.action,
  control = Weka_control(), options = NULL)

#R interfaces to Weka clustering algorithms.
Cobweb(x, control = NULL)
FarthestFirst(x, control = NULL)
SimpleKMeans(x, control = NULL)#k-means
XMeans(x, control = NULL)#k-means extended by an “Improve-Structure part”
DBScan(x, control = NULL)

predict()
```

# tree
Sınıflandırma ve Regresyon Ağaçları
```{r}
library('tree')
tree(formula, data, weights, subset,na.action = na.pass, control = tree.control(nobs, ...),
     method = "recursive.partition",split = c("deviance", "gini"),model = FALSE,
     x = FALSE, y = TRUE, wts = TRUE, ...)
predict()
```

# xgboost
Paket, verimli doğrusal model çözücü ve ağaç öğrenme algoritmalarını içerir. Regresyon, sınıflandırma ve sıralama dahil olmak üzere çeşitli amaç işlevlerini destekler.
```{r}
library('xgboost')

#xgb.DMatrix nesnesini oluştur:xgb.train için
xgb.DMatrix(data,info = list(),missing = NA,silent = FALSE,nthread = NULL)

xgboost(data = NULL, #matrix, dgCMatrix, xgb.DMatrix
        label = NULL,missing = NA,weight = NULL,params = list(),nrounds,verbose = 1,
        print_every_n = 1L,early_stopping_rounds = NULL,maximize = NULL,
        save_period = NULL,save_name = "xgboost.model",xgb_model = NULL,
        callbacks = list(),...)

predict()
```


# ilaveler - unutulanlar

# VGAM
Yaklaşık 6 ana sınıfın bir uygulaması İstatistiksel regresyon modelleri. Merkezi algoritma Fisher puanlaması ve yinelemeli yeniden ağırlıklandırılmış en küçük kareler.
vektör genelleştirilmiş doğrusal ve toplamsal modelleri (VGLM'ler ve VGAM'ler), maksimum olasılık tahmini (MLE), ekolojideki kısıtlı ikinci dereceden koordinasyon (CQO)
```{r}
library('VGAM')
#vektör genelleştirilmiş toplam modeli (VGAM)
vgam(formula,family = stop("argument 'family' needs to be assigned"),data = list(),
     weights = NULL, subset = NULL,na.action = na.fail, etastart = NULL, mustart = NULL,
     coefstart = NULL, control = vgam.control(...),offset = NULL, method = "vgam.fit",
     model = FALSE,x.arg = TRUE, y.arg = TRUE, contrasts = NULL,constraints = NULL,
     extra = list(), form2 = NULL,qr.arg = FALSE, smart = TRUE, ...)

#İndirgenmiş dereceli vektör genelleştirilmiş doğrusal model (RR-VGLM)
rrvglm(formula, family = stop("argument 'family' needs to be assigned"),data = list(),
       weights = NULL, subset = NULL,na.action = na.fail, etastart = NULL, mustart = NULL,
       coefstart = NULL, control = rrvglm.control(...), offset = NULL,
       method = "rrvglm.fit", model = FALSE, x.arg = TRUE, y.arg = TRUE,contrasts = NULL,
       constraints = NULL, extra = NULL,qr.arg = FALSE, smart = TRUE, ...)

#vektör genelleştirilmiş doğrusal modeller (VGLM)
vglm(formula,family = stop("argument 'family' needs to be assigned"),data = list(),
     weights = NULL, subset = NULL,na.action = na.fail, etastart = NULL, mustart = NULL,
     coefstart = NULL, control = vglm.control(...), offset = NULL,method = "vglm.fit",
     model = FALSE, x.arg = TRUE, y.arg = TRUE,contrasts = NULL, constraints = NULL,
     extra = list(),form2 = NULL, qr.arg = TRUE, smart = TRUE, ...)

#Kısıtlı bir ikinci dereceden koordinasyon (CQO; daha önce kanonik Gauss koordinasyonu veya CGO olarak adlandırılıyordu) modeli, ikinci dereceden indirgenmiş sıralı vektör genelleştirilmiş doğrusal model (QRR-VGLM) çerçevesi kullanılarak takılır.
cqo(formula, family = stop("argument 'family' needs to be assigned"),data = list(),
    weights = NULL, subset = NULL,na.action = na.fail, etastart = NULL, mustart = NULL,
    coefstart = NULL, control = qrrvglm.control(...), offset = NULL,method = "cqo.fit",
    model = FALSE, x.arg = TRUE, y.arg = TRUE,contrasts = NULL, constraints = NULL,
    extra = NULL,smart = TRUE, ...)

predictvglm(object, newdata = NULL,type = c("link", "response", "terms"),
            se.fit = FALSE, deriv = 0, dispersion = NULL,untransform = FALSE,
            type.fitted = NULL, percentiles = NULL, ...)
```

# mda
Karışım ve esnek diskriminant analizi, çok değişkenli uyarlamalı regresyon eğrileri (MARS), BRUTO ve vektör tepkisi yumuşatma eğrileri.
```{r}
library(mda)
#bruto Fit an Additive Spline Model by Adaptive Backfitting
bruto(x, y, w, wp, dfmax, cost, maxit.select, maxit.backfit,
thresh = 0.0001, trace.bruto = FALSE, start.linear = TRUE,
fit.object, ...)

#fda Flexible Discriminant Analysis
fda(formula, data, weights, theta, dimension, eps, method,
keep.fitted, ...)

#gen.ridge Penalized Regression
gen.ridge(x, y, weights, lambda=1, omega, df, ...)

#mars Multivariate Adaptive Regression Splines
mars(x, y, w, wp, degree, nk, penalty, thresh, prune, trace.mars,
forward.step, prevfit, ...)

#mda Mixture Discriminant Analysis
mda(formula, data, subclasses, sub.df, tot.df, dimension, eps,
iter, weights, method, keep.fitted, trace, ...)

predict()
```


# mboost
Genelleştirilmiş doğrusal, toplamsal ve etkileşim modellerini potansiyel olarak yüksek boyutlu verilere uydurmak için temel öğreniciler olarak bileşen bazında (cezalandırılmış) en küçük kareler tahminlerini veya regresyon ağaçlarını kullanan genel risk fonksiyonlarını optimize etmeye yönelik fonksiyonel gradyan iniş algoritması (artırma).
```{r}
library(mboost)

# gamboost for boosted (generalized) additive models,
mboost(formula, data = list(), na.action = na.omit, weights = NULL,
offset = NULL, family = Gaussian(), control = boost_control(),
oobweights = NULL, baselearner = c("bbs", "bols", "btree", "bss", "bns"),
...)
gamboost(formula, data = list(), na.action = na.omit, weights = NULL,
offset = NULL, family = Gaussian(), control = boost_control(),
oobweights = NULL, baselearner = c("bbs", "bols", "btree", "bss", "bns"),
dfbase = 4, ...)

# glmboost for boosted linear models
glmboost(formula, data = list(), weights = NULL,
offset = NULL, family = Gaussian(),
na.action = na.pass, contrasts.arg = NULL,
center = TRUE, control = boost_control(), oobweights = NULL, ...)

# blackboost for boosted trees.
blackboost(formula, data = list(),
weights = NULL, na.action = na.pass,
offset = NULL, family = Gaussian(),
control = boost_control(),
oobweights = NULL,
tree_controls = partykit::ctree_control(
teststat = "quad",
testtype = "Teststatistic",
mincriterion = 0,
minsplit = 10,
minbucket = 4,
maxdepth = 2,
saveinfo = FALSE),
...)

predict(object, newdata = NULL,
type = c("link", "response", "class"), which = NULL,
aggregate = c("sum", "cumsum", "none"), ...)
```



# ******************************ek araçlar********************


#* BDgraph
Sürekli, sıralı/ayrık/sayılı ve karma veriler için yönlendirilmemiş grafik modellerde Bayes yapısının öğrenimine yönelik istatistiksel araçlar.
```{r}
library('BDgraph')
bdgraph( data, n = NULL, method = "ggm", #gcgm
         algorithm = "bdmcmc", #rjmcmc
         iter = 5000, burnin = iter / 2, not.cont = NULL, g.prior = 0.2, df.prior = 3,
         g.start = "empty", jump = NULL, save = FALSE,cores = NULL,
         threshold = 1e-8, verbose = TRUE, nu = 1 )

# bdgraph.dw :Search algorithm for Gaussian copula graphical models for count data
# bdgraph.mpl :Search algorithm in graphical models using marginal pseudo likehlihood
# bdgraph.npn :Nonparametric transfer
# bdw.reg :Bayesian estimation of (zero-inflated) Discrete Weibull regression
```

#* Boruta
öznitellik seçimi sarmalayıcı algoritması.
Orijinal niteliklerin önemini rastgele elde edilebilen önemle karşılaştırarak, bunların değiştirilmiş kopyaları (shado) kullanılarak tahmin edilerek ilgili özellikleri bulur.
```{r}
library('Boruta')
Boruta(x,y,pValue = 0.01,mcAdj = TRUE,maxRuns = 100,doTrace = 0,holdHistory = TRUE,
       getImp = getImpRfZ,...)

```


# DALEX
DALEX paketi, girdi değişkenleri ile model çıktısı arasındaki bağlantıyı anlamaya yardımcı olan çeşitli yöntemler içerir. Uygulanan yöntemler, modeli tek bir örnek düzeyinde ve tüm veri kümesi düzeyinde keşfetmeye yardımcı olur.
```{r}
library('DALEX')
explain(model,data = NULL,y = NULL,predict_function = NULL,
        predict_function_target_column = NULL,residual_function = NULL,
        weights = NULL,...,label = NULL,verbose = TRUE,precalculate = TRUE,
        colorize = !isTRUE(getOption("knitr.in.progress")),model_info = NULL,
        type = NULL)

model_prediction(explainer, new_data, ...)

model_info(model, is_multiclass = FALSE, ...)
```
models:
• class cv.glmnet and glmnet - models created with glmnet package
• class glm - generalized linear models
• class lrm - models created with rms package,
• class model_fit - models created with parsnip package
• class lm - linear models created with stats::lm
• class ranger - models created with ranger package
• class randomForest - random forest models created with randomForest package
• class svm - support vector machines models created with the e1071 package
• class train - models created with caret package
• class gbm - models created with gbm package


# GMMBoost
Bu paket, Genelleştirilmiş karma modeller için olasılığa dayalı güçlendirme yaklaşımları sağlar
```{r}
library('GMMBoost')
#bGAMM :Fit Generalized Semiparametric Mixed-Effects Models
bGAMM(fix=formula, add=formula, rnd=formula,data, lambda, family = NULL, control = list())
#bGLMM :Fit Generalized Mixed-Effects Models
bGLMM(fix=formula, rnd=formula, data, family = NULL, control = list())
#OrdinalBoost :Fit Generalized Mixed-Effects Models
OrdinalBoost(fix=formula, rnd=formula, data,model="sequential",control=list())

#NO PREDİCTİON
```
