---
title: "ML Packages"
output:
  pdf_document: default
  html_document: default
date: "2023-12-05"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Makine öğrenimi alanındaki kütüphaneler irdelenmiştir.

İçinde barındırdığı fonksiyonu en fazla olana göre sıralanmıştır.

Veri 1: Beton Basınç Dayanımı Veri Seti(regresyon, randomforest,
decision tree) Beton basınç dayanımı, yaş ve bileşenlerin oldukça
doğrusal olmayan bir fonksiyonudur. Örnek sayısı 1030, Öznitelik Sayısı
9. Öznitelik dökümü 8 nicel girdi değişkeni ve 1 nicel çıktı değişkeni.

İsim -- Veri Türü -- Ölçüm -- Açıklama

-   Çimento (bileşen 1) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişken Yüksek Fırın

-   Cürufu (bileşen 2) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişken

-   Uçucu Kül (bileşen 3) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişken

-   Su (bileşen 4) -- kantitatif -- m3 karışımında kg -- Giriş Değişkeni

-   Süperakışkanlaştırıcı (bileşen 5) -- kantitatif -- m3 karışımında kg
    -- Giriş Değişkeni

-   Kaba Agrega (bileşen 6) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişkeni

-   İnce Agrega (bileşen 7) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişkeni

-   Yaş -- kantitatif -- Gün (1\~365) -- Giriş Değişkeni

-   Beton basınç dayanımı -- kantitatif -- MPa -- Çıkış Değişkeni

    <https://www.kaggle.com/datasets/maajdl/yeh-concret-data/download?datasetVersionNumber=1>

Veri 2: Yetişkin bireylerin gelir düzeylerine yönelik anket bilgilerini
içerir. arules paketi AdultUCI veri seti, 48842 ile bir veri çerçevesi
içerir. Aşağıdaki 15 değişken üzerinde gözlemler.

-   age : yaş - sayısal bir vektör.

-   workclass : işçi sınıfı - düzeyleri olan bir faktör

-   education : eğitim - düzeyleri olan sıralı bir faktör

-   education-num : eğitim-num - Sayısal bir vektör.

-   marital-status : medeni durum - ile bir faktör düzeyleri

-   occupation :meslek - düzeyleri olan bir faktör

-   relationship : ilişki - düzeyleri olan bir faktör

-   race : ırk - bir faktör düzeyleri ile

-   sex : cinsiyet - düzeyleri olan bir faktör Female/Male

-   capital-gain : sermaye kazancı - Sayısal bir vektör.

-   capital-loss : sermaye kaybı - Sayısal bir vektör.

-   fnlwgt : Sayısal bir vektör.

-   hours-per-week : haftalık saat - Sayısal bir vektör.

-   native-country : anavatanı

-   income : gelir - sıralı bir faktör seviyeleri. small\<large

```         
#read.table('<https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data>'
```

Vei 3: Bu veride 167 ülkenin 9 farklı sosyo ekonomik bilgilerine
yerverilmiştir.

country : Ülkenin adı

child_mort : 1000 canlı doğumda 5 yaş altı çocuk ölümü

exports : Kişi başına mal ve hizmet ihracatı. Kişi başına düşen
GSYİH'nin yüzdesi olarak verilmiştir

health : Kişi başına düşen toplam sağlık harcaması. Kişi başına düşen
GSYİH'nin yüzdesi olarak verilmiştir

imports : Kişi başına mal ve hizmet ithalatı. Kişi başına düşen
GSYİH'nin yüzdesi olarak verilmiştir

Income : Kişi başına net gelir

Inflation : Toplam GSYİH'nın yıllık büyüme oranının ölçümü

life_expec : Mevcut ölüm oranlarının aynı kalması durumunda yeni doğan
bir çocuğun yaşayacağı ortalama yıl sayısı

total_fer : Mevcut yaş-doğurganlık oranları aynı kalırsa her kadının
doğacak çocuk sayısı gdpp : Kişi başına düşen GSYİH. Toplam GSYİH'nın
toplam nüfusa bölünmesiyle hesaplanır.

\#<https://rpubs.com/adellop/731487>

veri 4:lung

inst: Kurum kodu

time: Gün cinsinden hayatta kalma süresi

status: sansürleme durumu 1=sansürlendi, 2=ölü

age: Yıl olarak yaş

sex: Erkek=1 Kadın=2

ph.ecog: Doktor tarafından derecelendirilen ECOG performans puanı. 0=asemptomatik, 1=semptomatik ancak tamamen ayaktan, 2= günün <%50'si yatakta, 3= günün >%50'si yatakta ama yatağa bağımlı değil, 4= yatağa bağlı

ph.karno: Doktor tarafından derecelendirilen Karnofsky performans puanı (kötü=0-iyi=100)

pat.karno: Hasta tarafından derecelendirilen Karnofsky performans puanı (0 = kötü, 100 = iyi)

food.cal: Yemeklerde tüketilen kaloriler

wt.loss: Son altı ayda kilo kaybı

```{r, datasets}
beton<-read.csv(unzip("Concrete.zip","Concrete_Data_Yeh.csv"))##regression
adult<-read.csv('adult.csv', sep = ',', fill = F, strip.white = T,
                col.names= c('age', 'workclass','fnlwgt', 'educatoin',
                             'educatoin_num', 'marital_status', 'occupation',
                             'relationship', 'race', 'sex', 'capital_gain',
                             'capital_loss', 'hours_per_week', 'native_country',
                             'income'),
                stringsAsFactors = T
                )##classification
life <- read.csv(unzip("archive.zip","Country-data.csv")) ##clustiring

lung<-survival::lung#data(cancer, package = 'survival')
lung<-na.omit(lung)

#-----data manipulation-----
sapply(adult,function(x)levels(x))
adult<-adult[,-c(3,4)]#clear id column
adult <- replace(adult,adult=="?",NA)#replace ? to NA
adult <- droplevels(adult)#update levels

#____impute NA cells___
adlt1<-missForest::missForest(adult, maxiter = 3, ntree = 100)$ximp#PROFF

adlt<-adult
i<-c(2,4:8,12:13) #categorical columns to numeric value for e1071
adlt[ , i] <- apply(adlt[ , i], 2, function(x) as.numeric(factor(x)))
adlt <- e1071::impute(adlt, what = "median")#BASIC
adlt<-data.frame(adlt)

indx<-sample(nrow(adlt), round(0.7*nrow(adlt)))
train_adlt<-adlt[indx, ]
test_adlt<-adlt[-indx, ]
rm(i,indx)
```

kontrol et; formula(model.frame(income\~.,data=train_adlt))

# 1. mlpack

C++ ile yazılmış, en ileri makine öğrenimi algoritmalarının hızlı,
genişletilebilir uygulamalarını sağlamayı amaçlayan hızlı, esnek bir
makine öğrenimi kitaplığı. 33 methods

\#<https://cran.r-project.org/web/packages/mlpack/mlpack.pdf>
\#<https://www.mlpack.org/doc/stable/r_documentation.html#gmm_train>

```{r}
library('mlpack')

#Verileri eğitim ve test veri kümesine etiketiyle birlikte bölme
adlt1 <- preprocess_split(adlt[,-13], input_labels =  data.frame((adlt$income)),
  no_shuffle = FALSE, seed = NA, test_ratio = .25, stratify_data = FALSE,
  verbose = FALSE)

#Veriyi ölçeklendirme ve bölme
btn<-data.frame(scale(beton,F,F))
btn <- preprocess_split(btn[,-9], input_labels = data.frame(btn$csMPa),
                        test_ratio = .25)

#sayısal verileri al
life <- life[,-1]
```

list2env(btn, .GlobalEnv) : veri kolonlarını ortam değişkeni olarak atar

```{r}
#classification************************sınıflandırma
#etiketli veriler üzerinde bir AdaBoost modelini eğitmek veya yeni noktaların sınıflarını tahmin etmek için mevcut bir AdaBoost modelini kullanmak için kullanılabilir
c1 <- adaboost(input_model = NA, iterations = 1000,
               labels = adlt1$training_labels, test = NA , tolerance = 1e-10,
               training = adlt1$training, verbose = T,
               weak_learner = "decision_stump")
prd<- adaboost(input_model = c1$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd)

#Sayısal veya kategorik özelliklere sahip etiketli veriler verildiğinde, bir karar ağacı eğitilebilir ve kaydedilebilir; veya yeni noktalara göre sınıflandırma yapmak için mevcut bir karar ağacı kullanılabilir.
c2 <- decision_tree(input_model = NA, labels = adlt1$training_labels,
                    maximum_depth = 0, minimum_gain_split = 1e-07,
                    minimum_leaf_size = 20, print_training_accuracy = FALSE,
                    print_training_error = FALSE, test = NA, test_labels = NA,
                    training = adlt1$training, verbose = FALSE, weights = NA)
prd<- decision_tree(input_model = c2$output_model, test =adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd)

#Çok sınıflı sınıflandırma için doğrusal SVM'nin bir uygulaması. Etiketli veriler verildiğinde, bir model gelecekte kullanılmak üzere eğitilip kaydedilebilir; veya yeni noktaları sınıflandırmak için önceden eğitilmiş bir model kullanılabilir.
c3 <-linear_svm(delta = 1, epochs = 50, input_model = NA,
                labels = adlt1$training_labels, lambda = 0.0001,
                max_iterations = 10000, no_intercept = FALSE, num_classes = 0,
                optimizer = 'lbfgs', seed = 0, shuffle = FALSE,step_size = 0.01,
                test = NA, test_labels = NA, tolerance = 1e-10,
                training = adlt1$training, verbose = FALSE)
prd<- linear_svm(input_model = c3$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd)

#İki sınıflı sınıflandırma için L2-düzenlenmiş lojistik regresyonun bir uygulaması. Etiketli veriler verildiğinde, bir model gelecekte kullanılmak üzere eğitilip kaydedilebilir; veya yeni noktaları sınıflandırmak için önceden eğitilmiş bir model kullanılabilir.
c4<-logistic_regression(batch_size = 64, decision_boundary = 0.5,
                        input_model = NA, labels = adlt1$training_labels,
                        lambda = 0, max_iterations = 10000, optimizer = 'lbfgs',
                        step_size = 0.01, test = NA, tolerance = 1e-10,
                        training = adlt1$training, verbose = FALSE)
prd <- logistic_regression(input_model = c4$output_model,
                           test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd)

#Sınıflandırma için kullanılan Naive Bayes Sınıflandırıcının bir uygulaması. Etiketli veriler verildiğinde, bir NBC modeli eğitilip kaydedilebilir veya sınıflandırma için önceden eğitilmiş bir model kullanılabilir.
c5 <- nbc(incremental_variance = FALSE, input_model = NA, verbose = FALSE,
        labels = adlt1$training_labels, test = NA, training = adlt1$training)
prd <- nbc(input_model = c5$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd)

#Sınıflandırma için bir algılayıcının (tek seviyeli bir sinir ağı) uygulanması. Etiketli veriler verildiğinde, bir algılayıcı gelecekte kullanılmak üzere eğitilip kaydedilebilir; veya yeni noktalara göre sınıflandırma yapmak için önceden eğitilmiş bir algılayıcı kullanılabilir.
c6<-perceptron(input_model = NA, labels = adlt1$training_labels,verbose = FALSE,
               max_iterations = 1000, test = NA, training = adlt1$training)
prd<-perceptron(input_model = c6$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd)

#Leo Breiman'ın sınıflandırma için standart rastgele orman algoritmasının bir uygulaması. Etiketli veriler verildiğinde, rastgele bir orman eğitilip gelecekte kullanılmak üzere kaydedilebilir; veya sınıflandırma için önceden eğitilmiş bir rastgele orman kullanılabilir.
c7<-random_forest(input_model = NA, labels = adlt1$training_labels,
                  maximum_depth = 0, minimum_gain_split = 0,
                  minimum_leaf_size = 1, num_trees = 10,
                  print_training_accuracy = FALSE, seed = 2024,
                  subspace_dim = 0, test = NA, test_labels = NA,
                  training = adlt1$training,verbose = FALSE, warm_start = FALSE)
prd<-random_forest(input_model = c7$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd)

#Lojistik regresyonun çok sınıflı bir genellemesi olan sınıflandırma için softmax regresyonunun bir uygulaması
c8<-softmax_regression(input_model = NA, labels = adlt1$training_labels,
                       lambda = 0.0001,max_iterations = 10,no_intercept = FALSE,
                       number_of_classes = 0, test = NA, test_labels = NA,
                       training = scale(adlt1$training,center = F),
                       verbose = FALSE)

prd<- softmax_regression(input_model = c8$output_model,
                        test = scale(adlt1$test,center = F))$predictions
Metrics::accuracy(adlt1$test_labels, prd)


#REGRESSION*************************regresyon
#Bayes doğrusal regresyonunun bir uygulaması
r1 <- bayesian_linear_regression(center = FALSE,input = btn$training,
                           input_model = NA, responses = btn$training_labels,
                           scale = FALSE, test = btn$test, verbose = FALSE)
prd<- bayesian_linear_regression(input_model = r1$output_model, test = btn$test)
Metrics::mae(btn$test_labels, prd$predictions)


#LARS olarak da bilinen En Küçük Açı Regresyonunun (Aşamalı/laSso) bir uygulaması. Bu, bir LARS/LASSO/Elastic Net modelini eğitebilir ve bir test seti için regresyon tahminlerinin çıktısını almak üzere bu modeli veya önceden eğitilmiş bir modeli kullanabilir.
r2<-lars(input = btn$training, input_model = NA, lambda1 = 0, lambda2 = 0,
         no_intercept = FALSE,no_normalize =FALSE,responses=btn$training_labels,
         test = NA, use_cholesky = FALSE, verbose = FALSE)
prd<-lars(input_model = r2$output_model, test = btn$test)
Metrics::mae(btn$test_labels, prd$output_predictions[,1])


#Sıradan en küçük kareler kullanılarak basit doğrusal regresyon ve sırt regresyonunun bir uygulaması. önceden eğitilmiş bir model, bir test seti için regresyon tahminlerinin çıktısını almak üzere kullanılabilir
r3<-linear_regression(input_model = NA, lambda = 0, test = NA,
                      training = btn$training,
                      training_responses = btn$training_labels,
                      verbose = FALSE)
prd<-linear_regression(input_model = r3$output_model,
                       test = btn$test)
Metrics::mae(btn$test_labels, prd$output_predictions[,1])



##clustering****************kümeleme
#Bir veri kümesi verildiğinde, bu, söz konusu veri kümesinin kümelenmesini hesaplayabilir ve döndürebilir.
clu1 <- dbscan(input=life, epsilon = 1, min_size = 5, naive = TRUE,
             selection_type = 'ordered', single_mode = TRUE,
             tree_type = "kd", verbose = FALSE)

#Gauss karışım modellerinin (GMM'ler) eğitimi için EM algoritmasının bir uygulaması. Bir veri kümesi göz önüne alındığında, bu, GMM'yi gelecekte diğer araçlarla kullanmak üzere eğitebilir
clu2<-gmm_train(gaussians=10, input=life, diagonal_covariance = FALSE,
                input_model = NA, kmeans_max_iterations = 1000,
                max_iterations = 250, no_force_positive = FALSE, noise = 5,
                percentage = 0.02, refined_start = TRUE, samplings = 100,
                seed = 0, tolerance = 1e-10, trials = 1, verbose = FALSE)

#Bir veri kümesi ve bir k değeri verildiğinde, bu, söz konusu veriler üzerinde bir k-ortalama kümelemesini hesaplar ve döndürür.
clu3<-kmeans(clusters=5, input=life, algorithm = 'naive',
             allow_empty_clusters = FALSE, in_place = FALSE,
             initial_centroids = NA, kill_empty_clusters = FALSE,
             kmeans_plus_plus = FALSE, labels_only = FALSE,
             max_iterations = 1000, percentage = 0.02, refined_start = FALSE,
             samplings = 10, seed = 0, verbose = FALSE)

#İkili ağaç aralık aramasını kullanarak ortalama kaydırmalı kümelemenin hızlı bir uygulaması
clu4<-mean_shift(input=life, force_convergence = FALSE, in_place = FALSE,
                 labels_only = FALSE, max_iterations = 1000, radius = 0,
                 verbose = FALSE)

#Önceden eğitilmiş GMM'ler için örnek oluşturucu
clu5<-gmm_generate(input_model=clu2$output_model, samples=9, seed = 0,
                   verbose = FALSE)

#GMM'ler için bir olasılık hesaplayıcısı. Önceden eğitilmiş bir GMM ve bir dizi nokta göz önüne alındığında, bu, her bir noktanın verilen GMM'den olma olasılığını hesaplayabilir.
clu6<-gmm_probability(input=clu5$output, input_model=clu2$output_model,
                      verbose = FALSE)

#Sınıflandırmaya yönelik bir akış karar ağacı biçimi olan Hoeffding ağaçlarının bir uygulaması.
clu7<-hoeffding_tree(batch_mode = FALSE, bins = 10, confidence = .95,
                     info_gain = FALSE, input_model = NA, labels = NA,
                     max_samples = 150, min_samples = 10,
                     numeric_split_strategy = 'domingos',
                     observations_before_binning = 50, passes = 1, test = NA,
                     test_labels = NA, training = life, verbose = FALSE)



##AĞAÇ*********************
#Yaklaşık en uzak komşu araması -UYGUN VERİ YOK-
#ag1<-approx_kfn(algorithm = "ds", calculate_error =FALSE, exact_distances = NA,
#           input_model = NA, k = 0, num_projections = 5, num_tables = 5,
#           query = NA, reference = NA, verbose = FALSE)

#Hızlı Öklid Minimum Yayılan Ağaç
ag2<-emst(input=beton, leaf_size = 1, naive = FALSE, verbose = FALSE)

#Tek ağaçlı ve çift ağaçlı hızlı maksimum çekirdek arama (FastMKS) algoritmasının bir uygulaması.
ag3<-fastmks(bandwidth = 1,base = 2,degree = 2,input_model = NA,k = 10,
        kernel = "linear",naive = FALSE,offset = 0,query = NA,
        reference = btn$training, scale = 1, single = FALSE,verbose = FALSE)

#Yerelliğe duyarlı karma (LSH) ile yaklaşık k-en yakın komşu aramasının bir uygulaması
ag4<-lsh(bucket_size = NA,hash_width = NA,input_model = NA,k = 10,
         num_probes = NA, projections = NA,query = NA,reference = btn$training,
         second_hash_size = NA, seed = NA,tables = NA,true_neighbors = NA,
         verbose = FALSE)

#Tek ağaç ve çift ağaç algoritmaları kullanan k-en yakın komşu aramasının bir uygulaması
ag5<-knn(algorithm = "dual-tree",epsilon = 0,input_model = NA,k = 10,
         leaf_size = 20,query = NA, random_basis = FALSE,
         reference = btn$training,rho = .7,seed = 0,tau = 0,tree_type = "kd",
         true_distances = NA,true_neighbors = NA,verbose = FALSE)

#Tek ağaçlı ve çift ağaçlı algoritmaları kullanan k-en uzak komşu aramasının bir uygulaması.
ag6<-kfn(algorithm = "dual_tree",epsilon = 0,input_model = NA,k = 10,
         leaf_size = 20, percentage = 1,query = NA,random_basis = FALSE,
         reference = btn$training,seed = 0,tree_type = "kd",true_distances = NA,
         true_neighbors = NA, verbose = FALSE)

#Tek ağaç ve çift ağaç algoritmalarını kullanan, yaklaşık k-en yakın komşu aramasının (kRANN) bir uygulaması.
ag7<-krann(alpha = .95,first_leaf_exact = FALSE,input_model = NA,k = 10,
      leaf_size = 20,naive = FALSE,query = NA,random_basis = FALSE,
      reference = btn$training,sample_at_leaves = FALSE,seed = 0,
      single_mode = FALSE,single_sample_limit = 20,tau = 5,
      tree_type = "kd",verbose = FALSE)



#transformation***********************dönüşüm/boyut azaltma
#Çekirdek Temel Bileşenler Analizi(KPCA), belirli bir veri kümesinde doğrusal olmayan boyutluluk azaltma veya ön işleme gerçekleştirmek için
tr1<-kernel_pca(input=cbind(matrix(unlist(adlt1$test), ncol = 12, byrow = TRUE),
                            matrix(unlist(adlt1$test_labels), ncol = 1,
                                   byrow = TRUE)),
                kernel='linear', bandwidth = 1, center = FALSE, degree = 1,
                kernel_scale = 1, new_dimensionality = 0,
                nystroem_method = FALSE, offset = 0, sampling = "kmeans",
                verbose = FALSE)
#kernel=gaussian,polynomial,hyptan,laplacian, epanechnikov,cosine

#Bir uzaktan öğrenme tekniği olan Büyük Marjlı En Yakın Komşuların (LMNN) bir uygulaması. Etiketli bir veri kümesi verildiğinde, bu, k-en yakın komşunun performansını artıran veri dönüşümünü öğrenir; bu bir ön işleme adımı olarak faydalı olabilir.
tr2<-lmnn(input=adlt1$training, batch_size = 50, center = FALSE,
          distance = matrix(integer(), 0, 0), k = 1,
          labels = adlt1$training_labels, linear_scan = FALSE,
          max_iterations = 10000, normalize = FALSE, optimizer = "amsgrad",
          passes = 50, print_accuracy = FALSE, range = 1, rank = 0,
          regularization = 0.5, seed = 0, step_size = 0.01, tolerance = 1e-07,
          verbose = FALSE)

#Ön işleme için kullanılabilecek bir uzaktan öğrenme tekniği olan komşuluk bileşenleri analizinin bir uygulaması.Etiketli bir veri kümesi göz önüne alındığında, bu, k-en yakın komşu sınıflandırmasını iyileştirmeyi amaçlar
tr3<-nca(input=adlt1$training, armijo_constant = 0.0001, batch_size = 50,
         labels = adlt1$training_labels, linear_scan = FALSE,
         max_iterations = 500000, max_line_search_trials = 50, max_step = 1e+20,
         min_step = 1e-20, normalize = FALSE, num_basis = 5, optimizer = "sgd",
         seed = 0, step_size = 0.01, tolerance = 1e-07, verbose = FALSE,
         wolfe = 0.9)

#Bir veri kümesi ve istenen yeni boyutluluğa göre PCA tarafından belirlenen doğrusal dönüşümü kullanarak verinin boyutluluğunu azaltabilir.
tr4<-pca(input=adlt1$training, decomposition_method = "exact",
         new_dimensionality = 10, scale = FALSE, var_to_retain = 1,
         verbose = FALSE)

#bağımsız bileşen analizi (ICA) için bir yöntem
tr5<-radical(input=adlt1$training, angles = 150, noise_std_dev = 0.175,
             objective = FALSE, replicates = 10, seed = 0, sweeps = 0,
             verbose = FALSE)

#Sözlük Öğrenimi ile Seyrek Kodlamanın bir uygulaması
tr6<-sparse_coding(atoms = 9,initial_dictionary = NA,input_model = NA,
                   lambda1 = 0, lambda2 = 0, max_iterations = 0,
                   newton_tolerance = 1e-06, normalize = FALSE,
                   objective_tolerance = 0.01,seed = 0,test = NA,
                   training = scale(life),verbose = FALSE)
```

SORUNLU FONKSİYONLAR

```         
# DİĞER***************************
#Tavsiye sistemleri için çeşitli işbirlikçi filtreleme (CF) tekniklerinin uygulanması.
cf(algorithm = "NMF",all_user_recommendations = FALSE,input_model = NA,
   interpolation = "average",iteration_only_termination = FALSE,
   max_iterations = 1000, min_residue = 1e-05,neighbor_search = "euclidean",
   neighborhood = 5,normalization = "none", query = NA,rank = 0,
   recommendations = 5,seed = 0,test = NA,training = life, verbose = FALSE)

#Yoğunluk Tahmin Ağaçları ile Yoğunluk Tahmini
det(folds = 10,input_model = NA,max_leaf_size = 10,min_leaf_size = 5,
    path_format = "lr",skip_pruning = FALSE,test = NA,training = life,
    verbose = FALSE)

#------------ÇALIŞMIYORLAR----
hava<-read.csv("golf_train.csv", fileEncoding = "latin5",check.names = F)[,-1]
hava1<-hava[,1:2]
hava1[,1]<-as.numeric(factor(hava1[,1],levels = c("soğuk","orta","sıcak")))
hava1[,2]<-as.numeric(factor(hava1[,2],levels = c("güneşli","bulutlu","yağmurlu")))
#Gizli Markov Modelleri (HMM'ler) için eğitim algoritmalarının uygulanması
mdl<-hmm_train(input_file=hava, batch = FALSE, gaussians = 2,
          input_model = NA, labels_file = "", seed = 0, states = 0,
          tolerance = 1e-05, type =list("gaussisn"), verbose = FALSE)

#Önceden eğitilmiş bir Gizli Markov Modelinden (HMM) rastgele diziler oluşturmaya yönelik bir yardımcı program. İstenilen dizinin uzunluğu belirtilebilir ve rastgele bir gözlem dizisi döndürülür.
hmm_generate(length, model=mdl, seed = NA, start_state = NA, verbose = FALSE)

#Gizli Markov Modelleri (HMM'ler) için bir dizinin log olasılığını hesaplamaya yönelik bir yardımcı program
hmm_input<-hmm_loglik(input=hava, input_model="mdl", verbose = FALSE)

#Gizli Markov Modelleri (HMM'ler) için en olası gizli durum sırasını hesaplamaya yönelik bir yardımcı program.
hmm_viterbi(input=hava, input_model="mdl", verbose = FALSE)
#-----------

#Çift ağaç algoritmalarıyla çekirdek yoğunluğu tahmininin bir uygulaması.
kde(abs_error = 0,algorithm = "dual-tree",bandwidth = 1,
    initial_sample_size = 100, input_model = NA,kernel = "gaussian",
    mc_break_coef = .4,mc_entry_coef = 3, mc_probability = .95,
    monte_carlo = FALSE,query = NA,reference = NA, rel_error = NA,tree = NA,
    verbose = FALSE)

#bir giriş veri kümesini iki düşük dereceli, negatif olmayan bileşene ayırmak için kullanılabilir.
nmf(input,rank,initial_h = NA,initial_w = NA,max_iterations = NA,
    min_residue = NA, seed = NA,update_rules = NA,verbose = FALSE)
    #Non-negative Matrix Factorization
```


# 2. RWeka

Weka, Java'da yazılmış, veri ön işlemeye yönelik araçlar içeren, veri
madenciliği görevleri için bir makine öğrenme algoritmaları
koleksiyonudur. sınıflandırma, regresyon, kümeleme, birliktelik
kuralları ve görselleştirme.

```{r}
library('RWeka')

#R interfaces to Weka regression and classification function learners.
RW1<-LinearRegression(formula=csMPa~., data=beton, subset=NULL,
  na.action=na.omit, control = Weka_control(), options = NULL)
RW2<-Logistic(formula=income~., data=adult, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)
RW3<-SMO(formula=Species~., data=iris, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)

#R interfaces to Weka lazy learners.k-nn
RW4<-IBk(formula=income~., data=train_adlt,subset=NULL,na.action=na.omit,
  control = Weka_control(), options = NULL)
##LBR(formula=income~., data=train_adlt, subset=NULL, na.action=na.omit,
  ##control = Weka_control(), options = NULL)

#R interfaces to Weka meta learners.
RW5<-AdaBoostM1(formula=income~., data=adlt, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)
RW6<-Bagging(formula=income~., data=adult, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)
RW7<-LogitBoost(formula=income~., data=adult, subset=NULL, na.action=na.omit,# additive logistic regression
  control = Weka_control(), options = NULL)
#MultiBoostAB(formula=income~., data=adult, subset=, na.action=na.omit,
#  control = Weka_control(), options = NULL)
RW8<-Stacking(formula=income~., data=adult, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)
#CostSensitiveClassifier(formula=income~., data=adult, subset= NULL,
# na.action=na.omit, control = Weka_control(), options = NULL)

#R interfaces to Weka rule learners.
RW9<-JRip(formula=income~., data=adult, subset=NULL, na.action=na.omit,#"Repeated Incremental Pruning to Produce
                                      #Error Reduction” (RIPPER)
  control = Weka_control(), options = NULL)
RW10<-M5Rules(formula=csMPa~., data=beton, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)
RW11<-OneR(formula=income~., data=adult, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)
RW12<-PART(formula=income~., data=adult, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)

#R interfaces to Weka regression and classification tree learners.
RW13<-J48(formula=income~., data=adult, subset=NULL, na.action=na.omit,#unpruned or pruned C4.5 decision trees
  control = Weka_control(), options = NULL)
RW14<-LMT(formula=income~., data=adult, subset=NULL, na.action=na.omit,#“Logistic Model Trees”
  control = Weka_control(), options = NULL)
RW15<-M5P(formula=income~., data=train_adlt, subset=NULL,
    na.action=na.omit, control = Weka_control(R=TRUE), options = NULL)
RW16<-DecisionStump(formula=income~., data=train_adlt, subset=NULL,
              na.action=na.omit, control = Weka_control(), options = NULL)

Metrics::mae(actual = beton[1:25,9], predict(RW1,beton[1:25,-9]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW2,adult[1:150,-13]))
Metrics::accuracy(actual = iris[1:25,5], predict(RW3,iris[1:25,-5]))
Metrics::accuracy(actual = test_adlt[,13], predict(RW4,test_adlt[,-13]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW5,adult[1:150,-13]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW6,adult[1:150,-13]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW7,adult[1:150,-13]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW8,adult[1:150,-13]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW9,adult[1:150,-13]))
Metrics::mae(actual = beton[1:25,9], predict(RW10,beton[1:25,-9]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW11,adult[1:150,-13]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW12,adult[1:150,-13]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW13,adult[1:150,-13]))
Metrics::accuracy(actual = adult[1:150,13], predict(RW14,adult[1:150,-13]))
Metrics::accuracy(actual = test_adlt[,13],
                  ifelse(predict(RW15,test_adlt[,-13])<1.5,1,2))
Metrics::accuracy(actual = test_adlt[,13],
                  ifelse(predict(RW16,test_adlt[,-13])<1.3,1,2))

##-----YARDIMCI FONKSİYONLAR--
#Takılan kümeleyicilerden sınıf kimliklerini veya üyelikleri tahmin etmek için bir tahmin yöntemi
Cobweb(x=adult, control = NULL) #cluster
FarthestFirst(x=adult, control = NULL)#en uzak ilk geçiş algoritmasını
SimpleKMeans(x=adult, control = NULL)#k-means modellenen hızlı, basit yaklaşık bir kümeleyici
#XMeans(x=adult, control = NULL)#k-means extended by an “Yapıyı İyileştirme bölümü”
#DBScan(x=adult, control = NULL)#yoğunluk tabanlı kümeleme algoritmasını

#Weka_associators R/Weka Associators
x <- read.arff(system.file("arff", "contact-lenses.arff",
                           package = "RWeka"))
Apriori(x, control = NULL)
#Tertius(x, control = NULL)
```



# 3. BART

Bayesian Toplamalı Regresyon Ağaçları (BART), sürekli, ikili, kategorik
ve olaya kadar geçen süre sonuçları için ortak değişkenlerin esnek
parametrik olmayan modellemesini sağlar.

```{r}
library('BART')

#abart :AFT Olaya kadar geçen süre sonuçları için BART
abart(
  x.train=lung[,-c(2,3)], times=lung$time, delta=lung$status-1,
  x.test=matrix(0,0,0), K=100,
  type='abart', ntype=1,
  sparse=FALSE, theta=0, omega=1,
  a=0.5, b=1, augment=FALSE, rho=NULL,
  xinfo=matrix(0,0,0), usequants=FALSE,
  rm.const=TRUE,
  sigest=NA, sigdf=3, sigquant=0.90,
  k=2, power=2, base=0.95,
  lambda=NA, tau.num=c(NA, 3, 6)[ntype=1],
  offset=NULL, w=rep(1, length(lung$times)),
  ntree=c(200L, 50L, 50L)[ntype=1], numcut=100L,
  ndpost=1000L, nskip=100L,
  keepevery=c(1L, 10L, 10L)[ntype=1],
  printevery=100L, transposed=FALSE,
  mc.cores = 1L, ## mc.abart only
  nice = 19L, ## mc.abart only
  seed = 99L ## mc.abart only
)

#gbart: Sürekli ve ikili sonuçlar için genelleştirilmiş BART
y.train=ifelse(train_adlt[,13]==1,0,1)
ntype=as.integer(factor('pbart', levels=c('wbart', 'pbart', 'lbart')))
mdl1<-gbart(x.train = train_adlt[,-13], y.train = y.train, x.test=matrix(0,0,0),
           type='pbart',  ntype= ntype, sparse=FALSE, theta=0, omega=1,
           a=0.5, b=1, augment=FALSE, rho=NULL, xinfo=matrix(0,0,0),
           usequants=FALSE, rm.const=TRUE, sigest=NA, sigdf=3, sigquant=0.90,
           k=2, power=2, base=0.95, lambda=NA, tau.num=c(NA, 3, 6)[ntype],
           offset=NULL, w=rep(1, length(y.train)),
           ntree=c(200L, 50L, 50L)[ntype], numcut=100L, ndpost=500L,
           nskip=100L, keepevery=c(1L, 10L, 10L)[ntype], printevery=100L,
           transposed=FALSE, hostname=FALSE,
           mc.cores = 1L, ## mc.gbart only
           nice = 19L, ## mc.gbart only
           seed = 99L ## mc.gbart only
           )
predict(mdl1,test_adlt[,-13])


#lbart :Lojistik latentlerle ikili sonuçlar için Logit BART
ndpost=1000
mdl2<-lbart(x.train = train_adlt[,-13], y.train = train_adlt$income-1,
      x.test=matrix(0.0,0,0), sparse=FALSE, a=0.5, b=1, augment=FALSE, rho=NULL,
      xinfo=matrix(0.0,0,0), usequants=FALSE, cont=FALSE, rm.const=TRUE,
      tau.interval=0.95, k=2.0, power=2.0, base=.95, binaryOffset=NULL,
      ntree=200L, numcut=100L, ndpost=1000L, nskip=100L, keepevery=1L,
      nkeeptrain=ndpost, nkeeptest=ndpost, nkeeptreedraws=ndpost,
      printevery=100L, transposed=FALSE)
predict(object=mdl2, newdata=test_adlt[,-13], mc.cores=1,
        openmp=(mc.cores.openmp()>0))

#mbart :Daha az kategoriye sahip kategorik sonuçlar için çok terimli BART
ntype=as.integer(factor('pbart', levels=c('wbart', 'pbart', 'lbart')))
mdl3<-mbart(x.train = train_adlt[,-13], y.train = train_adlt$income-1,
      x.test=matrix(0,0,0), type='pbart',
      ntype=as.integer(factor(type, levels=c('wbart', 'pbart', 'lbart'))),
      sparse=FALSE, theta=0, omega=1, a=0.5, b=1, augment=FALSE, rho=NULL,
      xinfo=matrix(0,0,0), usequants=FALSE, rm.const=TRUE, k=2, power=2,
      base=0.95, tau.num=c(NA, 3, 6)[ntype], offset=NULL,
      ntree=c(200L, 50L, 50L)[ntype], numcut=100L, ndpost=1000L, nskip=100L,
      keepevery=c(1L, 10L, 10L)[ntype], printevery=100L, transposed=FALSE,
      hostname=FALSE,mc.cores = 2L, ## mc.bart only
      nice = 19L, ## mc.bart only
      seed = 99L ## mc.bart only
  )
predict(object=mdl3, newdata=test_adlt[,-13], mc.cores=1,
        openmp=(mc.cores.openmp()>0))

#mbart2 :Daha fazla kategoriye sahip kategorik sonuçlar için çok terimli BART
ad<-adlt # 3 kategori yapalım
ad$income <- cut(ad$capital_gain, breaks = c(0, 18000, 40000, 99999),
                 labels = c(1, 2, 3), include.lowest = T)
ad_train<-ad[row.names(ad) %in% 1:30000, ]
ad_test<-ad[row.names(ad) %in% (30001):nrow(ad), ]
ntype=as.integer(factor('lbart', levels=c('wbart', 'pbart', 'lbart')))
mdl4<-mbart2(x.train=ad_train[,-13], y.train=ad_train[,13],
  x.test=matrix(0,0,0), type='lbart',
  ntype=as.integer(factor('lbart', levels=c('wbart', 'pbart', 'lbart'))),
  sparse=FALSE, theta=0, omega=1, a=0.5, b=1, augment=FALSE, rho=NULL,
  xinfo=matrix(0,0,0), usequants=FALSE, rm.const=TRUE, k=2, power=2, base=0.95,
  tau.num=c(NA, 3, 6)[ntype], offset=NULL, ntree=c(200L, 50L, 50L)[ntype],
  numcut=100L, ndpost=1000L, nskip=100L, keepevery=c(1L, 10L, 10L)[ntype],
  printevery=100L, transposed=FALSE, hostname=FALSE,
  mc.cores = 2L, ## mc.bart only
  nice = 19L, ## mc.bart only
  seed = 99L ## mc.bart only
)
predict(object=mdl4, newdata=ad_test[,-13], mc.cores=1,
        openmp=(mc.cores.openmp()>0))

#pbart :Normal latentlerle ikili sonuçlar için Probit BART
mdl5<-pbart(x.train = train_adlt[,-13], y.train = train_adlt$income-1,
      x.test=matrix(0.0,0,0), sparse=FALSE, theta=0, omega=1, a=0.5, b=1,
      augment=FALSE, rho=NULL, xinfo=matrix(0.0,0,0), usequants=FALSE,
      cont=FALSE, rm.const=TRUE, k=2.0, power=2.0, base=.95, binaryOffset=NULL,
      ntree=50L, numcut=100L, ndpost=1000L, nskip=100L, keepevery=1L,
      nkeeptrain=1000, nkeeptest=1000, nkeeptreedraws=1000, printevery=100L,
      transposed=FALSE)
predict(object=mdl5, newdata=test_adlt[,-13], mc.cores=1,
        openmp=(mc.cores.openmp()>0))

##recur.bart: Tekrarlayan olaylar için BART
r<-recur.pre.bart(times=as.matrix(lung$time), delta=as.matrix(lung$status-1),
                x.train=as.matrix(lung[,-c(2,3)]), tstop=NULL, last.value=TRUE )
ntype=as.integer(factor('pbart', levels=c('wbart', 'pbart', 'lbart')))
mdl6<-recur.bart(x.train=r$tx.train,
  y.train=r$y.train, times=r$times,
  delta=NULL, x.test=r$tx.test)
#  x.test.nogrid=FALSE, sparse=FALSE, theta=0, omega=1, a=0.5, b=1,
#  augment=FALSE, rho=NULL, xinfo=matrix(0,0,0), usequants=FALSE,
#  rm.const=TRUE, type='pbart',ntype=as.integer(
#  factor('pbart', levels=c('wbart', 'pbart', 'lbart'))),
#  k=2, power=2, base=0.95, offset=NULL, tau.num=c(NA, 3, 6)[ntype],
#  ntree=50, numcut = 100L, ndpost=1000, nskip=250, keepevery=10,
#  printevery = 100L, keeptrainfits = TRUE,
#  seed=99, ## mc.recur.bart only
#  mc.cores=2, ## mc.recur.bart only
#  nice=19L ## mc.recur.bart only
#)
#predict(object=mdl6, newdata=lung[90:110,-c(2,3)], mc.cores=1,
#        openmp=(mc.cores.openmp()>0))


#surv.bart :BART ile hayatta kalma analizi
mdl7<-surv.bart(x.train=lung[,-c(2,3)],
  y.train=NULL, times=lung$time, delta=lung$status-1,
  x.test=matrix(0,0,0),
  K=NULL, events=NULL, ztimes=NULL, zdelta=NULL,
  sparse=FALSE, theta=0, omega=1,
  a=0.5, b=1, augment=FALSE, rho=NULL,
  xinfo=matrix(0,0,0), usequants=FALSE,
  rm.const=TRUE, type='pbart',
  ntype=as.integer(
  factor('pbart', levels=c('wbart', 'pbart', 'lbart'))),
  k=2, power=2, base=.95,
  offset=NULL, tau.num=c(NA, 3, 6)[ntype],
  ntree=50, numcut=100, ndpost=1000, nskip=250,
  keepevery = 10L,
  printevery=100L,
  id=NULL, ## surv.bart only
  seed=99, ## mc.surv.bart only
  mc.cores=2, ## mc.surv.bart only
  nice=19L ## mc.surv.bart only
)
predict(object=mdl7, newdata=lung[80:120,-c(2,3)], mc.cores=1,
        openmp=(mc.cores.openmp()>0))


#wbart :Sürekli sonuçlar için BART
mdl8<-wbart(x.train=beton[,-9], y.train=beton[,9])
  #x.test=matrix(0.0,0,0), sparse=FALSE, theta=0, omega=1,  a=0.5, b=1,
  #augment=FALSE, rho=NULL, xinfo=matrix(0.0,0,0), usequants=FALSE, cont=FALSE,
  #rm.const=TRUE, sigest=NA, sigdf=3, sigquant=.90, k=2.0, power=2.0, base=.95,
  #sigmaf=NA, lambda=NA, fmean=mean(y.train=beton[,9]),
  #w=rep(1,length(y.train=beton[,9])), ntree=200L, numcut=100L, ndpost=1000L,
  #nskip=100L, keepevery=1L, nkeeptrain=1000, nkeeptest=1000,nkeeptestmean=1000,
  #nkeeptreedraws=1000, printevery=100L, transposed=FALSE)
predict(object=mdl8, newdata=beton[500:550,-9], mc.cores=1,
        openmp=(mc.cores.openmp()>0))

#-------------------------İLAVE FONKSİYONLAR
#crisk.bart: Rekabet eden riskler için BART
crisk.bart(x.train=matrix(0,0,0), y.train=NULL,
  x.train2=x.train, y.train2=NULL,
  times=NULL, delta=NULL, K=NULL,
  x.test=matrix(0,0,0), x.test2=x.test, cond=NULL,
  sparse=FALSE, theta=0, omega=1,
  a=0.5, b=1, augment=FALSE,
  rho=NULL, rho2=NULL,
  xinfo=matrix(0,0,0), xinfo2=matrix(0,0,0),
  usequants=FALSE,
  rm.const=TRUE, type='pbart',
  ntype=as.integer(
  factor(type, levels=c('wbart', 'pbart', 'lbart'))),
  k=2, power=2, base=0.95,
  offset=NULL, offset2=NULL,
  tau.num=c(NA, 3, 6)[ntype],
  ntree=50, numcut=100, ndpost=1000, nskip=250,
  keepevery = 10L,
  printevery=100L,
  id=NULL, ## crisk.bart only
  seed=99, ## mc.crisk.bart only
  mc.cores=2, ## mc.crisk.bart only
  nice=19L ## mc.crisk.bart only
)
predict(object, newdata, newdata2, mc.cores=1, openmp=(mc.cores.openmp()>0))

#crisk2.bart: Rekabet eden riskler için BART
crisk2.bart(x.train=matrix(0,0,0), y.train=NULL,
  x.train2=x.train, y.train2=NULL,
  times=NULL, delta=NULL, K=NULL,
  x.test=matrix(0,0,0), x.test2=x.test,
  sparse=FALSE, theta=0, omega=1,
  a=0.5, b=1, augment=FALSE,
  rho=NULL, rho2=NULL,
  xinfo=matrix(0,0,0), xinfo2=matrix(0,0,0),
  usequants=FALSE,
  rm.const=TRUE, type='pbart',
  ntype=as.integer(
  factor(type, levels=c('wbart', 'pbart', 'lbart'))),
  k=2, power=2, base=0.95,
  offset=NULL, offset2=NULL,
  tau.num=c(NA, 3, 6)[ntype],
  ntree=50, numcut=100, ndpost=1000, nskip=250,
  keepevery = 10L,
  printevery=100L,
  id=NULL, ## crisk2.bart only
  seed=99, ## mc.crisk2.bart only
  mc.cores=2, ## mc.crisk2.bart only
  nice=19L ## mc.crisk2.bart only
)
predict(object, newdata, newdata2, mc.cores=1, openmp=(mc.cores.openmp()>0))

#mc.crisk.pwbart: Önceden takılmış bir BART modeli ile yeni gözlemleri tahmin etme
mc.crisk.pwbart( x.test, x.test2,
  treedraws, treedraws2,
  binaryOffset=0, binaryOffset2=0,
  mc.cores=2L, type='pbart',
  transposed=FALSE, nice=19L
)

#mc.crisk2.pwbart Önceden takılmış bir BART modeli ile yeni gözlemlerin tahmin edilmesi
mc.crisk2.pwbart( x.test, x.test2,
  treedraws, treedraws2,
  binaryOffset=0, binaryOffset2=0,
  mc.cores=2L, type='pbart',
  transposed=FALSE, nice=19L
)

#pwbart: Daha önce takılmış bir BART modeli ile yeni gözlemlerin tahmin edilmesi
pwbart(x.test, treedraws, mu=0, mc.cores=1L, transposed=FALSE,
  dodraws=TRUE,nice=19L ## mc.pwbart only
)
```

sürekli için 'wbart', probit için 'pbart', logit için 'lbart'




# 4. grf

Ormana dayalı istatistiksel tahmin ve çıkarım. GRF, tümü eksik ortak
değişkenleri destekleyen, heterojen tedavi etkileri tahmini (isteğe
bağlı olarak sağ sansürlü sonuçlar, çoklu tedavi kolları veya sonuçları
veya araçsal değişkenler kullanılarak) ve ayrıca en küçük kareler
regresyonu, niceliksel regresyon ve hayatta kalma regresyonu için
parametrik olmayan yöntemler sağlar.

```{r}
library('grf')
#Boosted regression forest
mdlg<-boosted_regression_forest(X=train_adlt[,-13], Y=train_adlt[,13],
  num.trees = 2000, sample.weights = NULL,clusters = NULL,
  equalize.cluster.weights = FALSE,sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),ncol(train_adlt[,-13])),
  min.node.size = 5,honesty = TRUE,honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,alpha = 0.05,imbalance.penalty = 0,
  ci.group.size = 2,tune.parameters = "none",tune.num.trees = 10,
  tune.num.reps = 100,tune.num.draws = 1000,boost.steps = NULL,
  boost.error.reduction = 0.97,boost.max.steps = 5,
  boost.trees.tune = 10,num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))


#causal_forest :Causal forest
mdlc<-causal_forest(X=train_adlt[,-13], Y=train_adlt[,13],
  W=rep(1,nrow(train_adlt)), Y.hat = NULL, W.hat = NULL, num.trees = 2000,
  sample.weights = NULL, clusters = NULL, equalize.cluster.weights = FALSE,
  sample.fraction = 0.5, mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),
                                    ncol(train_adlt[,-13])),
  min.node.size = 5,honesty = TRUE,honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,alpha = 0.05,imbalance.penalty = 0,
  stabilize.splits = TRUE,ci.group.size = 2,tune.parameters = "none",
  tune.num.trees = 200,tune.num.reps = 50,tune.num.draws = 1000,
  compute.oob.predictions = TRUE,num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))

D <- ifelse(lung$status==1&lung$time<220,lung$status-1,1)#cencor time create
time<-scale(lung$time, center=min(lung$time), scale=diff(range(lung$time)))#0-1
#causal_survival_forest :Causal survival forest
mdlcs<-causal_survival_forest(X=lung[,-c(2,3)], Y=time,
  W=(lung[,3]-1),D=D, W.hat = NULL,
  target = c("RMST", "survival.probability"), horizon = 1,
  failure.times = NULL,num.trees = 2000, sample.weights = NULL,clusters = NULL,
  equalize.cluster.weights = FALSE,sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(lung[,-c(2,3)])) + 20),
             ncol(lung[,-c(2,3)])),
  min.node.size = 5,honesty = TRUE,honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,alpha = 0.05, imbalance.penalty = 0,
  stabilize.splits = TRUE, ci.group.size = 3,tune.parameters = "none",#
  compute.oob.predictions = TRUE,num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))


Zi=rbinom(nrow(train_adlt),1,0.5)#create instrument
Wi=rbinom(nrow(train_adlt),1,0.5)#create treatment
#instrumental_forest :Intrumental forest
mdli<-instrumental_forest(X=train_adlt[,-13], Y=train_adlt[,13],
  W=Wi,Z=Zi,Y.hat = NULL,
  W.hat = NULL,Z.hat = NULL,num.trees = 2000,
  sample.weights = NULL,clusters = NULL,
  equalize.cluster.weights = FALSE,sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),
            ncol(train_adlt[,-13])),
  min.node.size = 5,honesty = TRUE,honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,alpha = 0.05,
  imbalance.penalty = 0,stabilize.splits = TRUE,
  ci.group.size = 2,reduced.form.weight = 0,
  tune.parameters = "none",tune.num.trees = 200,
  tune.num.reps = 50,tune.num.draws = 1000,
  compute.oob.predictions = TRUE,num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))

#ll_regression_forest :Local linear forest
mdlll<-ll_regression_forest(X=train_adlt[,-13], Y=train_adlt[,13],
  enable.ll.split = FALSE, ll.split.weight.penalty = FALSE,
  ll.split.lambda = 0.1, ll.split.variables = NULL, ll.split.cutoff = NULL,
  num.trees = 2000, clusters = NULL, equalize.cluster.weights = FALSE,
  sample.fraction = 0.5, mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),
                                    ncol(train_adlt[,-13])),
  min.node.size = 5, honesty = TRUE, honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0,
  ci.group.size = 2, tune.parameters = "none", tune.num.trees = 50,
  tune.num.reps = 100, tune.num.draws = 1000, num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))

#lm_forest :LM Forest
mdllm<-lm_forest(X=train_adlt[,-13], Y=train_adlt[,13], W=Wi,
  Y.hat = NULL, W.hat = NULL, num.trees = 2000, sample.weights = NULL,
  gradient.weights = NULL, clusters = NULL, equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),ncol(train_adlt[,-13])),
  min.node.size = 5, honesty = TRUE, honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0,
  stabilize.splits = FALSE, ci.group.size = 2, compute.oob.predictions = TRUE,
  num.threads = NULL, seed = runif(1, 0, .Machine$integer.max))

#multi_arm_causal_forest :Multi-arm/multi-outcome causal forest
mdlma<-multi_arm_causal_forest(X=train_adlt[,-13], Y=train_adlt[,13],
  W=factor(Wi), Y.hat = NULL, W.hat = NULL, num.trees = 2000,
  sample.weights = NULL, clusters = NULL, equalize.cluster.weights = FALSE,
  sample.fraction = 0.5, mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),
                                    ncol(train_adlt[,-13])),
  min.node.size = 5, honesty = TRUE, honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0,
  stabilize.splits = TRUE, ci.group.size = 2, compute.oob.predictions = TRUE,
  num.threads = NULL, seed = runif(1, 0, .Machine$integer.max))

#multi_regression_forest :Multi-task regression forest
mdlmr<-multi_regression_forest(X=train_adlt[,-c(11,12)],Y=train_adlt[,c(11,12)],
  num.trees = 2000, sample.weights = NULL, clusters = NULL,
  equalize.cluster.weights = FALSE, sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-c(1,3)])) + 20),
             ncol(train_adlt[,-c(1,3)])),
  min.node.size = 5, honesty = TRUE, honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0,
  compute.oob.predictions = TRUE, num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))

#Probability forest
mdlp<-probability_forest(X=train_adlt[,-13], Y=factor(train_adlt[,13]),
  num.trees = 2000,sample.weights = NULL,clusters = NULL,
  equalize.cluster.weights = FALSE,sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),ncol(train_adlt[,-13])),
  min.node.size = 5,honesty = TRUE,honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,alpha = 0.05,imbalance.penalty = 0,
  ci.group.size = 2,compute.oob.predictions = TRUE,num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))

#quantile_forest :Quantile forest
mdlq<-quantile_forest(X=train_adlt[,-13], Y=train_adlt[,13], num.trees = 2000,
  quantiles = c(0.1, 0.5, 0.9), regression.splitting = FALSE, clusters = NULL,
  equalize.cluster.weights = FALSE, sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),ncol(train_adlt[,-13])),
  min.node.size = 5, honesty = TRUE, honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0, 
  compute.oob.predictions = FALSE, num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))

#regression_forest :Regression forest
mdlr<-regression_forest(X=train_adlt[,-13], Y=train_adlt[,13], num.trees = 2000,
  sample.weights = NULL, clusters = NULL, equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),ncol(train_adlt[,-13])),
  min.node.size = 5, honesty = TRUE, honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE, alpha = 0.05, imbalance.penalty = 0,
  ci.group.size = 2, tune.parameters = "none", tune.num.trees = 50,
  tune.num.reps = 100, tune.num.draws = 1000, compute.oob.predictions = TRUE,
  num.threads = NULL, seed = runif(1, 0, .Machine$integer.max))

#survival_forest :Survival forest
mdls<-survival_forest(X=lung[,-c(2,3)], Y=lung[,2],
  D=(lung[,3]-1),
  failure.times = NULL,num.trees = 100, sample.weights = NULL, clusters = NULL,
  equalize.cluster.weights = FALSE, sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-13])) + 20),ncol(train_adlt[,-13])),
  min.node.size = 15, honesty = TRUE, honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE, alpha = 0.05,
  prediction.type = c("Kaplan-Meier", "Nelson-Aalen"),
  compute.oob.predictions = TRUE, num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))

#prd<-predict(mdlg, test_adlt[,-13])
#Metrics::auc(test_adlt[,13], ifelse(prd<1.5,1,2))

p1<-predict(object=mdlg,newdata = test_adlt[,-13],boost.predict.steps = NULL,
        num.threads = NULL)
Metrics::accuracy(test_adlt$income,ifelse(p1<1.5,1,2))

#predict(object=mdlc,newdata = test_adlt[,-13],linear.correction.variables=NULL,
#        ll.lambda = NULL,ll.weight.penalty = T,num.threads = NULL,
#        estimate.variance = T)

p2<-predict(object=mdlcs,newdata = lung[1:25,-c(2,3)],num.threads = NULL,
        estimate.variance = FALSE)
predict(object=mdli,newdata = test_adlt[,-13],num.threads = NULL,
        estimate.variance = FALSE)
p3<-predict(object=mdlll,newdata = test_adlt[,-13],
            linear.correction.variables = NULL, ll.lambda = NULL,
            ll.weight.penalty = FALSE,num.threads = NULL,
            estimate.variance = FALSE)
Metrics::accuracy(test_adlt$income, ifelse(p3$prediction < 1.5,1,2))

p4<-predict(object=mdllm,newdata = test_adlt[,-13], num.threads = NULL,
        estimate.variance = FALSE,drop = FALSE)
Metrics::accuracy(test_adlt$income, ifelse(p4$predictions[,,1] < 0,1,2))

p5<-predict(object=mdlma,newdata = test_adlt[,-13],num.threads = NULL,
        estimate.variance = FALSE,drop = FALSE)
Metrics::accuracy(test_adlt$income, ifelse(p5$predictions[,,1] < 0,1,2))

p6<-predict(object=mdlmr, newdata = test_adlt[,-c(11,12)], num.threads = NULL,
        drop = FALSE)
Metrics::auc(test_adlt$hours_per_week, p6$predictions[,1])
Metrics::auc(test_adlt$native_country, p6$predictions[,2])


p7<-predict(object=mdlp,newdata = test_adlt[,-13], num.threads = NULL,
        estimate.variance = FALSE)#probability_forest
Metrics::accuracy(test_adlt$income, ifelse(p7$predictions[,1] < 0.5,1,2))

p8<-predict(object=mdlq, newdata = test_adlt[,-13], quantiles = NULL,
            num.threads = NULL)
Metrics::accuracy(test_adlt$income, p8$predictions[,1])
Metrics::accuracy(test_adlt$income, p8$predictions[,2])
Metrics::accuracy(test_adlt$income, p8$predictions[,3])

p9<-predict(object=mdlr,newdata = test_adlt[,-13],
            linear.correction.variables = NULL, ll.lambda = NULL,
            ll.weight.penalty = FALSE,num.threads = NULL,
            estimate.variance = FALSE)
Metrics::accuracy(test_adlt$income, ifelse(p9$prediction < 1.5,1,2))

predict(object=mdls,newdata = lung[1:25,-c(2,3)],failure.times = NULL,
        prediction.times = c("curve", "time"),
        prediction.type = c("Kaplan-Meier", "Nelson-Aalen"),num.threads = NULL)

```



# 5. kernlab

Sınıflandırma, regresyon, kümeleme, yenilik tespiti, niceliksel
regresyon ve boyutluluğun azaltılması için çekirdek tabanlı makine
öğrenimi yöntemleri. Diğer yöntemlerin yanı sıra 'kernlab', Destek
Vektör Makinelerini, Spektral Kümelemeyi, Çekirdek PCA'yı, Gauss
Süreçlerini ve bir QP çözücüyü içerir.

```{r}
library('kernlab')

#Gausspr, sınıflandırma ve regresyon için Gauss süreçlerinin bir uygulamasıdır.
mdl1<-gausspr(x=as.matrix(train_adlt[,-13]), y=factor(train_adlt[,13]),
              scaled = FALSE, type= "classification", kernel="anovadot",
              kpar="automatic", var=0.1, variance.model = FALSE, tol=10,
              cross=0, fit=TRUE, subset, na.action = na.fail)
              #type->"classification" or "regression"

#Ünlü k-ortalamalar algoritmasının ağırlıklı çekirdek versiyonu.
mdl2<-kkmeans(x=as.matrix(train_adlt[,-13]), centers=5, kernel = "rbfdot",
              kpar =  list(sigma = 0.01),#"automatic"
              alg="kkmeans", p=1, na.action = na.omit)

#Çekirdek Maksimum Ortalama Farklılığı kmmd, parametrik olmayan bir dağıtım testi gerçekleştirir.
mdl3<-kmmd(x=train_adlt[,-13], y=train_adlt[,13], kernel="stringdot",
           kpar=list(type="spectrum"), alpha = 0.05, asymptotic = FALSE,
           replace = TRUE, ntimes = 150, frac = 1)

#Çekirdek Temel Bileşenler Analizi, temel bileşen analizinin doğrusal olmayan bir şeklidir.
mdl4<-kpca(x=as.matrix(train_adlt[1:1000,-13]), kernel = "rbfdot",
           kpar = list(sigma=0.1), features=12, th = 1e-4, na.action = na.omit)

#Çekirdek Kantil Regresyon algoritması kqr, parametrik olmayan Kantil Regresyon gerçekleştirir.
mdl5<-kqr(x=as.matrix(train_adlt[1:1000,-13]), y=train_adlt[1:1000,13],
          scaled = TRUE, tau = 0.8, C = 1, kernel = "rbfdot",
          kpar = "automatic", reduced = FALSE, rank = 150, fit = TRUE,
          cross = 0, na.action = na.omit)

#ksvm, iyi bilinen C-svc, nu-svc, (sınıflandırma) tek sınıf-svc (yenilik) eps-svr, nu-svr (regresyon) formülasyonlarının yanı sıra yerel çok sınıflı sınıflandırma formülasyonlarını ve sınırlı kısıtlama SVM formülasyonlarını destekler.
mdl6<-ksvm(x=as.matrix(train_adlt[,-13]), y=factor(train_adlt[,13]),
           scaled = F, type = "nu-svc", kernel ="rbfdot", kpar = "automatic",
           C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,
           class.weights = NULL, cross = 0, fit = TRUE, cache = 40, tol = 10,
           shrinking = TRUE, subset, na.action = na.omit)

#lssvm, csi işlevi tarafından hesaplanan çekirdek matrisinin ayrıştırılmasını kullanan En Küçük Kareler SVM'nin küçültülmüş bir sürümünü içerir.
mdl7<-lssvm(x=as.matrix(train_adlt[,-13]), y=factor(train_adlt[,13]),
            scaled = TRUE, kernel = "rbfdot", kpar = "automatic", type = NULL,
            tau = 0.01, reduced = TRUE, tol = 0.0001,
            rank = floor(dim(train_adlt)[1]/3), delta = 40,
            cross = 0, fit = TRUE, subset, na.action = na.omit)

#İlgililik Vektör Makinesi, aynı fonksiyonel formun destek vektör makinesine regresyonu ve sınıflandırılması için bir Bayes modelidir. rvm işlevi şu anda yalnızca regresyonu desteklemektedir.
mdl8<-rvm(x=as.matrix(train_adlt[1:1000,-13]), y=train_adlt[1:1000,13],
          type="regression", kernel="rbfdot", kpar="automatic",
          alpha= ncol(as.matrix(train_adlt[1:1000,-1])), var=0.1, var.fix=FALSE,
          iterations=100, verbosity = 0, tol = .Machine$double.eps,
          minmaxdiff = 1e-3, cross = 0, fit = TRUE, subset, na.action = na.omit)

#Bir spektral kümeleme algoritması. Kümeleme, verilerin bir ilgi matrisinin özvektörlerinin alt uzayına yerleştirilmesiyle gerçekleştirilir.
mdl9<-specc(x=as.matrix(train_adlt[1:1000,-13]), centers = 5, kernel = "rbfdot",
            kpar = "automatic", nystrom.red = FALSE,
            nystrom.sample = dim(train_adlt[1:1000,])[1]/6, iterations = 200,
            mod.sample = 0.75, na.action = na.omit)


Metrics::accuracy(actual=test_adlt[,13],
 predicted=predict(mdl1,test_adlt[,-13],type ="response", coupler ="minpair"))
Metrics::accuracy(actual=test_adlt[,13],
                  predicted=predict(mdl5,test_adlt[,-13]))
Metrics::accuracy(actual=test_adlt[,13],
  predicted=predict(mdl6,test_adlt[,-13],type ="response", coupler ="minpair"))

##kernel hebbian algoritma sınıflandırması
kha(as.matrix(train_adlt[1:1000,-13]), kernel = "rbfdot")
#, kpar = list(sigma = 0.1), features = 5, eta = 0.005, th = 1e-4,
#maxiter = 10000, verbose = FALSE, na.action = na.omit)
```

KERNEL
• rbfdot Radial Basis kernel function "Gaussian"
• polydot Polynomial kernel function
• vanilladot Linear kernel function
• tanhdot Hyperbolic tangent kernel function
• laplacedot Laplacian kernel function
• besseldot Bessel kernel function
• anovadot ANOVA RBF kernel function
• splinedot Spline kernel


KPAR
• sigma inverse kernel width for the Radial Basis kernel function
"rbfdot" and the Laplacian kernel "laplacedot".
• degree, scale, offset for the Polynomial kernel "polydot"
• scale, offset for the Hyperbolic tangent kernel function "tanhdot"
• sigma, order, degree for the Bessel kernel "besseldot".
• sigma, degree for the ANOVA kernel "anovadot".



# 6. evclass

Dempster-Shafer kütle fonksiyonları biçiminde çıktılar sağlayan farklı
kanıtsal sınıflandırıcılar. Yöntemler şunlardır: kanıtsal K-en yakın
komşu kuralı, kanıtsal sinir ağı, radyal temel fonksiyonlu sinir ağları,
lojistik regresyon, ileri beslemeli sinir ağları.

```{r}
library('evclass')

#EkNN sınıflandırıcısı için parametrelerin başlatılması
int<-EkNNinit(x=train_adlt[,-13],y=train_adlt[,13], alpha = 0.95)#gamma,alpha

#EkNN sınıflandırıcının eğitimi
mdl1<-EkNNfit(x=train_adlt[,-13],y=train_adlt[,13],K=5,param = int,alpha = 0.95,
        lambda = 1/max(as.numeric(c('1','2'))),optimize = TRUE,
        options = list(maxiter = 300, eta = 0.1, gain_min = 1e-06, disp = TRUE))

#Bir test setinin EkNN sınıflandırıcı tarafından sınıflandırılması
V1<-EkNNval(xtrain=train_adlt[,-13], ytrain=train_adlt[,13],
          xtst=test_adlt[,-13],K=5, ytst = test_adlt[,13], param = mdl1$param)
#metric ??? v$ypred ~~ test

##--------------
#kanıtsal sinir ağı sınıflandırıcısı için başlangıç parametre değerlerini döndürür.
int<-proDSinit(x=train_adlt[,-13],y=train_adlt[,13], nproto=2,
               nprotoPerClass = T, crisp = T)

#kanıtsal sinir ağı sınıflandırıcısı için parametre optimizasyonu gerçekleştirir
mdl2<-proDSfit(x=train_adlt[,-13],y=train_adlt[,13],param=int,
         lambda = 1/max(as.numeric(c('1','2'))),mu = 0,optimProto = TRUE,
         options = list(maxiter = 500, eta = 0.1, gain_min = 1e-04, disp = 10))

#kanıta dayalı sinir ağı sınıflandırıcısını kullanarak bir test kümesindeki örnekleri sınıflandırır.
V2<-proDSval(x=test_adlt[,-13],y=test_adlt[,13], param=mdl2$param)

##--------------
#RBFinit, Radyal Temel Fonksiyon sınıflandırıcısı için başlangıç parametre değerlerini döndürür.
int<-RBFinit(x=train_adlt[,-13],y=train_adlt[,13], nproto=5)

#RBFfit, radyal temel fonksiyon (RBF) sınıflandırıcısı için parametre optimizasyonu gerçekleştirir.
mdl3<-RBFfit(x=train_adlt[,-13],y=train_adlt[,13],param=int,lambda = 0,
       control = list(fnscale = -1, trace = 2, maxit = 1000),optimProto = TRUE)

#Bir test setinin radyal temel fonksiyon sınıflandırıcısı ile sınıflandırılması
V3<-RBFval(x=test_adlt[,-13],y=test_adlt[,13], param=mdl3$param,
           calc.belief = TRUE)
```




# 7. caret

Sınıflandırma ve regresyon modellerini eğitmek ve çizmek için çeşitli
işlevler.

BU PAKET train FONKSİYONU İLE BAŞKA PAKETLERİ KULLANARAK MODELLEME YAPMA
YETENEĞİNE SAHİP (112 adet)
<https://topepo.github.io/caret/using-your-own-model-in-train>

```{r}
library('caret')
##------data manuplation: soru işareti yerine kategorik değer atamak için
tmp<-adult
i<-c(2,4:8,12,13) #categorical
tmp[ , i] <- apply(tmp[ , i], 2, function(x) as.numeric(factor(x)))
tmp1 <- preProcess(tmp, method="medianImpute")
adlt <- try(predict(tmp1, tmp), silent = TRUE)
x<-createDataPartition(y=adlt$income,times = 1,p = 0.7,list = TRUE,
                    groups = min(5, length(adlt$income)))
train_adlt<-adlt[x[[1]], ]
test_adlt<-adlt[-x[[1]], ]
rm(tmp,tmp1,i,x)
#--------

#Model Ortalamasını Kullanan Sinir Ağları
mod1<-avNNet(x=train_adlt[,-13], y=factor(train_adlt[,13]), repeats = 5, bag =T,
            allowParallel = TRUE, seeds = sample.int(1e+05, 5), size = 25,
            linout = TRUE, trace = FALSE)

prd<-predict(mod1, test_adlt[,-13], type="class")
confusionMatrix(table(Prediction=as.numeric(prd),
                      Referance=test_adlt[,13]))$overall[1]

#Bu fonksiyon, bir eğitim seti için sınıf ağırlık merkezlerini ve kovaryans matrisini hesaplar.
#Örneklerin her sınıf merkezine olan Mahalanobis mesafeleri.
mod2<-classDist(x=train_adlt[,-13], y=factor(train_adlt[,13]), groups = 5,
              pca = T, keep = NULL)
splom(predict(mod2, newdata=test_adlt[,-13], trans = log), grp=test_adlt[,13])


#PCA'yı bir veri kümesinde çalıştırın, ardından bunu bir sinir ağı modelinde kullanın
mod3<-pcaNNet(x=train_adlt[,-13], y=factor(train_adlt[,13]),thresh = 0.99,
              size=12, linout = TRUE, trace = FALSE)

confusionMatrix(table(as.numeric(predict(object=mod3, newdata = test_adlt[,-13],
                                   type = "class")), test_adlt[,13]))$overall[1]

#plsda, sınıflandırma için standart PLS modellerine uyacak şekilde kullanılırken splsda, seyrek PLS gerçekleştirir.özellik seçimi ve düzenlemeyi aynı amaç için yerleştirir.
mod4<-plsda(x=train_adlt[,-13], y=factor(train_adlt[,13]), ncomp = 2,
      probMethod = "bayes", prior = NULL)

confusionMatrix(predict(object=mod4, newdata = test_adlt[,-13], ncomp = NULL,
                        type = "class"), factor(test_adlt[,13]))$overall[1]


#-----kullanışlı fonksiyonlar--


#Genetik algoritmalar kullanılarak denetlenen özellik seçimi
gafs(train_adlt[1:1000,-13], factor(train_adlt[1:1000,13]),
     gafsControl = gafsControl(functions = rfGA, method = "boot",number = 3),
     iters = 10, popSize = 50, pcrossover = 0.8, pmutation = 0.1, elite = 0,
     suggestions = NULL, differences = TRUE)


#knn classification method --PREDICT NOT USE !
knn3(x=adult[,-13],y=adult[,13],subset,na.action, k = 5)

#knn regression method --PREDICT NOT USE !
knnreg(x=beton[,-9], y=beton[,9], subset, na.action, k = 5)

#Bu işlev, eğitim verilerine basit, filtre tabanlı özellik seçimi uygulandığında modeller için yeniden örnekleme tahminleri elde etmek için kullanılabilir.
sbf(train_adlt[,-13], factor(train_adlt[,13]),
     sbfControl = sbfControl(functions = rfSBF, method = "boot"))
```

PREPROCESS: "BoxCox", "YeoJohnson", "expoTrans", "center", "scale",
"range", "knnImpute", "bagImpute", "medianImpute", "pca", "ica" and
"spatialSign"



# 8. rms

ikili veya sıralı regresyon modelleri, Cox regresyonu, hızlandırılmış
başarısızlık süresi modelleri, sıradan doğrusal modeller, Buckley-James
modeli, seri veya uzamsal korelasyon için genelleştirilmiş en küçük
kareler ile çalışmak üzere yazılmıştır.

```{r}
library('rms')

#rms Version of glm
f<-as.formula(paste("csMPa ~ ", paste(names(beton[,-9]), collapse= "+")))

fit1<-Glm(formula=f, data=beton, family = gaussian(), weights=NULL,
          subset=NULL, na.action = na.delete, start = NULL, offset = NULL,
          control = glm.control(epsilon=2,maxit=25), model = TRUE,
          method = "glm.fit", x = FALSE, y = TRUE, contrasts = NULL)

#Fit Linear Model Using Generalized Least Squares
fit2<-Gls(model=f, data=beton, method="ML")
# correlation=NULL, weights=NULL, subset=NULL, na.action=na.omit,control=list(),
#verbose=FALSE, B=0, dupCluster=FALSE,pr=FALSE, x=FALSE)

#formulden sıkıntılı veriyi çıkardım
formul<-as.formula(paste("csMPa ~ ", paste(names(beton[,-c(2,3,4,5,9)]),
                                        collapse= "+")))

#Logistic Regression Model
fit3<-lrm(formula=formul, data=beton[,-c(2,3,4,5)])

#Linear Model Estimation Using Ordinary Least Squares
fit4<-ols(formula=formul, data=beton[,-c(2,3,4,5)])

#Ordinal Regression Model
fit5<-orm(formula=formul, data=beton[,-c(2,3,4,5)])


#bj, Buckley-James dağıtımından bağımsız en küçük kareler çoklu regresyon modeline muhtemelen sağdan sansürlü bir yanıt değişkenine uyar
bj(formula=Surv(time=lung$time, event=lung$status-1)~age, data=lung,
   subset=NULL, na.action = na.delete, link="log",
   control=list(iter.max = 20, trace=T, max.cycle=30), method='fit', x=FALSE,
   y = FALSE, time.inc = 30)


#Parametric Survival Model
psm(formula=Surv(time=lung$time, event=lung$status-1)~age, data=lung,
    #weights=NULL,subset=NULL, na.action=na.delete,
    #dist="weibull",init=NULL, scale=0,control=list(),parms=NULL,
    #model=FALSE, x=FALSE, y=TRUE, time.inc=30
    )

#Cox Proportional Hazards Model and Extensions ÇİFT HEDEF DEĞİŞKENİ
cph(formula=Surv(time=lung$time, event=lung$status-1)~age, data=lung)
# weights, subset,na.action=na.delete,
# method=c("efron","breslow","exact","model.frame","model.matrix"),
# singular.ok=FALSE, robust=FALSE,model=FALSE, x=FALSE, y=FALSE, se.fit=FALSE,
# linear.predictors=TRUE, residuals=TRUE, nonames=FALSE,eps=1e-4, init,
# iter.max=10,tol=1e-9, surv=FALSE, time.inc,type=NULL, vartype=NULL,
# debug=FALSE)

#Cox Survival Estimates
#survest(fit, newdata, linear.predictors, x, times,fun, loglog=FALSE,
#  conf.int=0.95, type, vartype,conf.type=c("log", "log-log", "plain", "none"),
#   se.fit=TRUE, what=c('survival','parallel'),individual=FALSE, ...)

#Cox Predicted Survival
#survfit(formula, newdata, se.fit=TRUE, conf.int=0.95,individual=FALSE,
#   type=NULL, vartype=NULL,conf.type=c('log', "log-log", "plain", "none"), id)

Metrics::mae(beton[1:25,9],
             predict(object=fit2, beton[1:25,-9], type=c("lp")))

Metrics::mae(beton[1:25,9],
             predict(object=fit3, beton[1:25,-c(2,3,4,5,9)], type=c("lp")))

Metrics::mae(beton[1:25,9],
             predict(object=fit4, beton[1:25,-c(2,3,4,5,9)], type=c("lp")))

Metrics::mae(beton[1:25,9],
             predict(object=fit5, beton[1:25,-c(2,3,4,5,9)], type=c("lp")))

# "fitted","fitted.ind", "mean", "x", "data.frame","terms", "cterms",
#"ccterms", "adjto","adjto.data.frame","model.frame"),
```



# 9. MASS

birçok veri ve analiz yöntemi içerir. glmnb, glmmpql, lmgls, lmridge,
loglm, lqs ve negativeBinomial,polr, rlm modeller için çözüm sunar
--AŞIRI TEKNİK ALGORİTMA MANTIĞI VAR--
```{r}
library(MASS)

#Negatif Binom genelleştirilmiş doğrusal modeli için ek parametre tetanın tahminini içerecek şekilde sistem fonksiyonu glm()'nin bir modifikasyonu.
mdl1<-glm.nb(formula = csMPa ~ cement+slag+flyash+water /(superplasticizer +
                 coarseaggregate + fineaggregate + age), 
       data = beton, weights=NULL, subset=NULL,
       na.action=na.omit,
      # start = NULL, etastart=1, mustart=1,
       control = glm.control(epsilon = 1e-8, maxit = 25, trace = FALSE),
       method = "glm.fit",model = TRUE, x = FALSE, y = TRUE, contrasts = NULL,
       init.theta=10, link = log #log, sqrt or identity
      )

#Cezalandırılmış Yarı Olabilirliği kullanarak çok değişkenli normal rastgele efektlere sahip bir GLMM modelini yerleştirin
mdl2<-glmmPQL(fixed = csMPa~cement+slag+flyash+water+superplasticizer+
                coarseaggregate+I(age),
        random = ~1|fineaggregate, family = "poisson", data = beton,
        correlation = NULL, weights = NULL, control = poisson(link = "log"),
        niter = 10, verbose = TRUE)

#lda Doğrusal diskriminant analizi.
mdl3<-lda(formula=Species~., data=iris, prior=c(1,1,1)/3, subset=NULL,
          na.action=na.pass)

#lm.gls Doğrusal modelleri Genelleştirilmiş En Küçük Karelere göre sığdır
#mdl4<-lm.gls(formula=csMPa~., data=beton, W=1030, subset=NULL,
#  na.action=na.pass, inverse = FALSE, method = "lm.fit", model = FALSE,
#  x = FALSE, y = FALSE, contrasts = NULL)

#lm.ridge Sırt regresyonuyla doğrusal bir model yerleştirin
mdl5<-lm.ridge(formula=csMPa~., data=beton, subset=NULL, na.action=na.omit,
         lambda = 0, model = FALSE,
         x = FALSE, y = FALSE, contrasts = NULL)

#Veri kümesindeki iyi noktalara bir regresyon uydurun, böylece yüksek kırılma noktasına sahip bir regresyon tahmincisi elde edin. lmsreg ve ltsreg uyumluluk sarmalayıcılardır.
mdl6<-lqs(formula=csMPa~., data=beton, method = "lts",#"lqs","lms","S","model.frame"
    subset=NULL, na.action=na.omit, model = TRUE,x.ret = FALSE, y.ret = FALSE,
    contrasts = NULL)


#Bir lojistik veya probit regresyon modelini sıralı bir faktör yanıtına uyar. Varsayılan lojistik durum, orantısal olasılık lojistik regresyonudur ve fonksiyon buna göre adlandırılır.
#mdl7<-polr(formula=Species~., data=iris, weights=NULL,# start="zeta", subset,
 #    na.action=na.omit, contrasts = NULL,
  #   Hess = FALSE, model = TRUE,
   #  method = "logistic")# "probit", "loglog", "cloglog", "cauchit"

#qda İkinci dereceden diskriminant analizi
mdl8<-qda(formula=factor(Species)~., data=iris, subset=NULL, na.action=na.omit)

#Bir M tahmincisi kullanarak sağlam regresyonla doğrusal bir model yerleştirin.
mdl9<-rlm(formula=csMPa~., data=beton, weights=NULL, subset=NULL,
    na.action=na.omit, method = "M",# "MM", "model.frame"
    wt.method = "inv.var", #"case"
    model = TRUE, x.ret = TRUE, y.ret = FALSE, contrasts = NULL)

#Bir dizi faktörün çoklu uygunluk analizini hesaplar.
hava<-read.csv("golf_train.csv", fileEncoding = "latin5",check.names = F,
               stringsAsFactors = T)[,-1]
mdl10<-mca(df=hava, nf = 3, abbrev = FALSE)



#predict.glmmPQL
predict(object=mdl2, newdata = beton[1:100,], type = c("link", "response"))

#predict.lda
predict(object=mdl3, newdata=iris[sample(1:150,25),-5], prior = mdl3$prior,
        dimen=3,  method = "plug-in")#, "predictive", "debiased"))

#predict.lqs
predict(object=mdl6, newdata=beton[1:100,-9], na.action = na.pass)

#predict.mca : !!!ANLAMSIZ, yanıt değişkeni yok

#predict.qda
predict(object=mdl8, newdata=iris[sample(1:150,25),-5], prior = mdl8$prior,
        method = "plug-in")# "predictive", "debiased", "looCV"
```

binomial(link = "logit") gaussian(link = "identity") Gamma(link =
"inverse") inverse.gaussian(link = "1/mu\^2") poisson(link = "log")
quasi(link = "identity", variance = "constant") quasibinomial(link =
"logit") quasipoisson(link = "log")




# 10. CORElearn

Tahmine dayalı modeller, örneğin isteğe bağlı yapıcı tümevarımlı
sınıflandırma ve regresyon ağaçlarını ve yapraklardaki modelleri,
rastgele ormanları, kNN'yi, naiveBayes'i ve yerel ağırlıklı regresyonu
içerir.

```{r}
library('CORElearn')

md1<-CoreModel(factor(income)~.,train_adlt,model="rfNear")#basic random forests classifier
md2<-CoreModel(factor(income)~.,train_adlt,model="rf")#random forests classifier
md3<-CoreModel(factor(income)~.,train_adlt,model="tree")#decision tree
md4<-CoreModel(factor(income)~.,train_adlt,model="knn")#k nearest neighbors classifier
md5<-CoreModel(factor(income)~.,train_adlt,model="knnKernel")#weighted k nearest neighbors classifier
md6<-CoreModel(factor(income)~.,train_adlt,model="bayes")#naive Bayesian classifier
md7<-CoreModel(income~.,train_adlt[1:1000,],model="regTree")#regression trees

modelEval(model=md1, correctClass=test_adlt[,13],
          predictedClass=predict(md1, test_adlt[,-13], type = "class"),
          predictedProb=NULL, costMatrix=NULL, priorClProb = NULL,
          avgTrainPrediction = NULL, beta = 1)$accuracy

modelEval(model=md2, correctClass=test_adlt[,13],
          predictedClass=predict(md2, test_adlt[,-13], type = "class"),
          predictedProb=NULL, costMatrix=NULL, priorClProb = NULL,
          avgTrainPrediction = NULL, beta = 1)$accuracy

modelEval(model=md3, correctClass=test_adlt[,13],
          predictedClass=predict(md3, test_adlt[,-13], type = "class"),
          predictedProb=NULL, costMatrix=NULL, priorClProb = NULL,
          avgTrainPrediction = NULL, beta = 1)$accuracy

modelEval(model=md4, correctClass=test_adlt[,13],
          predictedClass=predict(md4, test_adlt[,-13], type = "class"),
          predictedProb=NULL, costMatrix=NULL, priorClProb = NULL,
          avgTrainPrediction = NULL, beta = 1)$accuracy

modelEval(model=md5, correctClass=test_adlt[,13],
          predictedClass=predict(md5, test_adlt[,-13], type = "class"),
          predictedProb=NULL, costMatrix=NULL, priorClProb = NULL,
          avgTrainPrediction = NULL, beta = 1)$accuracy

modelEval(model=md6, correctClass=test_adlt[,13],
          predictedClass=predict(md6, test_adlt[,-13], type = "class"),
          predictedProb=NULL, costMatrix=NULL, priorClProb = NULL,
          avgTrainPrediction = NULL, beta = 1)$accuracy

modelEval(model=md7, correctClass=test_adlt[,13],
          predictedClass=predict(md7, test_adlt[,-13], type = "class"),
          predictedProb=NULL, costMatrix=NULL, priorClProb = NULL,
          avgTrainPrediction = NULL, beta = 1)$MAE
```



# 11. e1071

Gizli sınıf analizine yönelik işlevler, kısa zamanlı Fourier dönüşümü,
bulanık kümeleme, destek vektör makineleri, en kısa yol hesaplaması,
torbalı kümeleme, saf Bayes sınıflandırıcısı, genelleştirilmiş k-en
yakın komşu.

```{r}
library('e1071')

#gknn, genel mesafe ölçümlerinden yararlanan k-en yakın komşular algoritmasının 
#bir uygulamasıdır. Bir formül arayüzü sağlanmıştır
m1<-gknn(income~., data = adult, subset=NULL, na.action = na.pass,
         scale = TRUE)
m1_1<-gknn(x=adult[,-13], y=adult[,13], k = 1, method = NULL,
           scale = TRUE, use_all = TRUE, FUN = mean)

#Bayes kuralını kullanarak bağımsız tahmin değişkenleri verildiğinde kategorik
# bir sınıf değişkeninin koşullu arka olasılıklarını hesaplar
m2<-naiveBayes(income~., data = adult, laplace = 0, subset=NULL,
               na.action = na.pass)
m2_1<-naiveBayes(x=adult[,-13], y=adult[,13], laplace = 0)

#svm, bir destek vektör makinesini eğitmek için kullanılır
m3<-svm(factor(income)~., data = adlt[1:5000,], subset=NULL, na.action =na.omit,
        scale = TRUE)
m3_1<-svm(x=adlt[1:5000,-13], y=factor(adlt[1:5000,13]), probability = TRUE,
    type = "nu-classification", kernel ="radial", degree = 3,
    gamma = if (is.vector(adlt)) 1 else 1 / ncol(adlt[,-1]),
    coef0 = 0, cost = 1, nu = 0.1, class.weights = NULL, cachesize = 40,
    tolerance = 0.001, epsilon = 0.1, shrinking = TRUE, cross = 0,
    scale = TRUE, fitted = TRUE, subset, na.action = na.fail)

#Kmeans gibi bir bölümleme kümesi algoritması, orijinal verilerden alınan önyükleme örnekleri üzerinde tekrar tekrar çalıştırılır. Ortaya çıkan küme merkezleri daha sonra hiyerarşik küme algoritması hclust kullanılarak birleştirilir
m4<-bclust(x=adlt, centers=2, iter.base=10, minsize=0,dist.method="manhattan", #"euclidean",
       hclust.method="average", base.method="kmeans",base.centers=20,
       verbose=TRUE,final.kmeans=FALSE, docmdscale=FALSE,resample=TRUE,
       weights=NULL, maxcluster=100)
m4_1<-hclust.bclust(object=m4, x=adlt, centers=2, dist.method=m4$dist.method,
              hclust.method=m4$hclust.method, final.kmeans=FALSE,
              docmdscale = FALSE, maxcluster=m4$maxcluster)

#Bilinen kmeans kümeleme algoritmasının bulanık versiyonunun yanı sıra çevrimiçi bir varyantı (Denetimsiz Bulanık Rekabetçi öğrenme)
m5<-cmeans(x=adlt, centers=2, iter.max = 100, verbose = FALSE,dist ="euclidean",
       method = "cmeans", m = 2,rate.par = NULL, weights = 1, control = list())

#C-kabuk kümeleme algoritması, bulanık kmeans kümeleme yönteminin kabuk prototip tabanlı versiyonu (halka prototipleri)
#cshell(x=beton[1:1000,], centers=2, iter.max=100, verbose=FALSE,
#   dist="euclidean", method="cshell", m=2, radius = NULL)

#X tarafından verilen veriler üzerinde k sınıflı bir gizli sınıf analizi gerçekleştirilir.
m6<-lca(x=matrix(ifelse(runif(50,0,1)<.5,0,1),nrow = 10), k=2, niter=10,
        matchdata=FALSE, verbose=FALSE)

set.seed(1234)
new<- adult[sample(1:nrow(adult), 50),]

Metrics::accuracy(actual=new[,13], predicted=predict(m1, new[,-13]))
Metrics::accuracy(actual=new[,13], predicted=predict(m2, new[,-13]))
Metrics::accuracy(actual=test_adlt[,13],
                predicted=predict(m3, test_adlt[,-13],decision.values = TRUE))
Metrics::accuracy(actual=matrix(ifelse(runif(10,0,1)<.5,0,1),nrow = 2),
          predicted=predict(m6, matrix(ifelse(runif(10,0,1)<.5,0,1),nrow = 2)))

```




# 12. naivebayes

Naive Bayes sınıflandırıcısının bu uygulamasında aşağıdaki sınıf koşullu
dağılımları mevcuttur: Bernoulli, Kategorik, Gaussian, Poisson ve
Çekirdek Yoğunluğu Tahmini yoluyla tahmin edilen sınıf koşullu
yoğunluğunun parametrik olmayan temsili. Uygulanan sınıflandırıcılar
eksik verileri yönetir ve seyrek verilerden yararlanabilir.

```{r}
library('naivebayes')
m1<-bernoulli_naive_bayes(x=as.matrix(train_adlt[,-13]),
                          y=factor(train_adlt[,13]), prior = NULL, laplace = 0)
m2<-gaussian_naive_bayes(x=as.matrix(train_adlt[,-13]),
                         y=factor(train_adlt[,13]), prior = NULL)
m3<-multinomial_naive_bayes(x=as.matrix(train_adlt[,-13]),
                        y=factor(train_adlt[,13]),prior = NULL, laplace = 0.5)
m4<-naive_bayes(x=as.matrix(train_adlt[,-13]), y=factor(train_adlt[,13]),
                prior = NULL,laplace = 0,usekernel = FALSE, usepoisson = FALSE)
m5<-nonparametric_naive_bayes(x=as.matrix(train_adlt[,-13]),
                              y=factor(train_adlt[,13]), prior = NULL)
m6<-poisson_naive_bayes(x=as.matrix(train_adlt[,-13]),y=factor(train_adlt[,13]),
                        prior = NULL, laplace = 0)

Metrics::accuracy(test_adlt[,13], predict(object=m1,
                  newdata = as.matrix(test_adlt[,-13]), type = "class"))
Metrics::accuracy(test_adlt[,13], predict(object=m2,
                  newdata = as.matrix(test_adlt[,-13]), type = "class"))
Metrics::accuracy(test_adlt[,13], predict(object=m3,
                  newdata = as.matrix(test_adlt[,-13]), type = "class"))
Metrics::accuracy(test_adlt[,13], predict(object=m4,
                  newdata = as.matrix(test_adlt[,-13]), type = "class"))
Metrics::accuracy(test_adlt[,13], predict(object=m5,
                  newdata = as.matrix(test_adlt[,-13]), type = "class"))
Metrics::accuracy(test_adlt[,13], predict(object=m6,
                  newdata = as.matrix(test_adlt[,-13]), type = "class"))
```
.75-.79 arası



# 13. penalized

HAYATTA KALMA MODELLERİ
desteklenen regresyon modelleri doğrusal, lojistik ve Poisson regresyonu
ve Cox Orantılı Tehlike modelidir

```{r}
library(penalized)

lung<-survival::lung#data(cancer, package = 'survival')
lung<-na.omit(lung)
#hayatta kalma nesnesi oluşturun. status değişkeni 0-1 olmalı
S<-Surv(time=lung$time, event=lung$status-1, #time2,
     type=c('right'),origin=0)#'left','interval','counting','interval2','mstate'

# Olabilirlik çapraz doğrulamasını kullanarak, L1 (kement veya kaynaşmış kement) ve/veya L2 (sırt) cezalarıyla genelleştirilmiş doğrusal modellerin çapraz doğrulanması

cv1<-cvl(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
    lambda1 = 0, lambda2= 0, positive = FALSE,fusedl = FALSE, data=lung,
    model = c("cox", "logistic", "linear", "poisson"),
    startbeta=0, startgamma=0, fold=8, epsilon = 1e-10, maxiter=25,
    standardize = FALSE, trace = TRUE, approximate = FALSE)

cv2<-optL1(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
      minlambda1=0, maxlambda1=3, base1=0, lambda2 = 0,
      fusedl = FALSE, positive = FALSE, data=lung,
      model = c("cox", "logistic", "linear", "poisson"),
      startbeta=0, startgamma=0, fold=8,epsilon = 1e-10, maxiter = Inf,
      standardize = FALSE, tol = .Machine$double.eps^0.25,trace = TRUE)

cv3<-optL2(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
      lambda1 = 0, minlambda2=0, maxlambda2=3, base2=0,fusedl = FALSE ,
      positive = FALSE, data=lung,
      model = c("cox", "logistic", "linear", "poisson"), startbeta=0,
      startgamma=0,fold=8, epsilon = 1e-10, maxiter=25, standardize = FALSE,
      tol = .Machine$double.eps^0.25,trace = TRUE, approximate = FALSE)

cv4<-profL1(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
       minlambda1=0, maxlambda1=0, base1=0, lambda2 = 0,fusedl = FALSE,
       positive = FALSE, data=lung,
       model = c("cox", "logistic", "linear", "poisson"), startbeta=0,
       startgamma=0, fold=8,epsilon = 1e-10, maxiter = Inf, standardize = FALSE,
       steps = 100, minsteps = 100/3,log = FALSE, save.predictions = FALSE,
       trace = TRUE, plot = FALSE)

cv5<-profL2(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
       lambda1 = 0, minlambda2=5, maxlambda2=5, base2=0,fusedl = FALSE,
       positive = FALSE, data=lung,
       model = c("cox", "logistic", "linear", "poisson"), startbeta=0,
       startgamma=0, fold=8,epsilon = 1e-10, maxiter=25, standardize = FALSE,
       steps = 100, minsteps = 100/2,log = TRUE, save.predictions = FALSE,
       trace = TRUE, plot = FALSE, approximate = FALSE)

# Genelleştirilmiş doğrusal modellerin L1 (kement ve kaynaşmış kement) ve/veya L2 (sırt) cezaları veya ikisinin bir kombinasyonu ile takılması.
mdl<-penalized(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+
              lung$ph.karno+lung$pat.karno+lung$meal.cal+lung$wt.loss,
              unpenalized=~inst, lambda1=0, lambda2=0, positive = FALSE,
              data=lung, fusedl=FALSE,
              model = c("cox", "logistic", "linear", "poisson"),
              startbeta=0, startgamma=0, steps =1, epsilon = 1e-10,maxiter=25,
              standardize = FALSE, trace = TRUE)

p<-predict(object=mdl, penalized=lung$age[1:50]+lung$sex[1:50]+
          lung$ph.ecog[1:50]+lung$ph.karno[1:50]+lung$pat.karno[1:50]+
          lung$meal.cal[1:50]+lung$wt.loss[1:50], unpenalized=~lung$inst[1:50],
          data=lung[1:50,])
```




# 14. mclust

Bayes düzenlemesi ve boyut azaltma dahil olmak üzere model tabanlı
kümeleme, sınıflandırma ve yoğunluk tahmini için EM algoritması
aracılığıyla tahmin edilen Gauss sonlu karışım modelleri

```{r}
library(mclust)
#Mclust for clustering;
#Parametrelendirilmiş sonlu Gauss karışım modellerine dayalı model tabanlı kümeleme. #Modeller, hiyerarşik model tabanlı toplayıcı kümeleme tarafından başlatılan EM #algoritmasıyla tahmin edilir. Daha sonra BIC'e göre en uygun model seçilir.
mdl<-Mclust(data=life[,-1], G = 1:9, modelNames = NULL, prior = NULL,
       control = emControl(), initialization = NULL,
       warn = mclust.options("warn"), x = NULL, verbose = interactive())
p<-predict(object=mdl, newdata=life[1:25,-1])

# MclustDA for supervised classification;
#Gauss sonlu karışım modellemesine dayalı diskriminant analizi.
mdl1<-MclustDA(data=train_adlt[,-13], class=train_adlt[,13], G = 1:12,
         modelNames = "VVV", modelType = c("MclustDA", "EDDA"), prior = NULL,
         control = emControl(),initialization = NULL,
         warn = mclust.options("warn"),verbose = interactive())
p1<-predict(object=mdl1, newdata=test_adlt[,-13], prop = mdl1$prop)
table(p1$classification,test_adlt[,13])
Metrics::accuracy(test_adlt[,13],p1$classification)

# MclustSSC for semi-supervised classification;
#Gauss sonlu karışım modellemesine dayalı yarı denetimli sınıflandırma.
mdl2<-MclustSSC(data=train_adlt[,-13], class=train_adlt[,13], G = 2,
          modelNames = "VVV", prior = NULL,
          control = emControl(), warn = mclust.options("warn"),
          verbose = interactive())
p2<-predict(object=mdl1, newdata=test_adlt[,-13])
table(p2$classification,test_adlt[,13])
Metrics::accuracy(test_adlt[,13],p2$classification)

# densityMclust for density estimation
#Mclust'un Gauss sonlu karışım modelini kullanarak her veri noktası için bir yoğunluk tahmini üretir.
mdl3<-densityMclust(life[,-1], plot = TRUE)
p3<-predict(object=mdl3, newdata=life[1:25,-1], what = c("dens", "cdens", "z"),
            logarithm = FALSE)


#Model tabanlı hiyerarşik kümeleme ile başlatılan EM algoritması ile uyumlu parametrelendirilmiş Gauss karışım modelleri için BIC.
mclustBIC(data=life[,-1], G = 1:6, modelNames = "EII", prior = NULL,
          control = emControl(),
          initialization = list(hcPairs = NULL,subset = NULL,noise = NULL),
          Vinv = NULL, warn = mclust.options("warn"), x = NULL,
          verbose = interactive())

#Bir Gauss karışım modelinin parametreleri için standart hataların ve yüzdelik önyükleme güven aralıklarının önyükleme veya jackknife tahmini.
MclustBootstrap(object=mdl3, nboot = 99, type = c("bs", "wlbs", "pb", "jk"),
                max.nonfit = 10*99, verbose = interactive())

#Belirli bir sonlu karışım modeli parametreleştirmesinde karışım bileşenlerinin sayısını değerlendirmek için olabilirlik oranı testini (LRT) gerçekleştirin. Gözlemlenen anlamlılığa, olabilirlik oranı testi istatistiği (LRTS) için (parametrik) önyükleme kullanılarak yaklaşık değer hesaplanır.
mclustBootstrapLRT(data=life[,-1], modelName = "EII", nboot = 999, level = 0.05,
                   maxG = NULL, verbose = interactive())
```
0,80 - 0.80
modelName :c("E", "V","EII", "VII", "EEI", "EVI", "VEI", "VVI")




# 15. partykit

şunları içerir ('rpart', 'RWeka', 'PMML','party':cforest,ctree,mob)

```{r}
library('partykit')

#Temel öğreniciler olarak koşullu çıkarım ağaçlarını kullanan rastgele orman ve torbalama topluluğu algoritmalarının uygulanması
md1<-cforest(formula=income~., data=train_adlt, weights=NULL,
      subset=NULL, offset=NULL, cluster=NULL, strata=FALSE, na.action = na.omit,
      control = ctree_control(teststat = "quad", testtype = "Univ",
                              mincriterion = 0, saveinfo = FALSE),
        ytrafo = NULL, scores = NULL, ntree = 500L,
        perturb = list(replace = FALSE, fraction = 0.632),
        mtry = ceiling(sqrt(13)), applyfun = NULL, cores = NULL,trace = FALSE)

#Koşullu çıkarımda sürekli, sansürlenmiş, sıralı, nominal ve çok değişkenli yanıt değişkenleri için özyinelemeli bölümleme
md2<-ctree(formula=income~., data=train_adlt, subset=NULL,
           weights=NULL, na.action = na.pass, offset=NULL, cluster=NULL,
           control = ctree_control(teststat = "quad", testtype = "Univ",
                              mincriterion = 0, saveinfo = FALSE),
           ytrafo = NULL,converged = NULL, scores = NULL, doFit = TRUE)

#Genelleştirilmiş doğrusal modellere dayalı model tabanlı özyinelemeli bölümleme.
md3<-glmtree(formula=income~., data=train_adlt, subset=NULL,
             na.action=na.omit, weights=NULL, offset=NULL, cluster=NULL,
             family = gaussian, epsilon = 1e-8, maxit = 25, method = "glm.fit")

#En küçük kareler regresyonuna dayalı model tabanlı özyinelemeli bölümleme.
md4<-lmtree(formula=income~., data=train_adlt, subset=NULL,
            na.action=na.omit, weights=NULL, offset=NULL, cluster=NULL)

#MOB, her terminal düğümüyle ilişkili uygun modellere sahip bir ağaç sağlayan, model tabanlı özyinelemeli bölümlemeye yönelik bir algoritmadır. --NO PREDICT--
train_adlt1<-train_adlt
train_adlt1[,13]<-factor(train_adlt1[,13]-1)
f=income ~ 1|age+workclass + educatoin_num + marital_status + occupation + 
    relationship + race + sex + capital_gain + capital_loss + 
    hours_per_week + native_country #sabit uyumlu
f<- income  ~ workclass + educatoin_num + marital_status + occupation + 
    race + age + capital_gain + capital_loss + hours_per_week +
    native_country |sex+relationship # Xi:bölümleme and Zi:regresör variabels

logit <- function(y, x, start = NULL, weights = NULL, offset = NULL, ...) {
glm(y ~ 0 + x, family = binomial, start = start, ...)
}

md5<-mob(formula=f, data = train_adlt1, subset=NULL, na.action=na.omit,
         weights=NULL, offset=NULL, cluster=NULL,fit=logit,#coef,logLik,estfun
         control = mob_control()
        )

p1<-predict(md1, test_adlt[,-13])
Metrics::accuracy(test_adlt$income, ifelse(p1 < 1.5,1,2))

p2<-predict(md2, test_adlt[,-13])
Metrics::accuracy(test_adlt$income, ifelse(p2 < 1.5,1,2))

p3<-predict(md3, test_adlt[,-13])
Metrics::accuracy(test_adlt$income, ifelse(p3 < 1.5,1,2))

p4<-predict(md4, test_adlt[,-13])
Metrics::accuracy(test_adlt$income, ifelse(p4 < 1.5,1,2))
```




# 16. ipred

Dolaylı sınıflandırma yoluyla geliştirilmiş tahmin modelleri ve
sınıflandırma, regresyon ve hayatta kalma sorunları için torbalamanın
yanı sıra tahmin hatasının yeniden örneklemeye dayalı tahmin edicileri.

```{r}
library('ipred')
## S3 method for class 'factor'
m1<-ipredbagg(y=adult[1:30000,13], X=adult[1:30000,-13], nbagg=25,
          control=rpart::rpart.control(minsplit=2, cp=0, xval=0),
          comb=NULL, coob=FALSE, ns=length(adult[1:30000,13]), keepX = TRUE,
          na.action=na.omit)

## S3 method for class 'numeric'
m2<-ipredbagg(y=train_adlt[,13], X=train_adlt[,-13], nbagg=25,
              control=rpart::rpart.control(xval=0), comb=NULL, coob=FALSE,
              ns=length(train_adlt[,13]), keepX = TRUE)

## S3 method for class 'Surv'
m3<-ipredbagg(y=lung$status, X=lung[,-3], nbagg=25,
              control=rpart::rpart.control(xval=0), comb=NULL, coob=FALSE,
              ns=dim(lung)[1], keepX = TRUE)

## S3 method for class 'data.frame'
m4<-bagging(formula=income~., data=adult[1:30000,], subset=NULL,
            na.action=na.omit)

#k-Nearest Neighbour Classification
m5<-ipredknn(formula= income ~., data=train_adlt, k=5)

Metrics::accuracy(adult[30000:32560,13], predict(m1,adult[30000:32560,-13]))
Metrics::accuracy(test_adlt[,13], ifelse(predict(m2,test_adlt[,-13])<1.5,1,2))
Metrics::accuracy(adult[30000:32560,13], predict(m4,adult[30000:32560,-13]))
Metrics::accuracy(test_adlt[,13], predict(m5,test_adlt[,-13]))
```
randomForest ve cforest rastgele orman uygulamaları, önyükleme ile toplanan ağaçları hesaplamak için bu işlevden daha esnek ve güvenilirdir ve bunun yerine kullanılmalıdır. Ağaçları stabilize etmek için Breiman (1996a, 1998) tarafından sınıflandırma ve regresyon ağaçları için torbalama önerilmiştir. Bu işlevdeki ağaçlar, rpart paketindeki uygulama kullanılarak hesaplanır.



# 17. HiDimDA

Gözlemlerden (çok) daha fazla değişkene sahip problemler için güvenilir
kovaryans tahmin edicilerine dayalı olarak yüksek boyutlu problemlerde
doğrusal diskriminant analizi gerçekleştirir. Sınıflandırıcı eğitimi,
tahmin, çapraz doğrulama ve değişken seçimi için rutinler içerir.

```{r}
library(HiDimDA)
#Dlda: Çapraz Doğrusal Diskriminant Analizi
m1<-Dlda(data=train_adlt[,-13], grouping=factor(train_adlt[,13]),
  prior = "proportions",VSelfunct = SelectV,ldafun="classification")#"canonical"

#Slda: Küçültülmüş Doğrusal Diskriminant Analizi
m2<-Slda(data=train_adlt[,-13], grouping=factor(train_adlt[,13]),
     prior = "proportions", StddzData=TRUE,VSelfunct = SelectV,
     Trgt=c("CnstDiag","Idntty","VarDiag"),
     minp=20, ldafun="classification")#"canonical"

#Mlda: Maksimum Belirsizlik Doğrusal Diskriminant Analizi
m3<-Mlda(data=train_adlt[,-13], grouping=factor(train_adlt[,13]),
     prior = "proportions",StddzData=TRUE,VSelfunct = SelectV,
     ldafun="classification", #"canonical"
     PCAstep=FALSE)

#RFlda: Faktör-model Doğrusal Diskriminant Analizi
m4<-RFlda(data=train_adlt[,-13], grouping=factor(train_adlt[,13]), q = 1,
      prior = "proportions",CorrAp = TRUE, maxq=5, VSelfunct = SelectV,
      ldafun="classification", nstarts = 1, #"canonical"
      CVqtrials=1:3, CVqfolds=3, CVqrep=1, CVqStrt=TRUE)

#SelectV: Denetimli sınıflandırma için Yüksek Boyutlu değişken seçimi
SelectV(data=train_adlt[,-13], grouping=factor(train_adlt[,13]),
        Selmethod=c("ExpHC","HC","Fdr","Fair","fixedp"),
        NullDist=c("locfdr","Theoretical"), uselocfdr=c("onlyHC","always"),
        minlocfdrp=200, comvar=TRUE, Fdralpha=0.5,ExpHCalpha=0.5,
        HCalpha0=0.1, maxp=ncol(train_adlt), tol=1E-12)

Metrics::accuracy(test_adlt[,13],
  predict(m1, test_adlt[,-13],grpcodes=levels(factor(train_adlt$income)))$class)
Metrics::accuracy(test_adlt[,13],
  predict(m2, test_adlt[,-13],grpcodes=levels(factor(train_adlt$income)))$class)
Metrics::accuracy(test_adlt[,13],
  predict(m3, test_adlt[,-13],grpcodes=levels(factor(train_adlt$income)))$class)
Metrics::accuracy(test_adlt[,13],
  predict(m4, test_adlt[,-13],grpcodes=levels(factor(train_adlt$income)))$class)
```
0.81
0.81
0.81
0.81


# 18. mboost

Genelleştirilmiş doğrusal, toplamsal ve etkileşim modellerini potansiyel
olarak yüksek boyutlu verilere uydurmak için temel öğreniciler olarak
bileşen bazında (cezalandırılmış) en küçük kareler tahminlerini veya
regresyon ağaçlarını kullanan genel risk fonksiyonlarını optimize etmeye
yönelik fonksiyonel gradyan iniş algoritması (artırma).

```{r}
library('mboost')

# gamboost for boosted (generalized) additive models,
m1<-mboost(formula= factor(income) ~ btree(age)+bols(capital_gain)+
             bbs(hours_per_week),
       data = data.frame(train_adlt), na.action = na.omit, weights = NULL,
       offset = NULL, family = Binomial(type = "adaboost",link = "logit"),
       control = boost_control(), oobweights = NULL, baselearner = c("btree"))
        #"bss", "bns" "bbs", "bols"


m2<-gamboost(formula=factor(income)~., data = adlt, na.action = na.omit,
             weights = NULL, offset = NULL,
             family = Binomial(type = "adaboost",link = "logit"),
             control = boost_control(), oobweights = NULL, baselearner ="bols",
             # "bbs", "bols", "btree"
             dfbase = 4)

# glmboost :Temel öğreniciler olarak bileşen bazında doğrusal modellerin kullanıldığı keyfi kayıp fonksiyonlarını optimize etmek için gradyan artırma.
m3<-glmboost(age ~ csMPa, data = beton[,8:9], weights = NULL,
         offset = NULL, family = Gaussian(),
         na.action = na.pass, contrasts.arg = NULL, center = FALSE,
         control =  boost_control(mstop = 2000), oobweights = NULL)

# blackboost for boosted trees.
m4<-blackboost(age ~ csMPa, data = beton[,8:9], weights = NULL,
           na.action = na.pass, offset = NULL, family = Gaussian(),
           control = boost_control(), oobweights = NULL,
           tree_controls = partykit::ctree_control(teststat = "quad",
                                                   testtype = "Teststatistic",
                                                   mincriterion = 0,
                                                   minsplit = 10,
                                                   minbucket = 4,
                                                   maxdepth = 2,
                                                   saveinfo = FALSE))

Metrics::accuracy(actual = adlt[1:100,13],
  predict(object=m1, newdata = adlt[1:100,-13], type = "class",
        which = NULL, aggregate = c("sum", "cumsum", "none")))

Metrics::accuracy(actual = adlt[1:100,13],
predict(object=m2, newdata = adlt[1:100,-13], type = "class",#"link","response"
        which = NULL, aggregate = c("sum", "cumsum", "none")))


predict(object=m3, newdata = beton[1:25,8:9], type = "response",#"link","class"
        which = NULL, aggregate = c("sum", "cumsum", "none"))

predict(object=m4, newdata = beton[1:25,8:9], type = "link",#"response","class"
        which = NULL, aggregate = c("sum", "cumsum", "none"))

varimp(m1)
print(m1)
coef(m1)
```

FAMILY: AdaExp() AUC() Binomial(type = c("adaboost", "glm"), link =
c("logit", "probit", "cloglog", "cauchit", "log"), ...) GaussClass()
GaussReg() Gaussian() Huber(d = NULL) Laplace() Poisson()
GammaReg(nuirange = c(0, 100)) CoxPH() QuantReg(tau = 0.5, qoffset =
0.5) ExpectReg(tau = 0.5) NBinomial(nuirange = c(0, 100))
PropOdds(nuirange = c(-0.5, -1), offrange = c(-5, 5)) Weibull(nuirange =
c(0, 100)) Loglog(nuirange = c(0, 100)) Lognormal(nuirange = c(0, 100))
Gehan() Hurdle(nuirange = c(0, 100)) Multinomial() Cindex(sigma = 0.1,
ipcw = 1) RCG(nuirange = c(0, 1), offrange = c(-5, 5))



# 19. survival

Aşağıdakiler dahil temel hayatta kalma analizi rutinlerini içerir: Surv
nesnelerinin tanımı, Kaplan-Meier ve Aalen-Johansen (çok durumlu)
eğrileri, Cox modelleri, ve parametrik hızlandırılmış arıza süresi
modelleri

```{r}
library('survival')
#data(cancer, package = 'survival') 11 dataset

#hayatta kalma nesnesi oluşturun.
S<-Surv(time=lung$time, event=lung$status-1, #time2,
     type=c('right'),origin=0)#'left','interval','counting','interval2','mstate'

#Bu işlev, bir formülden, önceden takılmış bir Cox modelinden veya önceden takılmış hızlandırılmış arıza süresi modelinden hayatta kalma eğrileri oluşturur.
s1 <- survfit(S ~ 1, data = lung) ##Surv(time, status)

#Her katman için bir eğri olacak şekilde bir hayatta kalma eğrileri grafiği oluşturulur
plot(s1, conf.int=.95, mark.time=FALSE, pch=3, col=1, lty=1, lwd=1, cex=1,
     log=FALSE, xscale=1, yscale=1, ylim=1, fun="S",#, xmax=FALSE, xlim=FALSE
     xlab = "Days", ylab = "Overall survival probability", xaxs="r",
     conf.times=1, conf.cap=.005,conf.offset=.012,
     conf.type = c("log"),# "log-log", "plain", "logit", "arcsin"),
     mark=NULL, noplot="(s0)", cumhaz=FALSE)#,firstx, ymin=0

#bir yıl hayatta kalma olasılığı
summary(s1, times = 365.25)
summary(s1, times = c(1,30,60,90*(1:10)))

#iki veya daha fazla hayatta kalma eğrisi arasında fark olup olmadığını test eder
survdiff(formula= S ~ sex, data=lung)#,
#subset=c("inst","age","ph.ecog","ph.karno","pat.karno","meal.cal","wt.loss"),#c(1,4,6:10),
#na.action=na.omit, rho=0, timefix=FALSE)

#regresyon hesabı-Cox Orantılı Tehlikeler Modeli
coxph(S ~ sex, data = lung)



#Aalen'in sansürlenmiş veriler için eklemeli regresyon modeli
aareg(formula=S~sex, data=lung, weights=NULL, subset=NULL,
      na.action=NULL,qrtol=1e-07, nmin=3, dfbeta=FALSE,
      taper=1,test = c('aalen'),# 'variance', 'nrisk'),
      cluster=NULL,model=FALSE, x=FALSE, y=FALSE)



#Parametrik Hayatta Kalma Modeli için Regresyon
s2<-survreg(formula=S~sex + ph.ecog, data=lung, weights=NULL,
        subset=NULL,na.action=na.omit, dist="weibull", init=NULL, scale=0,
        control=survreg.control(),parms=NULL,model=FALSE, x=FALSE,y=TRUE,
        robust=FALSE, cluster=NULL, score=FALSE)

predict(object=s2, newdata = lung[1:25, c(3,5:6)],
        type=c("response"),#"link","lp","linear","terms","quantile","uquantile"
        se.fit=FALSE, terms=NULL, p=c(0.1, 0.9), na.action=na.pass)
```
survival time: bir olayın gerçekleşmesine kadar geçen süre. Örneğin: bir bireyin belirli bir takip dönemi boyunca "hayatta kaldığı" süre

survival probability: Bir bireyin o zamandan hemen önce hayatta kaldığı göz önüne alındığında, belirli bir sürenin ötesinde hayatta kalmanın koşullu olasılığı.

failure: ilgilenilen olay genellikle ölüm, hastalık vakası veya başka bir olumsuz bireysel deneyimdir.

censoring: bir denek, veri toplamanın sonunda ilgilenilen olayı deneyimlememişse meydana gelir. Hayatta kalma analizine özgü bir tür eksik veri sorunudur. Hayatta kalma süresini tam olarak bilmiyoruz çünkü çalışma sona eriyor, kişi takipten çıkıyor veya çalışmadan çekiliyor

right-censored: gerçek hayatta kalma süresi, gözlemlenen hayatta kalma süresine eşit veya bundan daha fazladır

left-censored: gerçek hayatta kalma süresi, gözlemlenen hayatta kalma süresinden daha az veya ona eşittir

interval-censored: gerçek hayatta kalma süresi bilinen bir zaman aralığı dahilindedir

Terminoloji: time = hayatta kalma süresi; event = başarısızlık. İki olası sonucumuz/durumumuz var: event/failure (1) and censored (0)
::olay/başarısızlık(1) ve sansürlendi(0).



# 20. party

Özyinelemeli bölümleme için hesaplamalı bir araç kutusu. Paketin özü,
ağaç yapılı regresyon modellerini iyi tanımlanmış bir koşullu regresyon
teorisine yerleştiren koşullu çıkarım ağaçlarının bir uygulaması olan
çıkarım prosedürleri ctree()'dir.Bu parametrik olmayan regresyon
ağaçları sınıfı, nominal, sıralı, sayısal, sansürlü ve ayrıca çok
değişkenli yanıt değişkenleri ve ortak değişkenlerin keyfi ölçüm
ölçekleri dahil olmak üzere her türlü regresyon problemine
uygulanabilir.

```{r}
library('party')

#Yanıt veya Giriş Değişkenlerinin Dönüşümleri
ptrafo<-ptrafo(data=adlt,
       numeric_trafo = function(x) scale(x),
       factor_trafo = function(x) factor(x),
       ordered_trafo = NULL, surv_trafo = NULL, var_trafo = NULL)

#Temel öğreniciler olarak koşullu çıkarım ağaçlarını kullanan rastgele orman ve torbalama topluluğu algoritmalarının uygulanması
m1<-cforest(formula=income~., data = train_adlt, subset = NULL,
            weights = NULL, controls = cforest_unbiased(), xtrafo = ptrafo,
            ytrafo = ptrafo, scores = NULL)

#table(test_adlt[,13], ifelse(predict(m1,test_adlt[,-13], OOB = TRUE))<1.5,1,2)
Metrics::accuracy(test_adlt[,13],
                  ifelse(predict(m1,test_adlt[,-13], OOB = TRUE)<1.5,1,2))

#Koşullu çıkarım çerçevesinde sürekli, sansürlü, sıralı, nominal ve çok değişkenli yanıt değişkenleri için yinelemeli bölümleme
m2<-ctree(formula=income~., data = train_adult, subset = NULL,
          weights = NULL, controls = ctree_control(), xtrafo = ptrafo,
          ytrafo = ptrafo, scores = NULL)

#table(ifelse(predict(m2,data.frame(test_adlt)) <1.5,1,2), test_adlt[,13])
Metrics::accuracy(test_adlt[,13],
                  ifelse(predict(m2,test_adlt[,-13], OOB = TRUE)<1.5,1,2))

#MOB, her terminal düğümüyle ilişkili uygun modellere sahip bir ağaç sağlayan, model tabanlı özyinelemeli bölümlemeye yönelik bir algoritmadır.
m3<-mob(formula=income~age+workclass+educatoin_num+marital_status+
          occupation+relationship+race+sex | capital_gain+capital_loss |
          hours_per_week+native_country,
          data = data.frame(train_adlt), weights=NULL, na.action = na.omit,
          model = glinearModel, control = mob_control(minsplit = 50))

table(ifelse(predict(m3,data.frame(test_adlt)) <1.5,1,2), test_adlt[,13])
Metrics::accuracy(test_adlt[,13],
                  ifelse(predict(m3,test_adlt[,-13], OOB = TRUE)<1.5,1,2))
```




# 21. grpreg

Gruplandırılmış cezalarla doğrusal regresyon, GLM ve Cox regresyon
modellerinin düzenlileştirme yolunu uydurmak için etkili algoritmalar.
Bu, grup kementi, grup MCP ve grup SCAD gibi grup seçim yöntemlerinin
yanı sıra grup üstel kement, bileşik MCP ve grup köprüsü gibi iki
düzeyli seçim yöntemlerini içerir.

```{r}
library('grpreg')
#gBridge :Grup köprüsü regresyon yolunu sığdır
gr<-gBridge(X=adlt[,-13], y=adlt[,13], group=1:12, family="binomial")
#  c("gaussian","poisson"), nlambda=100, lambda,
#  lambda.min={if (nrow(X) > ncol(X)) .001 else .05}, lambda.max, alpha=1,
#  eps=.001, delta=1e-7, max.iter=10000,
#  gamma=0.5, group.multiplier, warn=TRUE, returnX=FALSE, ...)

#grpreg :Düzenlileştirme parametresi lambda için bir değerler tablosu üzerinde gruplandırılmış cezalara sahip modeller için düzenleme yollarını sığdırın. Doğrusal ve lojistik regresyon modellerine uyar.
mdl1<-grpreg(X=adlt[,-13], y=adlt[,13],
       group=gr$group, #1:ncol(X),
       penalty="grLasso", #"grMCP", "grSCAD","gel", "cMCP"
       family=gr$family,#c("gaussian", "binomial", "poisson"),
       lambda=gr$lambda,
       group.multiplier=gr$group.multiplier,
       nlambda=100, log.lambda = TRUE, alpha=1, eps=1e-4, max.iter=10000,
       lambda.min={if (nrow(adlt[,-13]) > ncol(adlt[,-13])) 1e-4 else .05},
       dfmax=5, gmax=12, #length(unique(group)),
       gamma=3,#ifelse(penalty == "grSCAD", 4, 3),
       tau = 1/3, warn=TRUE, returnX = FALSE)

#grpsurv :Düzenlileştirme parametresi lambda için bir değerler tablosu üzerinde gruplandırılmış cezalarla Cox modelleri için düzenlileştirme yollarını sığdırın.
data(Lung)
mdl2<-grpsurv(X=Lung$X, y=Lung$y, group=Lung$group,#1:ncol(X),
              penalty="grLasso",#"grMCP", "grSCAD","gel", "cMCP"
              gamma=3,#ifelse(penalty=="grSCAD", 4, 3),
              alpha=1, nlambda=100, lambda=0.02,
              lambda.min={if (nrow(Lung$X) > ncol(Lung$X)) 0.001 else .05},
              eps=.001, max.iter=10000, dfmax=5, gmax=6,#length(unique(group)),
            tau=1/3, group.multiplier=c(1,1,1,1,1,1), warn=TRUE, returnX=FALSE)

p<-predict(mdl1, X=as.matrix(adlt[1:200,-13]), type="class")
table(adlt[1:200,13],p+1)

predict(mdl2, Lung$X[1:25,], type = "median")
```



# 22. HDclassif

Yüksek boyutlu verilerin düşük boyutlu farklı alt uzaylarda yaşadığı
varsayımına dayanan, yüksek boyutlu veriler için diskriminant analizi ve
veri kümeleme yöntemleri, boyut azaltma fikirlerini ve modeldeki
kısıtlamaları birleştiren Gauss karışım modelinin yeni bir
parametrelendirmesini önermektedir.

```{r}
library(HDclassif)

#Yüksek Boyutlu Diskriminant Analizi
md1<-hdda(data=train_adlt[,-13],cls=train_adlt[,13],model = "AkjBkQkDk",
     graph = FALSE,d_select = "Cattell",threshold = 0.2,com_dim = 2,
     show = getHDclassif.show(),scaling = FALSE,cv.dim = 1:10,
     cv.threshold = c(0.001, 0.005, 0.05, 1:9 * 0.1),cv.vfold = 10,
     LOO = FALSE,noise.ctrl = 1e-08)

#Yüksek Boyutlu Veri Kümeleme
md2<-hddc(data=train_adlt[,-13],K = 1:2,model = c("AkjBkQkDk"),threshold = 0.2,
     criterion = "bic",com_dim = 2,itermax = 10,eps = 0.001,algo = "CEM",
     d_select = "BIC",init = "kmeans",init.vector, show = getHDclassif.show(),
     mini.nb = c(5, 10),scaling = T, min.individuals =10,noise.ctrl = 1e-08,
     mc.cores = 1,nb.rep = 1, keepAllRes = TRUE,
     kmeans.control = list(iter.max=200,nstart=10,algorithm="Lloyd"),
     d_max = 100,subset = Inf)#'Hartigan-Wong', 'Lloyd', 'Forgy' or 'MacQueen'

#HD Gaussianlarla Karışım Diskriminant Analizi
md3<-hdmda(X=train_adlt[,-13],cls=train_adlt[,13],K=1:2,model='AkjBkQkDk',
           show=FALSE)

Metrics::accuracy(test_adlt[,13], predict(md1,test_adlt[,-13])$class)
Metrics::accuracy(test_adlt[,13],
                  predict(object=md2, data=test_adlt[,-13], cls = NULL)$class)
Metrics::accuracy(test_adlt[,13],
                  as.numeric(predict(object=md3, X=test_adlt[,-13])$class))
```
0.80
0.26
0.78



# 23. mgcv

Genelleştirilmiş toplamsal (karma) modeller, bunların bazı uzantıları ve
(Kısıtlı) Marjinal Olabilirlik, Genelleştirilmiş Çapraz Doğrulama ve
benzeri yoluyla çoklu yumuşatma parametresi tahminiyle veya tamamen
Bayes çıkarımı için yinelenmiş iç içe Laplace yaklaşımı kullanılarak
diğer genelleştirilmiş sırt regresyonu

```{r}
library(mgcv)
f=formula(x=csMPa~cement+ s(water)+ti(age)+te(slag)+te(flyash)+
            te(superplasticizer)+s(coarseaggregate)+s(fineaggregate))#s,te,ti,t2

c=gam.control(nthreads=1,ncv.threads=1,irls.reg=0.0,epsilon = 1e-07,
    maxit = 200,mgcv.tol=1e-7,mgcv.half=15, trace = FALSE,
    rank.tol=.Machine$double.eps^0.5,nlm=list(),
    optim=list(),newton=list(),
    idLinksBases=TRUE,scalePenalty=TRUE,efs.lspmax=15,
    efs.tol=.1,keepData=FALSE,scale.est="fletcher",
    edge.correct=FALSE)


#Çok büyük veri kümeleri için genelleştirilmiş toplamsal modeller
m1<-bam(formula=f,family=gaussian(),data=beton,weights=NULL,subset=NULL,
    na.action=na.omit, offset=NULL,method="fREML",control=c,
    select=FALSE,scale=0,gamma=1,knots=NULL,sp=NULL,min.sp=NULL,
    paraPen=NULL,chunk.size=10000,rho=0,AR.start=NULL,discrete=FALSE,
    cluster=NULL,nthreads=1,gc.level=0,use.chol=FALSE,samfrac=1,
    coef=NULL,drop.unused.levels=TRUE,G=NULL,fit=TRUE,drop.intercept=NULL,
    in.out=NULL)

#Entegre düzgünlük tahminine sahip genelleştirilmiş toplamsal modeller
m2<-gam(formula=f,family=gaussian(),data=beton,weights=NULL,subset=NULL,
    na.action=na.omit,offset=NULL,method="GCV.Cp",
    optimizer=c("outer","newton"),control=c,scale=0,
    select=FALSE,knots=NULL,sp=NULL,min.sp=NULL,H=NULL,gamma=1,
    fit=TRUE,paraPen=NULL,G=NULL,in.out,drop.unused.levels=TRUE,
    drop.intercept=NULL,nei=NULL,discrete=FALSE)


#Genelleştirilmiş Katkı Karma Modelleri
m3<-gamm(formula=f,random=NULL,correlation=NULL,family=gaussian(),
    data=beton,weights=NULL,subset=NULL,na.action=na.omit,knots=NULL,
    control=list(niterEM=0,optimMethod="L-BFGS-B",returnObject=TRUE),
    niterPQL=20,verbosePQL=TRUE,method="ML",drop.unused.levels=TRUE,
    mustart=NULL, etastart=NULL)

#Başka Bir Gibbs Katkı Modelleyicisi: mgcv için JAGS desteği.
#jagam(formula=f,family=gaussian,data=beton,file="dnm_jagam.txt",weights=NULL,
#      na.action=na.omit,offset=NULL,knots=NULL,sp=NULL,drop.unused.levels=TRUE,
#      control=gam.control(),centred=TRUE,sp.prior = "gamma",diagonalize=FALSE)


bam.prd<-predict(object=m1,newdata=beton[1:100,-9],type="link",se.fit=FALSE,
    terms=NULL, exclude=NULL,block.size=50000,newdata.guaranteed=FALSE,
    na.action=na.pass,cluster=NULL,discrete=TRUE,n.threads=1,gc.level=0)

gam.prd<-predict(object=m2,newdata=beton[1:100,-9],type="link",se.fit=FALSE,
    terms=NULL, exclude=NULL,block.size=NULL,newdata.guaranteed=FALSE,
    na.action=na.pass,unconditional=FALSE,iterms.type=NULL)

Metrics::mae(beton$csMPa[1:100], bam.prd)
Metrics::mse(beton$csMPa[1:100], gam.prd)
```
5.25
42.1

FAMILY:
Tweedie,negbin,betar,cnorm,nb,ocat,scat,tw,ziP,gfam,cox.ph,gammals,gaulss,
gevlss,gumbls,multinom,mvn,shash,twlss,ziplss

METHOD:
"GCV.Cp","GACV.Cp","NCV","QNCV","REML","P-REML","ML","P-ML","REML","ML","NCV","QNCV"

OPTIMIZER: "outer","newton","bfgs","optim","nlm","efs"

FORMULA: s:smooth, te:tensor smooth, ti:tensor product interaction,
t2:alternative tensor product smooth

BETON VERİSİNDEKİ DEĞİŞKENLERİN YANIT DEĞİŞKENİ İLE ETKİLEŞİMİ

```{r}
par(mar=c(2,2,2,2))
for(i in 1:8){plot(x=beton[,i], y=beton$csMPa, main = names(beton)[i])}
```





# 24. pls

Çok değişkenli regresyon yöntemleri, Kısmi En Küçük Kareler Regresyon
(PLSR), Temel Bileşen Regresyon (PCR) ve Kanonik Güçlendirilmiş Kısmi En
Küçük Kareler (CPPLS).
NOT***PLS denetimli bir prosedürdür, oysa PCA denetlenmiyor.***
--YENİ DEĞER İÇİN TAHMİN YAPMIYOR---
```{r}
library(pls)

#kısmi en küçük kareler regresyonu (PLSR), kanonik destekli kısmi en küçük kareler (CPPLS) veya temel bileşen regresyonu (PCR) gerçekleştirme işlevleri
m1<-mvr(formula = beton$csMPa ~ msc(as.matrix(beton[, c(1:8)])),
    ncomp = 8, data = beton, validation = "CV", #"LOO""none"
    method = pls.options()$plsralg)
    #Y.add=NULL, subset=NULL, na.action=NULL,scale = FALSE, center = TRUE,
    #model = F,x = FALSE,y = FALSE)

m2<-mvr(formula = beton$csMPa ~ msc(as.matrix(beton[, c(1:8)])),
    ncomp = 8, data = beton, validation = "CV", #"LOO""none"
    method = pls.options()$pcralg)

m3<-mvr(formula = beton$csMPa ~ msc(as.matrix(beton[, c(1:8)])),
    ncomp = 8, data = beton, validation = "CV", #"LOO""none"
    #Y.add=rep(1,nrow(beton)), weights=NULL,
    method = pls.options()$cpplsalg)

##note
#pls.options(list(mvralg = "kernelpls", plsralg = "kernelpls",cpplsalg ="cppls",
#                 pcralg = "svdpc", parallel = NULL, w.tol= .Machine$double.eps,
#                 X.tol = 10^-12))


crossval(object=m1,segments = 10, segment.type = "random",
         #"consecutive","interleaved"
         length.seg=1,jackknife = FALSE,trace = 15)


#---------------------
#Fits a PLS model using the CPPLS algorithm.
md1<-cppls.fit(X=as.matrix(beton[,-9]),Y=beton[,9],ncomp=8,Y.add = NULL,
               center = TRUE, stripped = FALSE,lower = 0.5, upper = 0.5,
               trunc.pow = FALSE,weights = NULL)

md2<-simpls.fit(X=as.matrix(beton[,-9]),Y=beton[,9],ncomp=8, center = TRUE,
           stripped = FALSE)
md3<-svdpc.fit(X=as.matrix(beton[,-9]),Y=beton[,9],ncomp=8, center = TRUE,
          stripped = FALSE)

#Fits a PLSR model with the kernel algorithm.
md4<-kernelpls.fit(X=as.matrix(beton[,-9]),Y=beton[,9], ncomp=8, center = TRUE,
              stripped = FALSE)
#------------------

#Prediction for mvr (PCR, PLSR) models.
predict(object=m1, newdata=beton, ncomp = 1:m1$ncomp,
        comps=8, type = "response",na.action = na.pass)# "scores"
predict(object=m2, newdata=beton, ncomp = 1:m2$ncomp,
        comps=8, type = "response",na.action = na.pass)# "scores"
predict(object=m3, newdata=beton, ncomp = 1:m3$ncomp,
        comps=8, type = "response",na.action = na.pass)# "scores"
```





# 25. abess

En iyi alt küme seçimi problemini çözmek için son derece etkili araç
seti. Paket, doğrusal model için polinom zamanlarında tam destek
kurtarmayı ve küresel olarak en uygun çözümü garanti etmek için yeni bir
sıralama ve birleştirme tekniğinden yararlanan şekilde tasarlanan
algoritmaları uygular ve genelleştirir. Ayrıca lojistik regresyon,
Poisson regresyon, Cox orantılı risk modeli, Gama regresyon, çoklu yanıt
regresyon, çok terimli lojistik regresyon, sıralı regresyon, (sıralı)
temel bileşen analizi ve sağlam temel bileşen analizi için en iyi alt
küme seçimini destekler. Grup seçiminin en iyi alt kümesi ve kesin
bağımsızlık taraması gibi diğer değerli özellikler de sağlanmaktadır.

```{r}
library('abess')

# Uyarlanabilir en iyi alt küme seçimi (genelleştirilmiş doğrusal model için)
mdl<-abess(x=train_adlt[,-13],y=train_adlt[,13],family = c("binomial"),
 # "gaussian", "poisson", "cox", "mgaussian","multinomial","gamma", "ordinal"
 tune.path = c("sequence", "gsection"),tune.type = c("cv"),#gic,ebic,bic,aic
 weight = NULL,normalize = NULL,fit.intercept = TRUE,
 beta.low = -.Machine$double.xmax, beta.high = .Machine$double.xmax,c.max = 2,
 support.size = NULL,gs.range = NULL, lambda = 0,always.include = NULL,
 group.index = NULL,init.active.set = NULL, splicing.type = 2,
 max.splicing.iter = 20,screening.num = NULL,important.search = NULL,
 warm.start = TRUE,nfolds = 5,foldid = NULL,cov.update = FALSE,
 newton = c("exact", "approx"),newton.thresh = 1e-06,max.newton.iter = NULL,
 early.stop = FALSE,ic.scale = 1,num.threads = 0,seed = 1,subset="fnlwgt")

# Temel bileşen analizi için uyarlanabilir en iyi alt küme seçimi
ml1<-abesspca(x=adlt[,-13],type = c("predictor", "gram"),
              sparse.type = c("fpc", "kpc"),cor = FALSE, kpc.num = NULL,
              support.size = NULL,gs.range = NULL,
              tune.path = c("sequence", "gsection"),
              tune.type = c("gic", "aic", "bic", "ebic", "cv"),
              nfolds = 5,foldid = NULL,ic.scale = 1,c.max = NULL,
              always.include = NULL, group.index = NULL,screening.num = NULL,
              splicing.type = 1, max.splicing.iter = 20,warm.start = TRUE,
              num.threads = 0)

Metrics::accuracy(actual = test_adlt[,13],
                  predicted = ifelse(
                    predict(mdl, test_adlt[,-13], type = "response")<.5,
                    1,2))

plot(ml1)
extract(mdl)
```

y yanıt değişkeni: -"binom" iki seviyeye sahip olmalıdır. -'poisson'
için y pozitif tam sayıya sahip bir vektör olmalıdır. -"cox" için y,
hayatta kalma paketi tarafından döndürülen bir Surv nesnesi (önerilir)
veya "time" ve "status" adlı sütunları olan iki sütunlu bir matris
olmalıdır. -"mgaussian" için, y niceliksel yanıtların bir matrisi
olmalıdır. -"multinomial" veya "ordinal" için, y en az üç seviyeli bir
faktör olmalıdır. "Binom", "ordinal" veya "multinomial" için, eğer y
sayısal bir vektör olarak sunulursa, bir faktöre dönüştürüleceğini
unutmayın.





# 26. adabag

Applies Multiclass AdaBoost.M1, SAMME and Bagging

```{r}
library('adabag')
#Torbalama algoritmasını bir veri kümesine uygular
mdl1<-bagging(formula=income~., data=adult, mfinal = 100, control=control,
              par=FALSE)

#AdaBoost.M1 ve SAMME algoritmalarını bir veri kümesine uygular
mdl2<-boosting(formula=income~., data=adult[1:5000,], boos = TRUE, mfinal = 100,
               coeflearn = 'Breiman', control=control)#’Freund’ ’Zhu’

p1<-predict(object=mdl1, newdata=adult[sample(nrow(adult),200,replace = F),],
            newmfinal=length(mdl1$trees))
p2<-predict(object=mdl2, newdata=adult[sample(nrow(adult),200,replace = F),],
            newmfinal=length(mdl2$trees))
p1$confusion;p2$confusion
```





# 27. randomForestSRC

Tek değişkenli, çok değişkenli, denetimsiz, hayatta kalma, rekabet eden
riskler, sınıf dengesizliği sınıflandırması ve niceliksel regresyon için
Breiman'ın rastgele ormanlarının hızlı OpenMP paralel hesaplanması. Yeni
Mahalanobis'in ilişkili sonuçlar için bölünmesi. Aşırı rastgele ormanlar
ve rastgele bölme. Eksik veriler için atama yöntemleri paketi. Alt
örneklemeyi kullanan hızlı rastgele ormanlar. Değişken önemi için güven
bölgeleri ve standart hatalar. Yeni geliştirilmiş bekletme önemi. Duruma
özel önem. Minimum derinlik değişkeninin önemi.

```{r}
library('randomForestSRC')
#veri hazırlama
adlt<-adult#clear id column
adlt <- replace(adlt,adlt=="?",NA)#replace ? to NA
adlt <- droplevels(adlt)#update levels
#impute NA cells
adlt <- impute(formula=NULL, data=adlt,
              ntree = 100, nodesize = 3, nsplit = 10,
              nimpute = 2, fast = FALSE, blocks=NULL,
              mf.q=0.5, max.iter = 10, eps = 0.01,
              ytry = NULL, always.use = NULL, verbose = TRUE)


#rfsrc Hayatta Kalma, Regresyon ve Sınıflandırma için Hızlı Birleşik Rastgele Ormanlar
md<-rfsrc(formula=income~., data=adlt, ntree = 500, mtry = 5, ytry = 1,
          nodesize = 5, nodedepth = NULL, splitrule = NULL, nsplit = 10,
          importance = "none",#c(FALSE, TRUE, "none", "anti", "permute", "random"),
          block.size = if (any(is.element(as.character("none"),
                                      c("none", "FALSE")))) NULL else 10,
      bootstrap = "none",#c("by.root", "none", "by.user"),
      samptype = "swr",#c("swor", "swr"),
      samp = NULL, membership = FALSE,
      sampsize = if(samptype=="swor") function(x){x * .632} else function(x){x},
      na.action = "na.omit",#c("na.omit", "na.impute"),
      nimpute = 1, ntime = 150, cause=1,
      perf.type = "none", proximity = FALSE, distance = FALSE,forest.wt = FALSE,
      xvar.wt = NULL, yvar.wt = NULL, split.wt = NULL,case.wt =NULL,forest=TRUE,
      save.memory = FALSE,var.used = FALSE,#c(FALSE, "all.trees", "by.tree"),
      split.depth = FALSE,#c(FALSE, "all.trees", "by.tree"),
      seed = NULL, do.trace = FALSE, statistics = FALSE)

p<-predict(object=md, newdata=adlt[1:100,-13], m.target = NULL,
        importance = FALSE,#c(FALSE, TRUE, "none", "anti", "permute", "random"),
        get.tree = NULL,
        block.size = if (any(is.element(as.character(FALSE),
                                        c("none", "FALSE")))) NULL else 10,
        na.action = "na.omit",#c("na.omit", "na.impute", "na.random"),
        outcome = c("train"),# "test"),
        perf.type = NULL,  proximity = FALSE,  forest.wt = FALSE, ptn.count = 0,
        distance = FALSE, var.used = FALSE,#c(FALSE, "all.trees", "by.tree"),
        split.depth = FALSE,#c(FALSE, "all.trees", "by.tree"), seed = NULL,
        do.trace = FALSE, membership = FALSE, statistics = FALSE)

table(adlt[1:100,13], p$class)

#Tek değişkenli veya çok değişkenli nicelik regresyon ormanını büyütür ve koşullu nicelik ve yoğunluk değerlerini döndürür. Hem eğitim hem de test amaçlı kullanılabilir.
quantreg(formula=csMPa~., data=beton,# object=NULL, newdata=NULL,
         method = "forest", splitrule = NULL, prob = NULL, prob.epsilon = NULL,
         oob = TRUE, fast = FALSE, maxn = 1e3)

```



# 28. glmpath

L1 düzenli genelleştirilmiş doğrusal modeller ve Cox orantılı tehlike
modeli için yol izleyen bir algoritma.

```{r}
library('glmpath')

#Fits the entire L1 regularization path for Cox proportional hazards model
coxpath(data=list(x=as.matrix(lung[,-c(2:3)]),time=lung$time,
                  status=lung$status),
        nopenalty.subset = NULL, method ="efron")#"breslow"
#  lambda2 = 1e-5, max.steps = 10 * min(n, m), max.norm = 100 * m,
#  min.lambda = (if (m >= n) 1e-3 else 0), max.vars = Inf,
#  max.arclength = Inf, frac.arclength = 1, add.newvars = 1,
#  bshoot.threshold = 0.1, relax.lambda = 1e-7, approx.Gram = FALSE,
#  standardize = TRUE, eps = .Machine$double.eps, trace = FALSE)

#predict(object, data, s, type = c("coefficients", "loglik",
#  "lp", "risk", "coxph"), mode = c("step",
#  "norm.fraction", "norm", "lambda.fraction", "lambda"),
#  eps = .Machine$double.eps)

#Fits the entire L1 regularization path for generalized linear models
n<-dim(train_adlt)[1]; m<-dim(train_adlt)[2]-1

civi<-cv.glmpath(x=as.matrix(train_adlt[,-13]), y=train_adlt[,13]-1,
                 family = binomial,plot.it = F,se=F, weight = rep(1, n),
              offset = rep(0, n), nfold = 2,fraction = seq(0, 1, length = 100),
              type = "response",#"loglik"
              mode = "lambda")#"norm"

mdl<-glmpath(x=as.matrix(train_adlt[,-13]), y=train_adlt[,13]-1,
        nopenalty.subset = NULL, family = binomial,
        weight = rep(1, n), offset = rep(0, n), lambda2 = 1e-5,
        max.steps = 10 * min(n, m), max.norm = 100 * m,
        min.lambda = (if (m >= n) 1e-6 else 0), max.vars = Inf,
        max.arclength = Inf, frac.arclength = 1, add.newvars = 1,
        bshoot.threshold = 0.1, relax.lambda = 1e-8,
        standardize = F, eps = .Machine$double.eps, trace = T)

p<-predict(mdl, newx=as.matrix(test_adlt[,-13]), type = "response") #"link","response","loglik", "coefficients"  newy=test_adlt[,13]-1,
prd<-ifelse(apply(p,1,function(x) mean(x))<0.5,1,2)
Metrics::accuracy(test_adlt[,13],prd)
```



# 29. ncvreg

Kement veya dışbükey olmayan cezalar kullanan doğrusal regresyon, GLM ve
Cox regresyon modelleri için düzenleme yollarına uyar; özellikle
minimaks içbükey ceza (MCP) ve yumuşak bir şekilde kırpılmış mutlak
sapma (SCAD) cezası ve ilave L2 cezaları seçenekleri ("elastik ağ"
fikri).

```{r}
library('ncvreg')
n<-dim(adlt)[1]; p<-dim(adlt)[2]

#ncvreg Fit an MCP- or SCAD-penalized regression path
mdl1<-ncvreg(X=adlt[,-13],y=adlt[,13],family ="binomial",
             # c("gaussian", "binomial", "poisson"),
       penalty ="MCP",# c("MCP", "SCAD", "lasso"),
       gamma = switch("MCP", SCAD = 3.7, 3),
       alpha = 1,lambda.min = ifelse(n > p, 0.001, 0.05),nlambda = 100,
       lambda= 0.01,
       eps = 1e-04,max.iter = 10000,convex = TRUE,dfmax = p + 1,
       penalty.factor = rep(1, ncol(adlt)-1),warn = TRUE,returnX=F)

#ncvsurv Fit an MCP- or SCAD-penalized survival model
data("Lung");n=137;p=9
mdl2<-ncvsurv(X=Lung$X, y=Lung$y, penalty ="MCP",# c("MCP", "SCAD", "lasso")
        gamma = switch("MCP", SCAD = 3.7, 3), alpha = 1,
        lambda.min = ifelse(n > p, 0.001, 0.05), nlambda = 100,
        lambda=0.01, eps = 1e-04, max.iter = 10000, convex = TRUE,
        dfmax = p, penalty.factor = rep(1, ncol(Lung$X)), warn = TRUE,returnX=F)

reg<-predict(object=mdl1, X=as.matrix(adlt[1:100,-13]), type ="class")
# c("link", "response", "class", "coefficients", "vars", "nvars")
Metrics::accuracy(adlt[1:100,13]-1, reg)

surv<-predict(object=mdl2,X=Lung$X[1:50,], type="response")
# c("link", "response", "survival", "median", "coefficients", "vars", "nvars"),
```


# 30. OneR

İyileştirmelerle Tek Kurallı Makine Öğrenimi Sınıflandırma Algoritması

```{r}
library('OneR')
mdl<-OneR(formula = income~., data = train_adlt, ties.method = "first",
     verbose = FALSE)#"chisq"

#Bir veri çerçevesindeki tüm sayısal verileri, kesme noktalarının hedef kategorilerle en iyi şekilde hizalandığı kategorik bölmelere ayırır, böylece bir faktör döndürülür. Bir OneR modeli oluştururken bu, daha fazla doğruluğa sahip daha az kuralla sonuçlanabilir.
data_bin<-optbin(formula=income~., data=adult, method = "logreg",
       na.omit = TRUE)#, "infogain","naive")
OneR(income~., data_bin)


p<-predict(object=mdl, newdata=test_adlt[,-13], type = "class")
eval_model(prediction=p, actual=test_adlt[,13],
           dimnames = c("Prediction", "Actual"), zero.print = "0")
```
.78


# 31. arm

Regresyon ve Çok Düzeyli/Hiyerarşik Modeller Kullanarak Veri Analizi

```{r}
library(arm)

#Bayesian genelleştirilmiş doğrusal modeller.
mdl<-bayesglm (formula=formula(model.frame(income~.,data=train_adlt)),
          family = gaussian, data=train_adlt, weights=NULL, subset=NULL)#,
          #na.action=na.omit, start = NULL, etastart, mustart=20, offset=NULL,
          #control = list(maxit=10),
          #model = TRUE, method = "glm.fit", x = FALSE, y = TRUE,
          #contrasts = contr.bayes.unordered(n=2, base = 1, contrasts = TRUE),
          #drop.unused.levels = TRUE, prior.mean = 0,
          #prior.scale = NULL, prior.df = 1, prior.mean.for.intercept = 0,
          #prior.scale.for.intercept = NULL, prior.df.for.intercept = 1,
          #min.prior.scale=1e-12, scaled = TRUE, keep.order=TRUE,
          #drop.baseline=TRUE, maxit=100,print.unnormalized.log.posterior=FALSE,
          #Warning=TRUE)

#Bayes Sıralı Lojistik veya Probit Regresyon
ad<-adult #üç sınıf elde etmek için
ad$income <- cut(ad$capital_gain, breaks = c(0, 10000, 40000, 99999),
                 labels = c("<18K", "18K-40K", ">40K"),include.lowest = T)
mdl1<-bayespolr(formula=formula(model.frame(income~.,data=ad)),
          data=ad, weights = NULL, start=0, subset=NULL, na.action=na.omit,
          contrasts = NULL, Hess = TRUE, model = TRUE,
          method = "logistic", #"probit", "cloglog", "cauchit"
          drop.unused.levels=TRUE, prior.mean = 0, prior.scale = 2.5,
          prior.df = 1, prior.counts.for.bins = NULL, min.prior.scale=1e-12,
          scaled = TRUE, maxit = 100, print.unnormalized.log.posterior = FALSE)

p1<-predict(mdl, test_adlt[,-13],"link")
p2<-predict(mdl1, ad[1:1000,-13], "class")
#table(ifelse(p1<1.5,1,2), test_adlt$income)
#(mat<-table(p2, ad[1:1000,13]))
Metrics::accuracy(test_adlt$income,ifelse(p1<1.5,1,2))
mltest::ml_test(p2, ad[1:1000,13])#(accuracy <- sum(diag(mat)) / sum(mat))
```




# 32. kknn

Ağırlıklı k-En Yakın Komşular Sınıflandırması, Regresyon ve Spektral
Kümeleme

```{r}
library(kknn)
#fit and predict
fp<-kknn(formula = income~., train=train_adlt, test=test_adlt[,-13],
     na.action = na.omit(),k = 7, distance = 2, kernel = "optimal",
     ykernel = NULL, scale=TRUE,
     contrasts = c('unordered' = "contr.dummy", ordered = "contr.ordinal"))

#fitting
f<-train.kknn(formula = income~., data = train_adlt, kmax = 11, ks = NULL,
          distance = 2, kernel = "optimal", ykernel = NULL, scale = TRUE,
          contrasts = c('unordered' = "contr.dummy",ordered = "contr.ordinal"))

#cv.kknn(formula = income~., data = train_adlt, kcv = 10)

Metrics::accuracy(test_adlt$income, ifelse(fp$fitted.values<1.5,1,2))
```
0.82



# 33. plsRglm

Genelleştirilmiş doğrusal modeller için (ağırlıklandırılmış) Kısmi En
Küçük Kareler Regresyonunu ve çeşitli kriterler kullanılarak bu tür
modellerin tekrarlanan k-katlı çapraz doğrulamasını sağlar. Açıklayıcı
değişkenlerdeki eksik verilere izin verir. Bootstrap güven aralıkları
yapıları da mevcuttur.

```{r}
library(plsRglm)
#install.package("plsdof")
#Bu işlev, tam veya eksik veri kümeleri için bir tanesini dışarıda bırakan çapraz doğrulamayla Kısmi En Küçük Kareler Regresyon modellerini uygular
m1<-plsR(csMPa~., beton,
         nt = 2, limQ2set = 0.0975, dataPredictY=beton[,-9], modele = "pls",
         family = NULL, typeVC = "none", EstimXNA = FALSE, scaleX = TRUE,
         scaleY = NULL, pvals.expli = FALSE, alpha.pvals.expli = 0.05,
         MClassed = FALSE, tol_Xi = 10^(-12), weights=NULL, subset=NULL,
         contrasts = NULL, sparse = FALSE, sparseStop = TRUE, naive = FALSE,
         verbose=TRUE)

#Bu işlev Kısmi En Küçük Kareler Regresyon genelleştirilmiş doğrusal modellerini tam veya eksik veri kümelerini uygular.
m2<-plsRglm(csMPa~., beton, nt=2, limQ2set=.0975, dataPredictY=beton[,-9],
            modele="pls-glm-gaussian", family=NULL, typeVC="none",
            EstimXNA=FALSE, scaleX=F, scaleY=NULL, pvals.expli=FALSE,
            alpha.pvals.expli=.05, MClassed=FALSE, tol_Xi=10^(-12),weights=NULL,
            sparse=FALSE, sparseStop=TRUE, naive=FALSE, verbose=TRUE)

p1<-predict(object=m1, newdata=beton[1:25,-9], comps = m1$computed_nt,
type = c("response", "scores"),
weights,methodNA = "adaptative",verbose = TRUE)

p2<-predict(object=m2, newdata=beton[1:25,-9], comps = m2$computed_nt,
type = "response", #"link", "response", "terms", "scores", "class", "probs"
se.fit = FALSE,weights,dispersion = NULL,methodNA = "adaptative",verbose = TRUE)

Metrics::mae(beton[1:25,9], p1)
Metrics::mae(beton[1:25,9], p2)
```
10.8
13.4




# 34. spls

Seyrek kısmi en küçük kareler (SPLS) regresyonunu ve sınıflandırmasını uydurmak için işlevler sağlar
```{r}
library(spls)
#SGPLS, seyrek kısmi en küçük kareleri (SPLS) genelleştirilmiş bir doğrusal model (GLM) çerçevesine dahil ederek değişken seçimli PLS tabanlı sınıflandırma sağlar
#m1<-sgpls(x=beton[,-9], y=beton[,9]-1, K=2, eta=.5, scale.x=TRUE,
#      eps=1e-5, denom.eps=1e-20, zero.eps=1e-5, maxstep=100, br=TRUE,
#      ftype='hat')

#predict(object=m1, newx=test_adlt[,-13], type = c("fit","coefficient"),
#        fit.type = c("class","response"))#sgpls

#SPLS, aynı anda doğru tahmin ve değişken seçimi elde etmek için PLS'nin boyut azaltma adımına doğrudan seyreklik uygular
m2<-spls(x=beton[,-9], y=beton[,9]-1, K=2, eta=.5, kappa=0.5,
         select="simpls",#"pls2",
         fit="kernelpls",# "widekernelpls","simpls", or "oscorespls"
         scale.x=TRUE, scale.y=FALSE, eps=1e-4,
         maxstep=100, trace=FALSE)

Metrics::mae(beton[1:25,9], predict(object=m2, newx=beton[1:25,-9],type ="fit"))

#SPLSDA, seyrek kısmi en küçük kareler (SPLS) kullanarak PLS'nin boyut azaltma adımına doğrudan seyreklik uygulayarak, değişken seçimli PLS tabanlı sınıflandırma için iki aşamalı bir yaklaşım sağlar
m3<-splsda(x=as.matrix(train_adlt[,-13]), y=train_adlt[,13]-1, K=1, eta=.5,
           kappa=0.5, classifier=c('logistic'), scale.x=TRUE)#'lda'

Metrics::accuracy(test_adlt[,13],
                  predict(object=m3, newx=test_adlt[,-13], type = "fit",
                  fit.type = "class"))

```
13.3
0.81




# 35. stepPlr

L2, ileri kademeli/ileri kademeli değişken seçim prosedürü ile hem sürekli hem de ayrık tahminciler için lojistik regresyonu cezalandırdı.--İKİNCİSİ UZUN SÜRÜYOR--ama aynı
```{r}
library(stepPlr)
#Bu işlev, katsayıların L2 normunun boyutunu cezalandıran bir lojistik regresyon modeline uyar.
m<-plr(x=as.matrix(train_adlt[,-13]), y=ifelse(train_adlt[,13]==1,0,1),
    weights = rep(1,length(train_adlt[,13])), offset.subset = NULL,
    offset.coefficients = NULL, lambda = 1e-4, cp = "bic")

prd=predict(object=m, newx = as.matrix(test_adlt[,-13]), type = "class")
table(prd,actual=(test_adlt[,13]-1))#"link", "response"

#Bu işlev, ileriye doğru adım adım seçim prosedürü aracılığıyla değişkenleri seçen bir dizi L2 cezalandırılmış lojistik regresyon modeline uyar.
md<-step.plr(x=as.matrix(train_adlt[,-13]), y=ifelse(train_adlt[,13]==1,0,1),
         weights = rep(1,length(train_adlt[,13])), fix.subset = NULL,
         level = NULL, lambda = 1e-4, cp = "bic", max.terms = 5,
         type = c("both", "forward", "forward.stagewise"), trace = FALSE)

prd1<-predict(object=md, x = as.matrix(train_adlt[,-13]),
        newx = as.matrix(test_adlt[,-13]),type = "class")#"link", "response"
table(prd1,actual=(test_adlt[,13]-1))#"link", "response"

Metrics::accuracy(test_adlt[,13]-1,prd)
Metrics::accuracy(test_adlt[,13]-1,prd1)
```
0.81
0.81




# 36. gbm

Freund ve Schapire'nin AdaBoost algoritmasının ve Friedman'ın gradyan
artırma makinesinin uzantılarının bir uygulaması. En küçük kareler,
mutlak kayıp, t-dağılımı kaybı, niceliksel regresyon, lojistik, çok
terimli lojistik, Poisson, Cox orantılı tehlikeler kısmi olasılığı,
AdaBoost üstel kaybı, Huberleştirilmiş menteşe kaybı ve Sıralamayı
Öğrenme ölçümleri için regresyon yöntemlerini içerir

```{r}
library('gbm')
#Genelleştirilmiş güçlendirilmiş regresyon modellerine uyar
ft<-gbm(formula = csMPa~., distribution = "tdist", data = beton, weights=NULL,
    var.monotone = NULL,n.trees = 100,interaction.depth = 1,n.minobsinnode = 10,
    shrinkage = 0.1,bag.fraction = 0.5,train.fraction = 1,cv.folds = 0,
    keep.data = FALSE,verbose = FALSE,class.stratify.cv = NULL,n.cores = NULL)


#Bir GBM nesnesi için en uygun yükseltme yineleme sayısını tahmin eder ve isteğe bağlı olarak çeşitli performans önlemlerini çizer : "OOB","test","cv"
gbm.perf(ft, plot.it = TRUE, oobag.curve = FALSE, overlay = TRUE, method="OOB")

newdata=beton[sample(1:nrow(beton), 50),]
pr<-predict(ft, newdata=newdata[,-9], n.trees=100,
        type = "link", single.tree = FALSE)

Metrics::mae(actual=newdata[,9], predicted=pr)
```

DISTRIBUTION:"gaussian" (squared error), "laplace" (absolute loss),
"tdist" (t-distribution loss), "bernoulli" (logistic regression for 0-1
outcomes), "huberized" (huberized hinge loss for 0-1 outcomes), classes,
"adaboost" (the AdaBoost exponential loss for 0-1 outcomes), "poisson"
(count outcomes), "coxph" (right censored observations), "quantile", or
"pairwise" (ranking measure using the LambdaMart algorithm).




# 37. randomForest

Breiman'a dayalı, rastgele girdiler kullanan bir ağaç ormanına dayalı
sınıflandırma ve regresyon.

```{r}
library('randomForest')
#veri hazırlama----FAZLA RAM İSTİYOR VE ÇOK UZUN SÜRÜYOR impute kısmı---
#a <- adult#clear id column
#a <- replace(a,a=="?",NA)#replace ? to NA
#a <- droplevels(a)#update levels
#ana<-na.roughfix(a)#Impute Missing Values by median/mode.
#ar<-randomForest(income~.,a,na.action=na.roughfix)#impute na model
#a <- rfImpute(income~.,a,iter=3, ntree=100)

mdl<-randomForest(formula=income~., data=adult, subset=NULL, na.action=na.omit)

p<-predict(mdl, adult[1:1000,-13], type="response",norm.votes=TRUE,
           predict.all=FALSE, proximity=FALSE, nodes=FALSE,
           cutoff=mdl$forest$cutoff)
caret::confusionMatrix(adult[1:1000,13], p)
#extract the structure of a tree from a randomForest object
  #getTree(rfobj=mdl, k=1, labelVar=FALSE)
# extractor function for variable importance measures
  #importance(x=mdl, type=NULL, class=NULL, scale=TRUE)
  #hist(treesize(mdl))
  #tuneRF(x=adlt[,-13], y=factor(adlt[,13]),  ntreeTry=50, stepFactor=2,
       #improve=0.05,trace=TRUE, plot=TRUE, doBest=T)
  #varImpPlot(mdl)
```






# 38. arules

İşlem verilerini ve modellerini (sık öğe kümeleri ve birliktelik
kuralları) temsil etmek, değiştirmek ve analiz etmek için altyapı
sağlar. Ayrıca Apriori ve Eclat ilişkilendirme madenciliği
algoritmalarının C uygulamalarını sağlar.

```{r}
library('arules')
##data(Adult,package="arules")

## remove attributes
adult[["fnlwgt"]] <- NULL
adult[["education_num"]] <- NULL

adult <- replace(adult,adult=="?",NA)#replace ? to NA
adult <- droplevels(adult)#update levels
#impute NA cells
adult <- randomForestSRC::impute(formula=NULL, data=adult,
              ntree = 100, nodesize = 3, nsplit = 10,
              nimpute = 2, fast = FALSE, blocks=NULL,
              mf.q=0.5, max.iter = 10, eps = 0.01,
              ytry = NULL, always.use = NULL, verbose = TRUE)

## map metric attributes
adult[["age"]] <- ordered(cut(adult[["age"]], c(15, 25, 45, 65, 100)),
                          labels = c("Young", "Middle-aged", "Senior", "Old"))
adult[["hours_per_week"]] <- ordered(cut(adult[["hours_per_week"]],
                                         c(0,25,40,60,100),
    labels = c("Part-time", "Full-time", "Over-time", "Workaholic")))
adult[["capital_gain"]] <- ordered(cut(adult[["capital_gain"]],
  c(-Inf,0,median(adult[["capital_gain"]][adult[["capital_gain"]] > 0]),
    Inf)), labels = c("None", "Low", "High"))
adult[["capital_loss"]] <- ordered(cut(adult[["capital_loss"]],
  c(-Inf,0, median(adult[["capital_loss"]][adult[["capital_loss"]] > 0]),
    Inf)), labels = c("None", "Low", "High"))
## create transactions
adult <- transactions(adult)
adult


#Apriori algoritmasını kullanarak sık öğe kümelerini, birliktelik kurallarını veya birliktelik hiper kenarlarını kazın.
m<-apriori(data=adult, parameter =  list(supp = 0.5, conf = 0.9, target = "rules"),
           appearance = NULL,#list(items = c("income=<=50K", "income=>50K", "age=Young")
           control = NULL)

confint(m, "oddsRatio", smoothCounts = .5)
crossTable(adult,measure="lift",sort=T)
DATAFRAME(head(adult))
DATAFRAME(m)
eclat(adult,parameter = list(supp = 0.1, maxlen = 5))
#fim4r(Adult, method = "fpgrowth",target = "rules", supp = .7, conf = .8)
z<-inspect(m[100:101], ruleSep = "~~>", itemSep = " + ", setStart = "",
           setEnd = "", linebreak = FALSE)
LIST(adult[1:5])


#Bir dosyadan işlem verilerini okur ve bir işlem nesnesi oluşturur.
#read.transactions(file,format = c("basket", "single"),header = FALSE,sep = "",
#cols = NULL,rm.duplicates = FALSE,quote = "\"'",skip = 0,encoding = "unknown")

#Eclat algoritmasıyla sık görülen öğe kümelerini bulun. Bu uygulama, ağırlıklı birliktelik kuralı madenciliği (WARM) uygulamak için optimize edilmiş işlem kimliği listesi birleştirmelerini ve işlem ağırlıklarını kullanır.
weclat(data=adult, parameter = NULL, control = NULL)
```

# 39. elasticnet

Elastic-Net'in tüm çözüm yolunu uydurmak için işlevler sağlar ve ayrıca
seyrek PCA yapmak için işlevler sağlar. LARS-EN algoritması sıfırdan
başlayarak tüm katsayılar ve uyum dizisini sağlar.

```{r}
library('elasticnet')
mdl<-enet(x=as.matrix(beton[,-9]), y=beton[,9], lambda=0, max.steps=100,
          normalize=TRUE, intercept=TRUE, trace = FALSE,
          eps = .Machine$double.eps)

p<-predict(object=mdl, newx=as.matrix(beton[1:100,-9]), s=1, type = "fit",
           #c("fit", "coefficients"),
        mode = c("step"),naive=FALSE)#,"fraction", "norm", "penalty"
Metrics::mae(beton[1:100,9],p$fit)
```

# 40. glmnet

Doğrusal regresyon, lojistik ve çok terimli regresyon modelleri, Poisson
regresyonu, Cox modeli, çok yanıtlı Gaussian vegruplandırılmış çok
terimli regresyon için tüm kement veya elastik ağ düzenleme yolunu
uydurmak için son derece etkili prosedürler.

```{r}
library('glmnet')
nvars<-8; dfmax = nvars + 1; nobs<-nrow(beton)
X = makeX(beton[,-9],na.impute = T,sparse = F)

#fit a GLM with lasso or elasticnet regularization
mdl<-glmnet(x = X, y=beton[,9],
       family = c("gaussian"),# "binomial", "poisson", "multinomial", "cox", "mgaussian"),
       weights = NULL, offset = NULL, alpha = 1, nlambda = 100,
       lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04), lambda = NULL,
       standardize = TRUE, intercept = TRUE, thresh = 1e-07, dfmax = nvars + 1,
       pmax = min(dfmax * 2 + 20, nvars), exclude = NULL,
       penalty.factor = rep(1, nvars), lower.limits = -Inf, upper.limits = Inf,
       maxit = 1e+05, type.gaussian = ifelse(nvars < 500,"covariance","naive"),
       type.logistic = c("Newton", "modified.Newton"),
       standardize.response = FALSE,
       type.multinomial = c("ungrouped", "grouped"), relax = FALSE,
       trace.it = 0)

p<-predict(mdl,X[1:100,], type = "link")
Metrics::mae(beton[1:100,9],p)
```


# 41. CatBoost

karar ağaçlarında gradyan artırma kullanan bir makine öğrenimi
algoritmasıdır.

```{r}
library(catboost)
pool<-catboost.load_pool(data=train_adlt[,-13],
                   label = train_adlt[,13],
                   cat_features = NULL,
                   column_description = NULL,
                   pairs = NULL,
                   delimiter = "\t",
                   has_header = FALSE,
                   weight = NULL,
                   group_id = NULL,
                   group_weight = NULL,
                   subgroup_id = NULL,
                   pairs_weight = NULL,
                   baseline = NULL,
                   feature_names = NULL,
                   thread_count = -1)

fit<-catboost.train(learn_pool=pool,
               test_pool = NULL,
               params = list(loss_function = 'Logloss',
                             iterations = 100,
                             ignored_features=4#education_num safdışı
                             )
               )
test_pool<-catboost.load_pool(data=test_adlt[,-13],
                              label = test_adlt[,13])
pr<-catboost.predict(model=fit,
                 pool=test_pool,
                 verbose=FALSE,
                 prediction_type="Class",
                 ntree_start=0,
                 ntree_end=0,
                 thread_count=-1)
catboost.get_feature_importance(model=fit,
                                pool = pool,
                                type = 'FeatureImportance',
                                thread_count = -1)

catboost.get_model_params(model=fit)
Metrics::accuracy(test_adlt[,13]-1, pr)
```
0.86

```{r}
library(caret)#fit control and grid için
library(catboost)

titanic_train=read.csv("./ML_GlobalAI/DecisionTrees_titanic.csv", sep = ",")
x<-titanic_train[,-1]
y<-titanic_train[,1]

fit_control <- trainControl(method = "cv",
                            number = 4,
                            classProbs = TRUE)

grid <- expand.grid(depth = c(4, 6, 8),
                    learning_rate = 0.1,
                    iterations = 100,
                    l2_leaf_reg = 1e-3,
                    rsm = 0.95,
                    border_count = 64)

report <- train(x, as.factor(make.names(y)),
                method = catboost.caret,
                logging_level = 'Verbose', preProc = NULL,
                tuneGrid = grid, trControl = fit_control)

print(report)

importance <- varImp(report, scale = FALSE)
print(importance)
```






# 42. C50

Quinlan'ın C5.0 algoritmasını kullanarak sınıflandırma ağacı modellerine
veya kural tabanlı modellere uyun

```{r}
library('C50')
mdl<-C5.0(x=train_adlt[,-13], y=factor(train_adlt[,13]),trials = 1,
          rules = FALSE, weights = NULL, control = C5.0Control(), costs = NULL)

summary(mdl)

p<-predict(object=mdl, newdata = test_adlt[,-13], trials = mdl$trials["Actual"],
        type = "class", na.action = na.pass)
Metrics::accuracy(test_adlt[,13], p)
#C5imp(object=mdl, metric = "usage", pct = TRUE)
```
0.86


# 43. cubist

Bu işlev, eğitim setindeki en yakın komşulara dayalı ek düzeltmelerle
birlikte Quinlan (1992) (diğer adıyla M5)'te açıklanan kural tabanlı
modele uyar.

```{r}
library('Cubist')
mdl<-cubist(x=beton[,-9], y=beton[,9], committees = 1,control = cubistControl(),
       weights = NULL)
p<-predict(object=mdl, newdata = beton[1:50,-9], neighbors = 0)
Metrics::mae(beton[1:50,9],p)
```
2.35


# 44. earth

Friedman'ın "Çok Değişkenli Uyarlanabilir Regresyon Splineları" ve
"Hızlı MARS" makalelerindeki teknikleri kullanarak bir regresyon modeli
oluşturun

```{r}
library('earth')
mdl<- earth(formula = csMPa~., data = beton,  weights = NULL, wp = NULL,
      subset = NULL, #na.action = na.fail,
      pmethod = "backward", #c("none", "exhaustive", "forward", "seqrep", "cv"),
      keepxy = FALSE, trace = 0, glm = NULL, degree = 1, nprune = NULL,nfold=0,
      ncross=1, stratify=TRUE,varmod.method = "none", varmod.exponent = 1,
      varmod.conv = 1, varmod.clamp = .1, varmod.minspan = -3,Scale.y = NULL)

p<-predict(object = mdl, newdata = beton[1:50,-9],
        type = "response") #c("link", "earth", "class", "terms"),
        #interval = "none", level = .95,thresh = .5, trace = FALSE)
Metrics::mae(beton[1:50,9],p)
```
5.96


# 45. evtree

optimum sınıflandırma ve regresyon ağaçlarını öğrenmek için evrimsel bir
algoritma uygular, CART alternatifidir

```{r}
library('evtree')
mdl<-evtree(formula=csMPa~., data = beton, subset=NULL, na.action=na.fail,
       weights=NULL, control = evtree.control(seed = 1))
p<-predict(object = mdl, newdata = beton[1:50,-9])
Metrics::mae(beton[1:50,9],p)
```
3.94



# 46. grplasso

Grup lasso cezasına sahip, kullanıcı tarafından belirlenen (GLM-)
modellere uyar

```{r}
library('grplasso')
train_adlt[,13]<-train_adlt[,13]-1
mdl<-grplasso(formula=income~., data = train_adlt, nonpen = ~ 1,
              weights=NULL, subset=NULL, na.action=na.fail, lambda=20,
              coef.init=1:13, penscale = sqrt, model = LogReg(), center = TRUE,
              standardize = TRUE, control = grpl.control(), contrasts = NULL)

p<-predict(object=mdl, newdata=test_adlt[,-13], type = "response",#"link"
        na.action = na.pass)
Metrics::accuracy(test_adlt[,13], ifelse(p<.5,1,2))
```
0.82


# 47. LiblineaR

Büyük ölçekli düzenli doğrusal sınıflandırma ve regresyonu çözmek için
basit bir kütüphane. Şu anda L2-düzenlenmiş sınıflandırmanın (lojistik
regresyon, L2-kayıp doğrusal SVM ve L1-kayıp doğrusal SVM gibi) yanı
sıra L1-düzenlileştirilmiş sınıflandırmayı (L2-kayıp doğrusal SVM ve
lojistik regresyon gibi) ve L2-düzenlenmiş destek vektörünü
desteklemektedir. regresyon (L1- veya L2 kaybıyla). LiblineaR'ın ana
özellikleri arasında çok sınıflı sınıflandırma (bire karşı geri kalanı
ve Crammer & Singer yöntemi), model seçimi için çapraz doğrulama,
olasılık tahminleri (yalnızca lojistik regresyon) veya dengesiz veriler
için ağırlıklar yer alır.

```{r}
library('LiblineaR')
md<-LiblineaR(data=beton,target=beton[,9],type = 12,cost = 1,epsilon = 0.1,
          svr_eps = 0.1,bias = 1,wi = NULL,cross = 0,verbose = FALSE,
          findC = FALSE,useInitC = TRUE)
mdl<-LiblineaR(data=train_adlt,target=train_adlt[,13],type = 0,cost = 1,
          epsilon = 0.01, svr_eps = NULL,bias = 1, wi = NULL,cross = 0,
          verbose = FALSE,findC = FALSE,useInitC = TRUE)

p1<-predict(object=md, newx=beton[1:50,], proba = FALSE, decisionValues = FALSE)
Metrics::mae(beton[1:50,9],p1$predictions)
p2<-predict(object=mdl, newx=test_adlt, proba = FALSE, decisionValues = FALSE)
Metrics::accuracy(test_adlt[,13], ifelse(p2$predictions<.5,1,2))
```
1.62
0.79


# 48. lightgbm

Yüksek Verimli Gradyan Artırma Karar Ağacı. Herkese açık veri kümeleri
üzerinde yapılan karşılaştırma deneyleri, 'LightGBM'nin, önemli ölçüde
daha düşük bellek tüketimiyle hem verimlilik hem de doğruluk açısından
mevcut güçlendirme çerçevelerinden daha iyi performans gösterebileceğini
gösteriyor.

```{r}
library('lightgbm')

data<-lgb.convert_with_rules(data=adult, rules = NULL)

fit<-lightgbm(data=as.matrix(data$data[,-13]),label=data$data[,13],
              weights = NULL,params = list(object="binomial"),nrounds = 100L,
              verbose = 1L,eval_freq = 1L,early_stopping_rounds = NULL,
              init_model = NULL,callbacks = list(),serializable = TRUE,
              objective = "auto",init_score = NULL,num_threads = NULL)

p<-predict(fit, as.matrix(adult[1:1000,-13]), type = "class")
Metrics::accuracy(as.numeric(adult[1:1000,13]), ifelse(p<1.240817,1,2))

#-----------------
mdl<-lightgbm(data=as.matrix(train_adlt[,-13]),label = factor(train_adlt[,13]),
              weights = NULL, params = list(), nrounds = 100L, verbose = 1L,
              eval_freq = 1L, early_stopping_rounds = NULL, init_model = NULL,
              callbacks = list(), serializable = TRUE, objective = "auto",
              init_score = NULL, num_threads = 2)

pr<-predict(object=mdl, newdata=as.matrix(test_adlt[,-13]), type = "class",
    start_iteration = NULL,num_iteration = NULL,header = FALSE,params = list())
Metrics::accuracy(test_adlt[,13], pr)
```
0.74
0.87




# 49. penalizedLDA

Bu paket, özellik sayısının (p) gözlem sayısını (n) aştığı yüksek
boyutlu ortam için tasarlanmış, cezalandırılmış doğrusal diskriminant
analizi gerçekleştirir.

```{r}
library('penalizedLDA')

mdl<-PenalizedLDA(x=as.matrix(adlt[50:60,-13]), y=adlt[50:60,13], xte=NULL,
             type = "standard", lambda=.1, K = 1, chrom =NULL, lambda2 = NULL,
             standardized = FALSE, wcsd.x = NULL, ymat = NULL, maxiter = 20,
             trace=FALSE)
#not:satır sayısı sütun sayısından az olacak ve her sütunda en az bir rakam olacak aralık seçildi

predict(object=mdl, xte=as.matrix(adlt[93:101,-13]))->p
table(adlt[93:101,13],p$ypred)
```

# 50. picasso

Bu paket, genelleştirilmiş doğrusal modeli dışbükey ve dışbükey olmayan
cezaya uydurmak için hesaplama açısından verimli araçlar sağlar.
Kullanıcılar, l1 ve ridge gibi dışbükey cezalara kıyasla önemli ölçüde
daha az tahmin hatası ve fazla uyum sağlayan SCAD ve MCP gibi dışbükey
olmayan cezaların üstün istatistiksel özelliğinden yararlanabilirler.
Hesaplama, çok aşamalı dışbükey gevşeme ve sıcak başlangıç başlatma,
aktif küme güncelleme ve hesaplamayı artırmak için koordinat ön seçimi
için güçlü kuraldan yararlanan ve benzersiz bir seyrek yerel optimuma
doğrusal bir yakınsama elde eden Yol Yönünde Kalibre Edilmiş Seyrek
Çekim algoritması (PICASSO) tarafından gerçekleştirilir. Optimum
istatistiksel özelliklere sahip. Hesaplama, seyrek matris çıktısı
kullanılarak bellek açısından optimize edilmiştir.

```{r}
library('picasso')
mdl<-picasso(X=as.matrix(train_adlt[,-13]), Y=train_adlt[,13], lambda = NULL,
             nlambda = 100, lambda.min.ratio =0.05,
             family = "binomial",#gaussian,sqrtlasso,poisson
             method = "l1", # "mcp", "scad"
             type.gaussian = "naive", #"covariance".
             gamma = 3, df = NULL, standardize = TRUE, intercept = TRUE,
             prec = 1e-07, max.ite = 1000, verbose = FALSE)

predict(object=mdl, newdata=as.matrix(test_adlt[1:100,-13]),
        lambda.idx = c(1:4), p.pred.idx = c(1:5))
```


# 51. quantregForest

Nicelik Regresyon Ormanları, koşullu niceliklerin tahminine yönelik ağaç
tabanlı bir topluluk yöntemidir. Özellikle yüksek boyutlu veriler için
çok uygundur. Karma sınıfların tahmin değişkenleri ele alınabilir.
Paket, 'randomForest' paketine bağlıdır.

```{r}
library('quantregForest')

m<-quantregForest(x=train_adlt[,-13],y=factor(train_adlt[,13]), nthreads=4,
                  keep.inbag=FALSE)
Metrics::accuracy(test_adlt[,13], predict(m, test_adlt[,-13],what=c(0.5, 0.5)))
```
0.86


# 52. ranger

Özellikle yüksek boyutlu veriler için uygun olan Rastgele Ormanların
hızlı bir uygulaması. Sınıflandırma, regresyon, hayatta kalma ve
olasılık tahmin ağaçlarından oluşan topluluklar desteklenmektedir. Genom
çapında ilişkilendirme çalışmalarından elde edilen veriler verimli bir
şekilde analiz edilebilir.

```{r}
library('ranger')
mdl<-ranger(formula = income~., data = train_adlt,classification = TRUE)
      #, num.trees = 500, mtry = NULL,
       #importance = "none", write.forest = TRUE, probability = FALSE,
       #min.node.size = 1, min.bucket = 1, max.depth = NULL,
       #replace = TRUE, sample.fraction = ifelse(replace, 1, 0.632),#
       #case.weights = NULL, class.weights = NULL, splitrule = "gini",
       #num.random.splits = 1, alpha = 0.5, minprop = 0.1,
       #split.select.weights = NULL, always.split.variables = NULL,
       #respect.unordered.factors = NULL, scale.permutation.importance = FALSE,
       #local.importance = FALSE, regularization.factor = 1,
       #regularization.usedepth = FALSE, keep.inbag = FALSE, inbag = NULL,
       #holdout = FALSE, quantreg = FALSE, time.interest = NULL,##
       #oob.error = TRUE, num.threads = NULL, save.memory = FALSE,
       #verbose = TRUE, node.stats = TRUE, seed = NULL,
       #dependent.variable.name = NULL, status.variable.name = NULL,
       #classification = TRUE)#, x = NULL, y = NULL)

Metrics::accuracy(test_adlt[,13],
                  (predict(object=m, data = test_adlt[,-13], predict.all =FALSE,
        num.trees = m$num.trees, type = "response", se.method = "infjack",
        quantiles = c(0.1, 0.5, 0.9), what = NULL, seed = NULL,
        num.threads = NULL, verbose = TRUE))$predictions)
```
0.86


# 53. RLT

Regresyon, sınıflandırma ve hayatta kalma analizi için çeşitli ek
özelliklere sahip rastgele orman.

```{r}
library('RLT')
x=train_adlt[,-13];reinforcement = FALSE;nmin = max(1, as.integer(log(nrow(x))))
#Reinforcement Learning Trees-Takviyeli Öğrenme Ağaçları
mdl<-RLT(x=x, y=factor(train_adlt[,13]), censor = NULL,
    model = "classification",#regression, classification or survival
    print.summary = 1, use.cores = 4,
    ntrees = if (reinforcement) 100 else 500,
    mtry = max(1, as.integer(ncol(x)/3)),
    nmin = max(1, as.integer(log(nrow(x)))), alpha = 0.4, split.gen = "random",
    nsplit = 1, resample.prob = 0.9, replacement = TRUE, npermute = 1,
    select.method = "var", subject.weight = NULL, variable.weight = NULL,
    track.obs = FALSE, importance = TRUE, reinforcement = FALSE, muting = -1,
    muting.percent = if (reinforcement) MuteRate(nrow(x), ncol(x),
                                                 speed = "aggressive",
                                                 info = FALSE) else 0,
    protect = as.integer(log(ncol(x))),combsplit = 1,combsplit.th = 0.25,
    random.select = 0,embed.n.th = 4 * nmin,
    embed.ntrees = max(1, -atan(0.01 * (ncol(x) - 500))/pi * 100 + 50),
    embed.resample.prob = 0.8, embed.mtry = 1/2,
    embed.nmin = as.integer(nrow(x)^(1/3)), embed.split.gen = "random",
    embed.nsplit = 1)

Metrics::accuracy(test_adlt[,13]-1,
                  predict(object=mdl, testx=test_adlt[,-13])$Prediction)

```
0.85



# 54. tree

Sınıflandırma ve Regresyon Ağaçları

```{r}
library('tree')
frm <- as.formula(paste("income ~ ",
                        paste(names(train_adlt[,-13]), collapse= "+")))
#ctrl<-tree.control(nobs=NULL, mincut = 5, minsize = 10, mindev = 0.01)

mdl<-tree(formula=frm, data=train_adlt)
     #, weights=NULL, subset=NULL,na.action = na.pass,
     #control = ctrl,
     #method = "recursive.partition",#"model.frame"
     #split = c("deviance"), model = FALSE,#, "gini"
     #x = FALSE, y = TRUE, wts = FALSE)

p<-predict(object=mdl, newdata = test_adlt[,-13], type = c("vector"),
        #"tree", "class", "where"),
        split = FALSE, nwts, eps = 1e-3)
Metrics::accuracy(test_adlt[,13], ifelse(p<1.5,1,2))
```

0.84


# 55. xgboost

Paket, verimli doğrusal model çözücü ve ağaç öğrenme algoritmalarını
içerir. Regresyon, sınıflandırma ve sıralama dahil olmak üzere çeşitli
amaç işlevlerini destekler.

```{r}
library('xgboost')
## VERİ TİPLERİ
#1. dense matrix (for xgboost model)
dtrain<-as.matrix(adlt[,-13])

#2. xgb.DMatrix (for xgb.train model)
dtrain1<-xgb.DMatrix(data=as.matrix(adlt[,-13]),label=adlt[,13]-1,
                     info = NULL, missing = NA,silent = FALSE,nthread = 2)
#3. dgCMatrix (for xgboost model)
dtrain2<-Matrix::sparse.model.matrix(income ~ ., data = adlt)[, -1]
#dtrain2<-as(as.matrix(adlt[,-13]), "sparseMatrix") alternate method

## PARAMETERS
global_param<-list(verbosity      =0,
                   use_rmm        =FALSE,
                   objective      ="binary:logistic",
                   base_score     =0.5,
                   eval_metric    =list("error","auc"))#("rmse","logloss")

tree_param<-list(booster                ="gbtree", #"gblinear"
                 eta                    =1, #remove for regression
                 gamma                  =0,
                 max_depth              =6,
                 min_chil_weight        =1,
                 subsample              =1,
                 colsample_bytree       =1,
                 lambda                 =1,
                 alpha                  =0,
                 num_parallel_tree      =1,
                 monotone_constraints   =0,
                 interaction_constraints="[[0,1],[2,3]]")

#linear_param<-list(booster="gblinear",lambda=0,alpha=0) LINEAR REGRESSION

## MODELS
#A :xgboost işlevi xgb.train için daha basit bir sarmalayıcıdır
basic<-xgboost(data = dtrain2,#dtrain,#matrix, dgCMatrix
        label = adlt[,13]-1,#adlt[,13]-1, #yanıt değişkeni 0-1 olmalı
        nrounds = 2,
        missing = FALSE,
        weight = NULL,
        params = append(global_param,tree_param),
        verbose = 1,
        print_every_n = 1L,
        early_stopping_rounds = NULL,
        maximize = NULL,
        save_period = NULL,
        save_name = "xgboost.model",
        xgb_model = NULL,
        callbacks = NULL)

#B: xgb.train, bir xgboost modelinin eğitimi için gelişmiş bir arayüzdür
dtest<-xgb.DMatrix(data=as.matrix(adlt[1:1000,-13]),label=adlt[1:1000,13]-1)
watchlist <- list(train=dtrain, test=dtest) #bu ikisi opsiyonel

advance<-xgb.train(
  params = append(global_param, tree_param),
  data = dtrain1, # only an xgb.DMatrix
  nrounds = 2,
  watchlist = watchlist, #NULL
  obj = NULL,
  feval = NULL,
  verbose = 1,
  print_every_n = 1L,
  early_stopping_rounds = NULL,
  maximize = NULL,
  save_period = NULL,
  save_name = "xgboost.model",
  xgb_model = NULL,
  callbacks = NULL#list()
)

p1<-predict(object=basic, newdata=as.matrix(test_adlt[,-13]),
  missing = NA, outputmargin = FALSE, ntreelimit = NULL, predleaf = FALSE,
  predcontrib = FALSE, approxcontrib = FALSE, predinteraction = FALSE,
  reshape = FALSE, training = FALSE, iterationrange = NULL,strict_shape = FALSE)

p2<-predict(object=advance, newdata=as.matrix(test_adlt[,-13]),
  missing = NA, outputmargin = FALSE, ntreelimit = NULL, predleaf = FALSE,
  predcontrib = FALSE, approxcontrib = FALSE, predinteraction = FALSE,
  reshape = FALSE, training = FALSE, iterationrange = NULL,strict_shape = FALSE)

mean(as.numeric(p1 > 0.5) != test_adlt[,13]-1)
mean(as.numeric(p2 > 0.5) != test_adlt[,13]-1)

Metrics::accuracy(test_adlt[,13], ifelse(p1<0.5,1,2))
Metrics::accuracy(test_adlt[,13], ifelse(p2<0.5,1,2))

##getinfo(dtest, "label")
##xgb.importance(model = basic)
##xgb.plot.importance(xgb.importance(model = basic))
##xgb.plot.tree(model = basic)
##xgb.plot.shap.summary(data=as.matrix(test_adlt[,-13]), model = basic)
##xgb.plot.shap(data=as.matrix(test_adlt[,-13]),  model = basic)
```

OBJECTIVE:reg:squarederror,reg:squaredlogerror,reg:logistic,reg:pseudohubererror
binary:logistic,binary:logitraw,binary:hinge,count:poisson,survival:cox
survival:aft,aft_loss_distribution,multi:softmax,multi:softprob,rank:pairwise
rank:ndcg,rank:map,reg:gamma,reg:tweedie
0.81




# 56. ordinalNet

Elastik net cezalı sıralı regresyon modellerine uyar.

```{r}
library(ordinalNet)
mdl<-ordinalNet(x=as.matrix(train_adlt[,-13]),y=factor(train_adlt[,13]),
      alpha = 1, standardize = TRUE, penaltyFactors = NULL, positiveID = NULL,
           family = "cumulative", #"sratio", "cratio", "acat"
        reverse = FALSE,link = "logit", #"probit", "cloglog", "cauchit"
        customLink = NULL,parallelTerms = TRUE,nonparallelTerms = FALSE,
        parallelPenaltyFactor = 1,lambdaVals = NULL,nLambda = 20,
        lambdaMinRatio = 0.01,includeLambda0 = FALSE,alphaMin = 0.01,
        pMin = 1e-08,stopThresh = 1e-08,threshOut = 1e-08,threshIn = 1e-08,
        maxiterOut = 100,maxiterIn = 100,printIter = FALSE,printBeta = FALSE,
        warn = TRUE,keepTrainingData = TRUE)

ordinalNetCV(x=as.matrix(train_adlt[,-13]),y=factor(train_adlt[,13]))
ordinalNetTune(x=as.matrix(train_adlt[,-13]),y=factor(train_adlt[,13]))
p<-predict(object=mdl, newx = as.matrix(test_adlt[,-13]),whichLambda = NULL,
        criteria = c("aic", "bic"),type = "class")#"response","link"
Metrics::accuracy(test_adlt[,13], p)
```
0.82



# 57. ada

Belirli bir veri kümesinde hem üstel hem de lojistik kayıp altında ayrı,
gerçek ve hafif bir artış gerçekleştirir. Ada paketi, küçük ve orta
büyüklükteki veri kümeleri için ideal olarak uygun, basit, iyi
belgelenmiş ve geniş bir sınıflandırma rutini sağlar.

```{r}
library(ada)
mdl<-ada(x=train_adlt[,-13], y=train_adlt$income,test.x=NULL,test.y=NULL,
    loss="logistic",type="discrete",#"exponential" / ,"real","gentle"
    iter=50, nu=0.1, bag.frac=0.5,model.coef=TRUE,bag.shift=FALSE,max.iter=20,
    delta=10^(-10),verbose=FALSE,na.action=na.rpart)

Metrics::accuracy(test_adlt[,13],
predict(object=mdl, newdata=test_adlt[,-13], type = "both",n.iter=NULL)$class)
#"vector", "probs","F"
```
0.86



# 58. binda

"Binda" paketi, karşılık gelen değişkenler için ikili tahminleyicileri
kullanarak çok sınıflı diskriminant analizine yönelik işlevleri uygular.

```{r}
library(binda)

life1<-apply(life[,-1],2,function(x)scales::rescale(x)) #scale to 0-1
L<-factor(c("asia", "europe", "africa", "africa", "africa", "america", "asia", "oceania", "europe", "asia", "america", "asia", "asia", "america", "europe", "europe", "america", "africa", "asia", "america", "europe", "africa", "america", "asia", "europe", "africa", "africa", "asia", "africa", "america", "africa", "africa", "africa", "america", "asia", "asia", "africa", "africa", "africa", "america", "america", "europe", "asia", "europe", "europe", "america", "america", "africa", "america", "america", "africa", "europe", "oceania", "europe", "europe", "africa", "africa", "asia", "europe", "africa", "europe", "america", "america", "america", "africa", "africa", "america", "america", "europe", "europe", "asia", "asia", "asia", "asia", "europe", "asia", "europe", "america", "asia", "asia", "asia", "africa", "oceania", "asia", "asia", "asia", "europe", "asia", "africa", "africa", "africa", "europe", "europe", "europe", "africa", "africa", "asia", "asia", "africa", "europe", "africa", "africa", "oceania", "europe", "asia", "europe", "africa", "africa", "asia", "africa", "asia", "europe", "oceania", "africa", "africa", "europe", "asia", "asia", "america", "america", "america", "asia", "europe", "europe", "asia", "europe", "europe", "africa", "oceania", "asia", "africa", "europe", "africa", "africa", "asia", "asia", "europe", "oceania", "africa", "asia", "europe", "europe", "america", "america", "europe", "europe", "asia", "africa", "asia", "asia", "africa", "oceania", "africa", "asia", "asia", "africa", "europe", "europe", "europe", "europe", "america", "asia", "oceania", "america", "asia", "asia", "africa"))
thr = optimizeThreshold(X=life1, L)# find optimal thresholds (one for each variable)
Xb = dichotomize(X=life1, thr) #convert into binary matrix, if value is lower than threshold -> 0 otherwise -> 1
train<-Xb[1:140,];test<-Xb[141:167,]

##chances(Xb, L)#Bernoulli parametrelerini tahmin eder.
##binda.ranking(Xb, L)#İkili Diskriminant Analizi: Değişken Sıralaması

mdl<-binda(Xtrain=train, L[1:140], lambda.freqs=0, verbose=TRUE)

table(predict(object=mdl, Xtest=test, verbose=TRUE)$class, L[141:167])
mltest::ml_test(predict(object=mdl, Xtest=test, verbose=TRUE)$class, L[141:167])
```


# 59. gam

Bu paket, genelleştirilmiş eklemeli modellerin takılması ve bu
modellerle çalışmaya yönelik işlevler sağlar. desteklenen yöntemler yerel
regresyon ve yumuşatma eğrileridir.

```{r}
library(gam)
mdl<-gam(formula=csMPa~.,family = gaussian, data = beton)
    #weights=NULL,subset=NULL,na.action=na.omit,start = NULL,etastart,mustart,
    #control = gam.control(epsilon = 1e-07,bf.epsilon = 1e-07,maxit = 30,
    #                  bf.maxit = 30,trace = FALSE),
    #model = TRUE,method = "glm.fit",x = FALSE,y = TRUE)

p<-predict(object=mdl,newdata=beton[1:100,-9],type = "response", #"link","terms"
        dispersion = NULL,se.fit = FALSE,na.action = na.pass,
        terms = labels(mdl))
Metrics::mae(beton[1:100,9],p)
```
12.84


# 60. hda
Heteroscedastic Discriminant Analysis
Eşit olmayan kovaryans matrislerine sahip sınıfların ayrımı için doğrusal bir dönüşüm yükleme matrisi hesaplar. e1071 paktini kullanıyor

```{r}
library(hda)
mdl<-hda(x = train_adlt[,-13], grouping = factor(train_adlt[,13]),
    newdim = 1:(ncol(train_adlt[,-13])-1), crule = T,reg.lamb = NULL,
    reg.gamm = NULL, initial.loadings = NULL, sig.levs = c(0.05,0.05),
    noutit = 7, ninit = 10, verbose = TRUE)

p<-predict(object=mdl, newdata=test_adlt[,-13], alldims=FALSE, task = "c")#"dr"
Metrics::accuracy(test_adlt[,13],p$prediction)
```





# 61. kohonen

Kendi kendini düzenleyen haritaları (SOM'lar) eğitme işlevleri. Ayrıca
haritaların sorgulanması ve tahmin eğitilmiş haritaların kullanılması
desteklenmektedir. Kohonen paketi kendi kendini düzenleyen haritaların
(SOM'lar) çeşitli biçimlerini uygular.

```{r}
library(kohonen)
#prepare data
Xtraining <- scale(train_adlt[,-13])
Xtest <- scale(test_adlt[,-13],
               center = attr(Xtraining, "scaled:center"),
               scale = attr(Xtraining, "scaled:scale"))
trainingdata <- list(gozlemler = Xtraining, yanit = factor(train_adlt$income))
testdata <- list(gozlemler = Xtest, yanit1 = factor(test_adlt$income))

#Bir süpersom, kendi kendini düzenleyen haritaların (SOM'ler), muhtemelen farklı
#sayılara ve farklı değişken türlerine (eşit sayıda nesneye rağmen) sahip birden
#fazla veri katmanına genişletilmesidir. NA'lara izin verilir. Eğitim sırasında
#kazanan birimleri belirlemek için tüm katmanlar üzerindeki ağırlıklı mesafe
#hesaplanır.
mdl<-supersom(data=trainingdata, grid=somgrid(5, 5, "hexagonal"), rlen = 100,
         alpha = c(0.05, 0.01),radius = quantile(0, 2/3),
         whatmap = NULL, user.weights = 1, maxNA.fraction = 0L,
         keep.data = TRUE, dist.fcts = NULL,mode = "online",#"batch", "pbatch"
         cores = -1, normalizeDataLayers = TRUE)#remove init

#Som ve xyf işlevleri, sırasıyla bir ve iki katmanlı süpersomlar için basit #sarmalayıcılardır.

prd<-predict(object=mdl,newdata = testdata, unit.predictions = NULL,
             trainingdata = NULL, whatmap = "gozlemler", threshold = 0,
             maxNA.fraction = "gozlemler")

Metrics::accuracy(factor(test_adlt$income), prd$predictions[["yanit"]])
```
0.80




# 62. partDSA

Tüm ortak değişken alanı üzerinde yoğun ve kapsamlı bir araştırmaya
dayalı, giderek daha karmaşık hale gelen tahmin edicilerin parça parça
sabit bir tahmin listesini oluşturmak için yeni bir araç.

```{r}
library(partDSA)
mdl<-partDSA(x=train_adlt[,-13], y=train_adlt[,13], wt=rep(1, 22792),
        x.test=train_adlt[,-13], y.test=train_adlt[,13],
        wt.test=rep(1, 22792), sleigh=1,
        control=DSA.control(vfold=10, minsplit = 20, minbuck=round(20/3),
        cut.off.growth=10, MPD=0.1, missing="impute.at.split",
        loss.function= "default", wt.method="KM", brier.vec=NULL,
        leafy=0, leafy.random.num.variables.per.split=4,
        leafy.num.trees=50, leafy.subsample=0, save.input=FALSE,
        boost=0, boost.rounds=100, cox.vec=NULL,IBS.wt=NULL, partial=NULL))

p<-predict(object=mdl, newdata1=test_adlt[,-13])
p1<-apply(p,1,mean)
table(prd=ifelse(p1<1.5,1,2),test_adlt$income)
Metrics::accuracy(test_adlt$income,ifelse(p1<1.5,1,2))
```
0.83




# 63. randomGLM

Genelleştirilmiş doğrusal modellere (GLM'ler) dayalı bir torbalama
tahmincisi uygulandı

```{r}
library(randomGLM)
x=train_adlt[,-13]; y=train_adlt[,13];xtest = NULL;type = "binary"
replace = TRUE;nBags = 100
mdl<-randomGLM(
# Input data
  x=x, y=y, xtest = NULL,
  weights = NULL,
# Which columns in x are categorical?
  categoricalColumns = NULL,
  maxCategoricalLevels = 2,
# Include interactions?
  maxInteractionOrder = 1,
  includeSelfinteractions = TRUE,
# Prediction type: type can be used to set
# the prediction type in a simplified way...
  type = type,#c("auto", "linear", "binary", "count", "general", "survival"),
# classify is retained mostly for backwards compatibility
  classify = switch(type,
    auto = !is.Surv(y) & (is.factor(y) | length(unique(y)) < 4),
  linear = FALSE,
  binary = TRUE ,
  count = FALSE,
  general = FALSE,
  survival = FALSE),
# family can be used to fine-tune the underlying regression model
  family = switch(type,
  auto = NULL,
  linear = gaussian(link="identity"),
  binary = binomial(link=logit),
  count = poisson(link = "log"),
  general = NULL,
  survival = NULL),
# Multi-level classification options - only apply to classification
# with multi-level response
  multiClass.global = TRUE,
  multiClass.pairwise = FALSE,
  multiClass.minObs = 1,
  multiClass.ignoreLevels = NULL,
# Sampling options
  nBags = 100,
  replace = TRUE,
  sampleBaggingWeights = NULL,
  nObsInBag = if (replace) nrow(x) else as.integer(0.632 * nrow(x)),
  nFeaturesInBag = ceiling(ifelse(ncol(x)<=10, ncol(x),
    ifelse(ncol(x)<=300, (1.0276-0.00276*ncol(x))*ncol(x), ncol(x)/5))),
  minInBagObs = min( max( nrow(x)/2, 5), 2*nrow(x)/3),
  maxBagAttempts = 100*nBags,
  replaceBadBagFeatures = TRUE,
# Individual ensemble member predictor options
  nCandidateCovariates=12,#50,
  corFncForCandidateCovariates= cor,
  corOptionsForCandidateCovariates = list(method = "pearson", use="p"),
  mandatoryCovariates = NULL,
  interactionsMandatory = FALSE,
  keepModels = is.null(xtest),
# Miscellaneous options
  thresholdClassProb = 0.5,
  interactionSeparatorForCoefNames = ".times.",
  randomSeed = 12345,
  nThreads = NULL,
  verbose =0 )



p<-predict(object=mdl, newdata=test_adlt[,-13], type="class",  #"response"
        thresholdClassProb = mdl$details$thresholdClassProb)
table(p,test_adlt[,13])
Metrics::accuracy(test_adlt[,13],p)
```
0.82




# 64. rpart

Sınıflandırma, regresyon ve hayatta kalma ağaçları için yinelemeli
bölümleme

```{r}
library('rpart')

control=rpart.control(minsplit = 20, minbucket = round(20/3), cp = 0.01,
maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,
surrogatestyle = 0, maxdepth = 30)

mdl<-rpart(formula=income~., data=adult, weights=NULL, subset=NULL,
      na.action = na.rpart, method='class',#"anova", "poisson", "class" or "exp"
      model = FALSE, x = FALSE, y = TRUE, parms=NULL, control=control)# cost

newdata=adult[sample(nrow(adult),200,replace = F),]
Metrics::accuracy(newdata$income,
                  predict(object=mdl, newdata=newdata, type = "class",
                          na.action = na.pass))#c("vector", "prob", "class", "matrix"),
        
```

method : If y is a survival object, then method ="exp" is assumed, if y
has 2 columns then method = "poisson" is assumed, if y is a factor then
method = "class" is assumed, otherwise method = "anova" is assumed.

İlave kardeş paket
```{r}
#CART çerçevesinde sıralı yanıtlar için sınıflandırma ağaçları oluşturmaya yönelik yinelemeli bölümleme yöntemleri
rpartScore::rpartScore(formula= status ~., data=lung, weights=NULL, subset=NULL,
           na.action = na.omit, split = "abs", prune = "mc", model = FALSE,
           x = FALSE, y = TRUE, control=control)
```





# 65. rFerns

Genel ve çok etiketli sınıflandırma için değiştirilmiş ve Kursa'da
tanıtıldığı gibi OOB hata yaklaşımı ve önem ölçüsünü içeren rastgele
eğrelti otları sınıflandırıcısını sağlar

```{r}
library(rFerns)
# Rastgele eğrelti otlarıyla sınıflandırma
m<-rFerns(x=train_adlt[,-13], y=factor(train_adlt[,13]), depth = 5,ferns = 1000,
       importance = "none", saveForest = TRUE, consistentSeed = NULL,
       threads = 0)

p<-predict(object=m, x=test_adlt[,-13], scores = FALSE)
table(p,test_adlt[,13])
```


# 66. rotationForest

İkili sınıflandırma için rotasyon ormanı modellerini yerleştirin ve
dağıtın. Döndürme ormanı, her temel sınıflandırıcının (ağaç), özellik
kümesinin rastgele bölümlerinin değişkenlerinin temel bileşenlerine
uyduğu bir topluluk yöntemidir.

```{r}
library(rotationForest)
mdl<-rotationForest(x=train_adlt[,-13], y=(train_adlt[,13]-1),
               K = round(ncol(train_adlt)/3, 0), L = 10, verbose = FALSE)

p<-predict(object=mdl, newdata=test_adlt[,-13], all = FALSE)
Metrics::accuracy(test_adlt[,13], ifelse(p<0.5,1,2))
```
0.84



# 67. sda
Değişken seçimiyle yüksek boyutlu doğrusal ve çapraz diskriminant analizi için etkili bir çerçeve sağlar. Sınıflandırıcı, James-Stein tipi büzülme tahmin edicileri kullanılarak eğitilir ve öngörücü değişkenler, korelasyona göre ayarlanmış t-puanları (CAT puanları) kullanılarak sıralanır. Değişken seçim hatası, yanlış keşfedilmeme oranları veya daha yüksek eleştiri kullanılarak kontrol edilir.
```{r}
library(sda)
m<-sda(Xtrain=as.matrix(train_adlt[,-13]), L=train_adlt[,13])
    #lambda,lambda.var, lambda.freqs, diagonal=FALSE, verbose=TRUE)

p<-predict(object=m, Xtest=as.matrix(test_adlt[,-13]), verbose=TRUE)
Metrics::accuracy(test_adlt[,13], p$class)
```
0.81



# 68. sparseLDA
Gaussian'lar ve Gaussian modellerinin karışımı için seyrek doğrusal diskriminant analizi gerçekleştirir.
```{r}
library(sparseLDA)
#Seyrek doğrusal diskriminant analizi gerçekleştirir. SDA kriterini en aza indirmek için alternatif bir minimizasyon algoritması kullanma.
p=ncol(train_adlt)-1;K=2#class number
X=normalize(train_adlt[,-13])
Y=fastDummies::dummy_cols(train_adlt$income,remove_selected_columns = T)
Xt=normalizetest(test_adlt[,-13],X)
m<-sda(x=X$Xc, y=as.matrix(Y), lambda = 1e-6, stop = -p,
    maxIte = 100, Q = K-1, trace = FALSE, tol = 1e-6)

#Gauss modellerinin karışımı için seyrek doğrusal diskriminant analizi gerçekleştirir.
#R=2
#smda(x=X$Xc, y=as.matrix(Y), Z = NULL, Rj = 2,
#     lambda = 1e-6, stop, maxIte = 50, Q=R-1, trace = FALSE, tol = 1e-4)

table(unclass(predict(object=m, newdata = Xt)$class), test_adlt[,13])
Metrics::accuracy(test_adlt[,13], unclass(predict(object=m,newdata = Xt)$class))
```
0.81


# 69. spikeslab
Doğrusal regresyon modellerinde tahmin ve değişken seçimi için sivri uç ve dilim. Değişken seçimi için genelleştirilmiş bir elastik ağ kullanır.
```{r}
library(spikeslab)
m<-spikeslab(formula=csMPa~., data = beton, x = NULL, y = NULL, n.iter1 = 500,
             n.iter2 = 500, mse = TRUE, bigp.smalln = FALSE,
             bigp.smalln.factor = 1, screen = FALSE, r.effects = NULL,
             max.var = 500, center = TRUE, intercept = TRUE, fast = TRUE,
             beta.blocks = 5, verbose = FALSE, ntree = 300, seed = NULL)

p<-predict(object=m, newdata = beton[1:25,-9])
Metrics::mae(actual=beton[1:25,9], predicted=p$yhat.gnet)#regression
```
13.1








## BİRAZ SIKINTILI PAKETLER ------------------------


#  klaR (70)

Sınıflandırma ve görselleştirmeye yönelik çeşitli işlevler, örn. düzenli
diskriminant analizi, sknn() çekirdek yoğunluğu saf Bayes, denetimli
sınıflandırma için 'svmlight' ve stepclass() sarmalayıcı değişken seçimi
için bir arayüz, sınıflandırma kurallarının partimat()
görselleştirilmesi ve küme sonuçlarının shardsplot() yanı sıra kmodes()
için bir arayüz kategorik veriler için kümeleme, corclust() değişken
kümeleme, farklı değişken kümeleme modellerinden değişken çıkarma ve
kanıt ağırlığı ön işleme.

SAĞLIKLI ÇALIŞMASI İÇİN svmlight PAKETİ GEREKLİ, ANCAK R'IN BU
VERSİYONUNA KURULAMIYOR. ADAMLAR svm HESAPLAMASI İÇİN e1071'i
ÖNERİYORLAR.

```{r, warning=FALSE}
library('klaR')
#Kategorik veriler üzerinde k-mod kümelemesi gerçekleştirin.Clustering
cl1<-kmodes(data=life[,-1], modes=35, iter.max = 10, weighted = FALSE,
            fast = TRUE)

#Doğrusal Diskriminant Analizinin yerelleştirilmiş bir versiyonu.
loc1<-loclda(x=train_adlt[,-13], grouping = factor(train_adlt[,13]), subset,
             na.action=na.omit)


#Alt sınıflarda ikili değişken seçimi gerçekleştirir.
subclass_class <- matrix(c("setosa","versicolor","virginica",
                           "mavi","mavi","yesil"),ncol=2, byrow = T)

cls5<-locpvs(x=iris[,-5], subclasses=iris$Species,
             subclass.labels=subclass_class, prior=NULL, method="lda",
             vs.method = c("stepclass"),#"ks.test" , "greedy.wilks"),
             niveau=0.05, fold=10, impr=0.1, direct="both", out=FALSE)

#Doğrudan sınıflandırma hatasını en aza indiren doğrusal boyut azaltma için bilgisayar yoğun yöntem.
cls6<-meclight(Species ~ ., data = iris, subset=NULL, na.action = na.omit)

#Bayes kuralını kullanarak bağımsız tahmin değişkenleri verildiğinde kategorik bir sınıf değişkeninin koşullu arka olasılıklarını hesaplar.
cl2<-NaiveBayes(x=train_adlt[,-13], grouping = factor(train_adlt[,13]),
                prior=NULL, usekernel = FALSE, fL = 0)

#En yakın ortalama sınıflandırma işlevi.
cls1<-nm(x=train_adlt[,-13], grouping = factor(train_adlt[,13]), gamma = 0)

#pvs Pairwise variable selection for classification(sınıf 2den fazla olmalı)
cls4<-pvs(x=iris[,-5], grouping = factor(iris[,5]), prior=NULL,
    method="lda",vs.method=c("ks.test"),#"stepclass","greedy.wilks"),
    niveau=0.05,fold=NULL, impr=NULL, direct=NULL, out=FALSE)

#rda Regularized Discriminant Analysis (RDA)
cls2<-rda(x=train_adlt[,-13], grouping = factor(train_adlt[,13]), prior = NULL,
          gamma = 0,lambda = 1, regularization = c(gamma = 0, lambda = 1),
          crossval = TRUE, fold = 10, train.fraction = 0.5,
          estimate.error = TRUE, output = FALSE, startsimplex = NULL,
          max.iter = 100, trafo = TRUE, simAnn = FALSE, schedule = 2,
          T.start = 0.1, halflife = 50, zero.temp = 0.01, alpha = 2,K = 100)

#sknn Simple k nearest Neighbours
cls3<-sknn(x=train_adlt[,-13], grouping = factor(train_adlt[,13]), kn = 3,
           gamma=0)

#İkili sınıflandırma için faktör değişkenlerinin kanıt dönüşümünün ağırlığını hesaplar.
cls7<-woe(x=adult, grouping = adult$income, weights = NULL, zeroadj = 0,
    ids = NULL, appont = TRUE)

# model başarım kriterleri - AC (accuracy)
test_iris<- iris[sample(nrow(iris), round(0.2*nrow(iris))),]

ucpm(predict(cl2,test_adlt[,-13])$posterior, factor(test_adlt[,13])) #0,59
ucpm(predict(cls1,test_adlt[,-13])$posterior, factor(test_adlt[,13])) #0,56
ucpm(predict(cls2,test_adlt[,-13])$posterior, factor(test_adlt[,13])) #0,48
ucpm(predict(cls3,test_adlt[,-13])$posterior, test_adlt[,13]) #0,62
ucpm(predict(cls4,test_iris[,-5])$posterior, test_iris[,5]) #0,97
##ucpm(predict(cls5,test_iris[,-5], quick = T,
##             return.subclass.prediction = TRUE)$posterior, test_iris[,5])
ucpm(predict(cls6,test_iris[,-5])$posterior, test_iris[,5]) #0,98
predict(cls7,adult[1:100,], replace = T) #only translate matrix
```


#  bst (71)

Temel öğreniciler olarak bileşen bazında doğrusal, düzleştirici eğriler
ve ağaç modelleri ile kayıp fonksiyonlarını optimize etmek için gradyan
artırma.  --SONUÇLAR SAÇMA--

```{r}
library('bst')
y<-ifelse(train_adlt[,13]<2,-1,1)
mdl<-bst(x=train_adlt[,-13], y=y, cost = 0.5, family = "gaussian",
#c( "hinge", "hinge2", "binom", "expo", "poisson", "tgaussianDC", "thingeDC",
#"tbinomDC", "binomdDC", "texpoDC", "tpoissonDC","huber", "thuberDC","clossR",
#"clossRMM", "closs", "gloss", "qloss", "clossMM","glossMM", "qlossMM", "lar"),
    ctrl = bst_control(mstop=100), control.tree = list(maxdepth = 1),
    learner = c("ls"))#, "sm", "tree"))

p<-predict(object=mdl, newdata=test_adlt[,-13], newy=NULL, mstop=100,
        type= "response")#"class", "all.res", "loss", "error"

#plot(x, type = c("step", "norm"),...)
coef(object=mdl, which=mdl$ctrl$mstop)

#mada :One-vs-all multi-class AdaBoost
#mbst :Boosting for Multi-Classification
#mhingebst :Boosting for Multi-class Classification
#mhingeova :Multi-class HingeBoost
#rbst :Robust Boosting for Robust Loss Functions
```

#  frbs (72)

Sınıflandırma ve regresyon görevleriyle ilgilenmek için bulanık kural
tabanlı sistemlere (FRBS'ler) dayalı çeşitli öğrenme algoritmalarının
uygulanması.  ------ÇOK HATALI TAHMİN YAPIYOR-----

```{r}
library('frbs')

range.data<- matrix(apply(train_adlt[,-13],2,range),ncol = 12)
train_adlt_n<-norm.data(train_adlt[,-13], range.data= range.data, min.scale = 0,
                        max.scale = 1)
train_adlt_n <-cbind(train_adlt_n, train_adlt[,13])
test_adlt_n<-norm.data(test_adlt[,-13], range.data= range.data, min.scale = 0,
                       max.scale = 1)


# model building
mdl<-frbs.learn(data.train = train_adlt_n, range.data = range.data,
                method.type = "FRBCS.W", control = list(num.labels=2,
                                                type.mf="GAUSSIAN",
                                                type.tnorm="MIN",
                                                type.snorm="MAX",
                                                type.implication.func= "ZADEH",
                                                name="Sim-0"))
prd<-predict(mdl, test_adlt_n)
```
 control:
• WM:regression
list(num.labels, type.mf, type.tnorm, type.defuz,type.implication.func, name)
• HYFIS: regression (neural network based)
list(num.labels, max.iter, step.size, type.tnorm,type.defuz,
     type.implication.func, name)
• ANFIS and FIR.DM:  regression (neural network based/gradient descent)
list(num.labels, max.iter, step.size,type.tnorm, type.implication.func , name)
• SBC: regression (clustering)
list(r.a, eps.high, eps.low, name)
• FS.HGD: regression (gradient descent)
list(num.labels, max.iter, step.size, alpha.heuristic,type.tnorm,
     type.implication.func, name)
• FRBCS.W and FRBCS.CHI:  classification
list(num.labels, type.mf, type.tnorm,type.implication.func, name)
• DENFIS method: regression (clustering)
list(Dthr, max.iter, step.size, d, name)
• GFS.FR.MOGUL:regression(genetic)
list(persen_cross, max.iter, max.gen, max.tune,persen_mutant, epsilon, name)
• GFS.THRIFT method: regression(genetic)
list(popu.size, num.labels, persen_cross,max.gen, persen_mutant, type.tnorm,
     type.defuz,type.implication.func, name)
• GFS.GCCL: classification (genetic)
list(popu.size, num.class, num.labels, persen_cross,max.gen, persen_mutant,name)
• FH.GBML: classification(genetic)
list(popu.size, max.num.rule, num.class, persen_cross,
max.gen, persen_mutant, p.dcare, p.gccl, name)
• SLAVE: classification(genetic)
list(num.class, num.labels, persen_cross, max.iter,
max.gen, persen_mutant, k.lower, k.upper, epsilon, name)
• GFS.LT.RS: regression(genetic)
list(popu.size, num.labels, persen_mutant, max.gen,mode.tuning, type.tnorm, type.implication.func,type.defuz, rule.selection, name)




#  VGAM (73)

VGAM, vektör genelleştirilmiş doğrusal ve toplam modelleri (VGLM'ler ve
VGAM'ler) ve ilgili modelleri (İndirgenmiş dereceli VGLM'ler, İkinci
Dereceden RR-VGLM'ler, İndirgenmiş dereceli VGAM'ler) uydurmak için
işlevler sağlar. Bu paket, maksimum olasılık tahmini (MLE) veya
cezalandırılmış MLE'ye göre birçok modele ve dağıtıma uyar. Ayrıca
ekolojideki kısıtlı ikinci dereceden koordinasyon (CQO) gibi kısıtlı
koordinasyon modellerine de uyar. ------ÇOK FAZLA RAM İSTİYOR \~50 GB

```{}
library('VGAM')

control <- vglm.control(checkwz = TRUE, Check.rank = TRUE, Check.cm.rank = TRUE,
                        criterion = names(.min.criterion.VGAM),epsilon = 1e-07,
                        half.stepsizing = TRUE, maxit = 30, noWarning = FALSE,
                        stepsize = 1, save.weights = FALSE, trace = FALSE,
                        wzepsilon = .Machine$double.eps^0.75, xij = NULL)

#vektör genelleştirilmiş toplam modeli (VGAM)
vgam(formula=csMPa~.,family = cratio(), data = beton,
     weights = NULL, subset = NULL,na.action = na.fail, etastart = NULL,
     mustart = NULL, coefstart = NULL, control = control,offset = NULL,
     method = "vgam.fit", model = FALSE,x.arg = TRUE, y.arg = TRUE,
     contrasts = NULL,constraints = NULL, extra = list(), form2 = NULL,
     qr.arg = FALSE, smart = TRUE)

#İndirgenmiş dereceli vektör genelleştirilmiş doğrusal model (RR-VGLM)
rrvglm(formula, family = stop("argument 'family' needs to be assigned"),
       data = list(), weights = NULL, subset = NULL,na.action = na.fail,
       etastart = NULL, mustart = NULL, coefstart = NULL,
       control = rrvglm.control(...), offset = NULL, method = "rrvglm.fit",
       model = FALSE, x.arg = TRUE, y.arg = TRUE,contrasts = NULL,
       constraints = NULL, extra = NULL,qr.arg = FALSE, smart = TRUE, ...)

#vektör genelleştirilmiş doğrusal modeller (VGLM)
vglm(formula=csMPa~.,family = cratio(), data = beton,
     weights = NULL, subset = NULL, na.action = na.fail, etastart = NULL,
     mustart = NULL, coefstart = NULL, control = control, offset = NULL,
     method = "vglm.fit", model = FALSE, x.arg = TRUE, y.arg = TRUE,
     contrasts = NULL, constraints = NULL, extra = list(),form2 = NULL,
     qr.arg = TRUE, smart = TRUE)

#Kısıtlı bir ikinci dereceden koordinasyon (CQO; daha önce kanonik Gauss koordinasyonu veya CGO olarak adlandırılıyordu) modeli, ikinci dereceden indirgenmiş sıralı vektör genelleştirilmiş doğrusal model (QRR-VGLM) çerçevesi kullanılarak takılır.
cqo(formula, family = stop("argument 'family' needs to be assigned"),data = list(),
    weights = NULL, subset = NULL,na.action = na.fail, etastart = NULL,
    mustart = NULL, coefstart = NULL, control = qrrvglm.control(...),
    offset = NULL,method = "cqo.fit", model = FALSE, x.arg = TRUE, y.arg = TRUE,
    contrasts = NULL, constraints = NULL, extra = NULL,smart = TRUE, ...)

#Goodman'ın RC ilişkilendirme modelini (GRC) bir sayım matrisine ve daha genel olarak satır sütun etkileşimine uyar
grc(y, Rank = 1, Index.corner = 2:(1 + Rank),str0 = 1, summary.arg = FALSE,
    h.step = 1e-04, ...)

rcim(y, family = poissonff, Rank = 0, M1 = NULL, weights = NULL, which.linpred = 1,
     Index.corner = ifelse(is.null(str0), 0, max(str0)) + 1:Rank,rprefix = "Row.",
     cprefix = "Col.", iprefix = "X2.", offset = 0, str0 = if (Rank) 1 else NULL,
     summary.arg = FALSE, h.step = 0.0001,rbaseline = 1, cbaseline = 1,
     has.intercept = TRUE, M = NULL, rindex = 2:nrow(y), cindex = 2:ncol(y),
     iindex = 2:nrow(y), ...)

#Kısıtlı bir toplamsal düzenleme (CAO) modeli, azaltılmış dereceli vektör genelleştirilmiş toplamsal model (RR-VGAM) çerçevesi kullanılarak takılır.
cao(formula, family = stop("argument 'family' needs to be assigned"),data = list(),
    weights = NULL, subset = NULL, na.action = na.fail,etastart = NULL,
    mustart = NULL, coefstart = NULL,control = cao.control(...), offset = NULL,
    method = "cao.fit", model = FALSE, x.arg = TRUE, y.arg = TRUE,
    contrasts = NULL, constraints = NULL,extra = NULL, qr.arg = FALSE,
    smart = TRUE, ...)

predictvglm(object, newdata = NULL,type = c("link", "response", "terms"),
            se.fit = FALSE, deriv = 0, dispersion = NULL,untransform = FALSE,
            type.fitted = NULL, percentiles = NULL)
```

#  mda (74)

Karışım ve esnek diskriminant analizi, çok değişkenli uyarlamalı
regresyon eğrileri (MARS), BRUTO ve vektör tepkisi yumuşatma eğrileri.
---SONUÇLAR SAÇMASAPAN---
```{r}
library(mda)
#----manipule test data, ? to NA and omit
ad <- adult#clear id column
ad <- replace(ad,ad=="?",NA)#replace ? to NA
ad <- droplevels(ad)#update levels
ad <- na.omit(ad)
ad$income <- cut(ad$capital_gain, breaks = c(0, 18000, 40000, 99999),
                 labels = c("<18K", "18K-40K", ">40K"),include.lowest = T)
#create 3 class


#bruto Fit an Additive Spline Model by Adaptive Backfitting
m1<-bruto(x=beton[1:900,-9], y=beton[1:900,9], w=1, wp=1/9, dfmax=50, cost=2,
      maxit.select=20, maxit.backfit=20,thresh = 0.0001, trace.bruto = FALSE,
      start.linear = TRUE)#, fit.object)

#fda Flexible Discriminant Analysis
m2<-fda(formula=income~., data=ad[1:20000,],
        #weights=rep(1,times=20000),theta=1, dimension=1, eps= .Machine$double.eps,
        method=mars, keep.fitted=F) #polyreg,bruto,gen.ridge


#mda Mixture Discriminant Analysis
m3<-mda(formula=income~., data=ad[1:20000,], method=mars)


#gen.ridge Penalized Regression:Cezalandırılmış diskriminant analizinde kullanıldığı gibi, cezalandırılmış bir regresyon gerçekleştirin
m4<-gen.ridge(x=adlt[,-13], y=adlt[,13])

#mars Çok değişkenli uyarlanabilir regresyon eğrileri.
m5<-mars(x=adlt[,-13], y=adlt[,13])

#polinom regression
m6<-polyreg(x=adlt[,-13], y=adlt[,13])

p1<-predict(m1,as.matrix(beton[901:1030,-9]), type = "fitted")
p2<-predict(m2,ad[20001:30161,-13], type = "class")
p3<-predict(m3,ad[20001:30161,-13], type = "class")
p4<-predict(m4,adlt[30001:32560,-13])
p5<-predict(m5,adlt[30001:32560,-13])
p6<-predict(m6,adlt[30001:32560,-13])

confusion(p2, ad[20001:30161,13])
confusion(p3, ad[20001:30161,13])

Metrics::mae(beton[901:1030,9], p1)
Metrics::accuracy(ad[20001:30161,13], p2)
Metrics::accuracy(ad[20001:30161,13], p3)
```


#  nlme (75)

Gauss doğrusal ve doğrusal olmayan karma etki modellerini yerleştirin ve
karşılaştırın. -- AŞIRI KARIŞIK, AYARLAMASI PROBLEMLİ---

```{r}
library('nlme')
b<-beton[,c(9,1:8)]
fb<-formula(b);rm(b)
#Bu fonksiyon genelleştirilmiş en küçük kareler kullanan doğrusal bir modele uyar.
mm<-gls(model=fb, data=beton, correlation=NULL, weights=NULL,#subset=NULL,
    method='ML', na.action=na.fail,control=list(), verbose=FALSE)

#Bu fonksiyon genelleştirilmiş en küçük kareler kullanan doğrusal olmayan bir modele uyar.
#gnls(model=csMPa ~ cement+water+age|sqrt(coarseaggregate), data=beton,
#     params= 1|sqrt(beton$coarseaggregate), start=1, correlation=NULL, weights=NULL,
#     subset=1:8, na.action=na.fail,
#     naPattern=NULL, control=list(), verbose=FALSE)

#Doğrusal Karma Efekt Modelleri
#lme(csMPa ~ fineaggregate+age, data = beton[,7:9], random = ~ 1)
#fixed, data, random, correlation, weights, subset, method,na.action, control,
#    contrasts = NULL, keep.data = TRUE)

#Doğrusal Olmayan Karma Etki Modelleri
#nlme(model, data, fixed, random, groups, start, correlation, weights,subset, method,
#     na.action, naPattern, control, verbose)


#plot(mm)
p<-predict(mm,beton[1:100,-9])
Metrics::mae(beton[1:100,9],p)
#qqnorm(mm)
```
BUNUNLA BİRLİKTE KULLANILMASI GEREKEN PAKETLER
-cAIC4:	'lme4' ve 'nlme' için Koşullu Akaike Bilgi Kriteri
-gamm4:	'mgcv' ve 'lme4' kullanan Genelleştirilmiş Katkı Karışık Modelleri
-trouBBlme4SolveR:	'lme4' için Sorun Çözücü
-Certara.NLME8:	Certara'nın Doğrusal Olmayan Karışık Efekt Modelleme Motoru için Yardımcı Programlar
-covBM:	Brownian Motion Processes for 'nlme'-Models
-ggPMX:	NLME için Tanı Grafiklerini Kolaylaştırmak için 'ggplot2' Tabanlı Araç Model
-lmeSplines:	'nlme'ye Yumuşatma Spline Modelleme Özelliği Ekleyin
-nlmeU:	'nlme'nin İşlevselliğini Artıran Veri Kümeleri ve Yardımcı İşlevler Paket
-nlmeVPC:	Doğrusal Olmayan Karışık Efekt Modeli için Görsel Model Kontrolü





#  bartMachine (76)

Bayesian Toplamalı Regresyon Ağaçlarının veri analizi ve
görselleştirmeye yönelik genişletilmiş özelliklere sahip gelişmiş bir
uygulaması. -- SÜREKLİ RAM PROBLEMİ VERİYOR--

```{r}
library('bartMachine')
options(java.parameters = "-Xmx8192m")#java RAM problemi için

#Builds a BART model for regression or classification.
m1<-bartMachine(X = train_adlt[,-13], y = train_adlt[,13], Xy = NULL,
                num_trees = 50,
            num_burn_in = 250, num_iterations_after_burn_in = 1000,
            alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3,
            prob_rule_class = 0.5, mh_prob_steps = c(2.5, 2.5, 4)/9,
            debug_log = FALSE, run_in_sample = TRUE, s_sq_y = "mse",
            sig_sq_est = NULL, print_tree_illustrations = FALSE,
            cov_prior_vec = NULL, interaction_constraints = NULL,
            use_missing_data = FALSE, covariates_to_permute = NULL,
            num_rand_samps_in_library = 10000,
            use_missing_data_dummies_as_covars = FALSE,
            replace_missing_data_with_x_j_bar = FALSE,
            impute_missingness_with_rf_impute = FALSE,
            impute_missingness_with_x_j_bar_for_lm = TRUE,
            mem_cache_for_speed = TRUE, flush_indices_to_save_RAM = TRUE,
            serialize = FALSE, seed = NULL,verbose = TRUE)

m2<-build_bart_machine(X = adult[,-c(3,13)], y = adult[,13], Xy = NULL,
                   num_trees = 50, num_burn_in = 250,
                   num_iterations_after_burn_in = 1000, alpha = 0.95, beta = 2,
                   k = 2, q = 0.9, nu = 3, prob_rule_class = 0.5,
                   mh_prob_steps = c(2.5, 2.5, 4)/9, debug_log = FALSE,
                   run_in_sample = TRUE, s_sq_y = "mse", sig_sq_est = NULL,
                   print_tree_illustrations = FALSE, cov_prior_vec = NULL,
                   interaction_constraints = NULL, use_missing_data = FALSE,
                   covariates_to_permute = NULL,
                   num_rand_samps_in_library = 10000,
                   use_missing_data_dummies_as_covars = FALSE,
                   replace_missing_data_with_x_j_bar = FALSE,
                   impute_missingness_with_rf_impute = FALSE,
                   impute_missingness_with_x_j_bar_for_lm = TRUE,
                   mem_cache_for_speed = TRUE, flush_indices_to_save_RAM = TRUE,
                   serialize = FALSE, seed = NULL, verbose = TRUE)

bart_predict_for_test_data(bart_machine=m1, Xtest=test_adlt[,-c(3,13)],
          ytest=test_adlt[,13], prob_rule_class = NULL)
plot(m2)
```

#  BayesTree (77)

Bu BART:Bayesian Katkısal Regresyon Ağaçlarının bir uygulamasıdır
--TAHMİN YAPILAMAZ -- predict yok
```{r}
library('BayesTree')
#makeind(adult,all=T) #faktör değişkenleri dummy kolona çevirir

mdl<-bart(x.train=as.matrix(train_adlt[,-13]), y.train=train_adlt[,13],
          x.test=matrix(0.0,22792,12), sigest=NA, sigdf=2, sigquant=.9, k=2.0,
          power=2.0, base=.95,binaryOffset=0, ntree=200, ndpost=1000, nskip=100,
          printevery=100, keepevery=1, keeptrainfits=TRUE, usequants=FALSE,
          numcut=100, printcutoffs=0, verbose=TRUE)

summary(mdl)
```


#  dipm (78)

DIPM yöntemi, belirli bir tedavi grubunda özellikle zayıf veya güçlü
performansa sahip alt grupları arayan bir sınıflandırma ağacıdır. Depth
Importance in Precision Medicine (DIPM) ----SIKINTILI----

```{r}
library('dipm')
lung<-survival::lung
lung1<-na.omit(lung)
lung1$status<- lung1$status-1
lung1$sex<- lung1$sex-1
#Bu işlev, belirli bir tedavi grubunda deneklerin özellikle iyi veya özellikle kötü performans gösterdiği alt grupları tanımlamak için tasarlanmış bir sınıflandırma ağacı oluşturur.
dipm(formula=Surv(time, status) ~ sex | ., data = lung1)
     #types = "nominal",
     ## "response", "treatment", "status", "binary", "ordinal", "nominal"
     #nmin = 5, nmin2 = 5, ntree = ceiling(min(max(sqrt(228), sqrt(10)), 1000)),
     #mtry = Inf, maxdepth = 2, maxdepth2 = 6, print = TRUE,
     #dataframe = FALSE, prune = FALSE)

#Bu işlev, belirli bir tedavi grubunda deneklerin özellikle iyi veya özellikle kötü performans gösterdiği alt grupları tanımlamak için tasarlanmış bir sınıflandırma ağacı oluşturur.
spmtree(formula=Surv(time, status) ~ sex | ., data = lung1)
        #types = NULL, nmin = 5, maxdepth = Inf, print = TRUE,
        #dataframe = FALSE, prune = FALSE)
```




#  lars (79)

Tek bir en küçük kareler uyumu maliyetiyle tüm bir kement dizisinin
yerleştirilmesi için etkili prosedürler. En küçük açı regresyonu ve
sonsuz küçük ileri aşamalı regresyon, aşağıdaki makalede açıklandığı
gibi kementle ilgilidir. Gürültülü veride hatalı sonuç verir.
--TAHMİN YAPMIYOR--çıkan şey başka
```{r}
library('lars')

mdl<-lars(x=scale(life[,-c(1,9)]), y=life[,9], type = "lasso", #lar,forward.stagewise,stepwise
     trace = FALSE, normalize = FALSE, intercept = TRUE,
     Gram = NULL, eps = 1e-12, max.steps=9, use.Gram = FALSE)

predict.lars(object=mdl, newx=life[1:25,-c(1,9)], s=2, type = "fit",
        mode = c("step","fraction", "norm", "lambda"))
```


#  pre (80)

Tahmin kuralı topluluklarını (PRE'ler) türetir. Büyük ölçüde, Friedman
ve Popescu (2008)'da açıklanan PRE'lerin türetilmesi prosedürünü,
ayarlamalar ve iyileştirmelerle takip etmektedir. Ana işlev pre(),
sürekli, ikili, sayım, çok terimli ve çok değişkenli sürekli yanıtlar
için kurallardan ve/veya doğrusal terimlerden oluşan tahmin kural
topluluklarını türetir. gpe() işlevi, tahmin değişkenlerinin
kurallarından, menteşe ve doğrusal işlevlerinden oluşan genelleştirilmiş
tahmin topluluklarını türetir.
--BAŞKA PAKETLERDEN DERLEME-- ayarlama ve tahmini tuhaf
```{r}
library('pre')
#Genel Tahmin Topluluğu (gpe) Türetme
mdl1<-gpe(formula=income~., data=train_adlt,
    base_learners = list(gpe_trees(), gpe_linear()),
    weights = rep(1, times = nrow(train_adlt)),sample_func = gpe_sample(),
    verbose = FALSE, penalized_trainer = gpe_cv.glmnet(),model = TRUE)

#Bir tahmin kuralı topluluğu türetin
mdl2<-pre(formula=income~., data=train_adlt)#, family = "binomial")
    #use.grad = TRUE, weights=NULL, type = "both", sampfrac = 0.5, maxdepth =3L,
    #learnrate = 0.01, mtry = Inf, ntrees = 500, confirmatory = NULL,
    #singleconditions = FALSE, winsfrac = 0.025, normalize = TRUE,
    #standardize = FALSE, ordinal = TRUE, nfolds = 10L,
    #tree.control=rpart_control(), #????
    #tree.unbiased = TRUE, removecomplements = TRUE,
    #removeduplicates = TRUE, verbose = FALSE, par.init = FALSE,
    #par.final = FALSE, sparse = FALSE)

predict(object=mdl1, newdata = test_adlt[,-13],type = "link",
        penalty.par.val = "lambda.1se")
predict(object=mdl2, newdata = test_adlt[,-13],type = "class",
        penalty.par.val = "lambda.1se")
```



#  rminer (81)

Kısa ve tutarlı bir işlevler kümesi sunarak sınıflandırma ve regresyon
(zaman serisi tahmini dahil) görevlerinde veri madenciliği
algoritmalarının kullanımını kolaylaştırır. fonksiyonları tamamen diğer
paketlerden çağırır. 
----PEKÇOK PAKETİN DERLENMİŞ HALİ ANCAK HATAYA ÇOK FAZLA AÇIK----

```{r}
library('rminer')

# train test data split (H$tr, H$ts)
H<-holdout(y=adlt[,13], ratio = 2/3, internalsplit = FALSE, mode = "stratified",
           #stratified,order,rolling,incremental,random
        iter = 1, seed = NULL, window=10, increment=1)

mdl<-fit(x=income~., data = adlt[H$tr,], model = "kknn", task = "default",
    search = "heuristic", mpar = NULL, feature = "none", scale = "default",
    transform = "none", created = NULL, fdebug = FALSE)

Metrics::accuracy(test_adlt[,13], ifelse(predict(mdl, adlt[H$ts,-13])<1.5,1,2))
```

MODEL: naive, ctree, cv.glm.net, rpart, kknn, knn, ksvm, lssvm, mlp,
mlpe, randomForest, xgboost, bagging, boosting, lda, multinom(lr),
naiveBayes, qda, cubist, lm, mr, mars, pcr, plsr, cppls, rvm 
TASK: prob,class, reg, "default"
Not: imputation özelliği sadece sayısal verilerde geçerli


#  Boruta (82)

öznitellik seçimi sarmalayıcı algoritması. Orijinal niteliklerin önemini
rastgele elde edilebilen önemle karşılaştırarak, bunların değiştirilmiş
kopyaları (shado) kullanılarak tahmin edilerek ilgili özellikleri bulur.
-- MODEL DEĞİL, SADECE YARDIMCI---
```{r}
library('Boruta')
#Feature selection with the Boruta algorithm
ss<-Boruta(x=adlt[,-13],y=adlt[,13],pValue = 0.01,mcAdj = TRUE,maxRuns = 100,
       doTrace = 0, holdHistory = TRUE, getImp = getImpRfZ)
getImpLegacyRfZ(x=adlt[,-13],y=factor(adlt[,13]))
getImpRfZ(x=adlt[,-13],y=factor(adlt[,13]), ntree = 500, num.trees = 500)
attStats(ss)
```

#  DALEX (83)

DALEX paketi, girdi değişkenleri ile model çıktısı arasındaki bağlantıyı
anlamaya yardımcı olan çeşitli yöntemler içerir. Uygulanan yöntemler,
modeli tek bir örnek düzeyinde ve tüm veri kümesi düzeyinde keşfetmeye
yardımcı olur.
--- MODEL DEĞİL SADECE YARDIMCI---
```{r}
library('DALEX')
#Bu işlev, açıklamalar için işlevler tarafından daha fazla işlenebilecek bir modelin birleşik bir temsilini oluşturur.
ex<-explain(model=mdl,data = adlt[,-13],y = adlt[,13],predict_function = NULL,
        predict_function_target_column = NULL,residual_function = NULL,
        weights = NULL,label = NULL,verbose = TRUE,precalculate = TRUE,
        colorize = !isTRUE(getOption("knitr.in.progress")),model_info = NULL,
        type = "classification")

#Bu, açıklayıcı nesneler için çalışan genel bir predict() işlevidir.
model_prediction(explainer=ex, new_data=test_adlt[,-13])

#Bu genel işlev, kullanıcının model hakkındaki temel bilgileri çıkarmasına olanak tanır
model_info(model=mdl, is_multiclass = FALSE)

#Bu fonksiyon artıkların model teşhisini gerçekleştirir. Artıklar tahminlere, gerçek y değerlerine veya seçilen değişkenlere göre hesaplanır ve grafiği çizilir
model_diagnostics(explainer=ex, variables = NULL)

#bu işlev feature_importance'ı çağırır
model_parts(explainer=ex, loss_function = loss_default(ex$model_info$type),
            type = "variable_importance",N = 1000,n_sample = 1000)

#sınıflandırma ve regresyon modelleri için çeşitli performans ölçümlerini hesaplar
model_performance(explainer=ex, cutoff = 0.5)

#model yanıtını araştıran bir veri kümesi düzeyindeki açıklamaları hesaplar
model_profile(explainer=ex, variables = NULL, N = 100, groups = NULL,
              k = NULL, center = TRUE, type = "partial")

predict_diagnostics(explainer=ex, new_observation = adlt[1:100,-13])
predict_parts(explainer=ex, new_observation = adlt[1:100,-13])
predict_profile(explainer = ex, new_observation = adlt[1:100,-13])
#shap_aggregated(explainer = ex, new_observation = adlt[1:100,-13])
#variable_effect(explainer = ex, new_observation = adlt[1:100,-13])
```

işlenecek modellers: model info • class cv.glmnet and glmnet - models
created with glmnet package • class glm - generalized linear models •
class lrm - modelscreated with rms package, • class model_fit - models
created with parsnip package • class lm - linear models created with
stats::lm • class ranger - models created with ranger package • class
randomForest-random forest models created with randomForest package •
class svm -support vector machines models created with the e1071 package
• class train - models created with caret package • class gbm - models
created with gbm package

#  GMMBoost (84)

Bu paket, Genelleştirilmiş karma modeller için olasılığa dayalı
güçlendirme yaklaşımları sağlar
--TAHMİN FONKSİYONU YOK, AŞIRI TEKNİK---
```{r}
library('GMMBoost')
data(knee)
#Yarı parametrik bir karma modeli veya genelleştirilmiş bir yarı parametrik karma modeli takın.
bGAMM(fix=pain ~ time + th,
      add= ~ age + sex,
      rnd= list(id=~1),
      data=knee, lambda=1e+5, family =  poisson(link = log),
      control = bGAMMControl(nue=0.1,add.fix=NULL,start=NULL,q_start=NULL,
                             OPT=TRUE,nbasis=20,spline.degree=3,diff.ord=2,
                             sel.method="aic",steps=500,method="EM",
                             overdispersion=FALSE)
      )

#Doğrusal bir karma modeli veya genelleştirilmiş bir doğrusal karma modeli takın.
bGLMM(fix=pain ~ time + th + I(age^2), rnd=list(id=~1+sex), data=knee,
      family =  poisson(link = log), 
      control = bGLMMControl(nue=0.1, lin="(Intercept)", start=NULL,
                             q_start=NULL, OPT=TRUE,sel.method="aic", steps=500,
                             method="REM",overdispersion=FALSE,print.iter=TRUE)
      )

#Sıralı yanıtlı genelleştirilmiş bir doğrusal karma modeli takın
OrdinalBoost(fix=pain ~ time + th + age + sex, rnd= list(id=~1), data=knee,
             model="sequential",
             control = OrdinalBoostControl(nue=0.1, lin=NULL, katvar=NULL,
                                           start=NULL, q_start=NULL,OPT=TRUE,
                                           sel.method="aic", steps=100,
                                           method="REML", maxIter=500,
                                           print.iter.final=FALSE,
                                           eps.final=1e-5)
             )
```


#  ordinalForest (85)

Sıralı orman (OF) yöntemi, yüksek boyutlu ve düşük boyutlu verilerle
sıralı regresyona izin verir. Bir eğitim veri seti kullanarak bir OF
tahmin kuralı oluşturduktan sonra, yeni gözlemler için sıralı hedef
değişkenin değerlerini tahmin etmek için kullanılabilir.
-----çakma RANGER paketi----
```{r}
library('ordinalForest')

data(hearth)
mdl<-ordfor(depvar="Class", data=hearth, nsets = 1000, ntreeperdiv = 100)#,
#            ntreefinal = 5000,importance = c("rps", "accuracy"),
#  perffunction = "equal",#"probability","proportional","oneclass","custom"
#  classimp, classweights, nbest = 10, naive = FALSE, num.threads = NULL,
#  npermtrial = 500, permperdefault = FALSE, mtry = NULL, min.node.size = NULL,
#  replace = TRUE, sample.fraction = ifelse(replace, 1, 0.632),
#  always.split.variables = NULL, keep.inbag = FALSE)

preds<-predict(mdl, hearth[1:100,])

table('true'=hearth$Class[1:100], 'predicted'=preds$ypred)
perff_equal(ytest=hearth$Class[1:100], ytestpred=preds$ypred)
perff_proportional(ytest=hearth$Class[1:100], ytestpred=preds$ypred)
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="1")
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="2")
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="3")
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="4")
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="5")
perff_custom(ytest=hearth$Class[1:100], ytestpred=preds$ypred,
             classweights=c(1,2,1,1,1))
```


#  brnn (86)

iki katmanlı bir sinir ağına uyar. Başlangıç ağırlıklarını atamak için
Nguyen ve Widrow algoritmasını (1990) ve optimizasyonu gerçekleştirmek
için Gauss-Newton algoritmasını kullanır
--ÇOK TUHAF BİR METOD-- sonuçlarda garip
```{r}
library(brnn)
#iki katmanlı bir sinir ağına uyar. Başlangıç ağırlıklarını atamak için Nguyen ve Widrow algoritmasını (1990) ve optimizasyonu gerçekleştirmek için Gauss-Newton algoritmasını kullanır
mdl1<-brnn(x=as.matrix(train_adlt[,-13]),y=train_adlt[,13],neurons=2,
           normalize=TRUE, epochs=1000, mu=0.005,mu_dec=0.1,mu_inc=10,
           mu_max=1e10,min_grad=1e-10, change = 0.001, cores=1,verbose=FALSE,
           Monte_Carlo = FALSE,tol = 1e-06, samples = 40)

#Brnn_extulated işlevi, iki katmanlı bir sinir ağına uyar. Başlangıç ağırlıklarını atamak için Nguyen ve Widrow algoritmasını (1990) ve optimizasyonu gerçekleştirmek için Gauss-Newton algoritmasını kullanır. Gizli katman, iki grup giriş değişkeni için farklı ön dağılımlar atamamıza izin veren iki grup nöron içerir.
#GELİR ve HARCAMA GRUP YAPALIM, KALANI BAŞKA GRUP
X=train_adlt[,-c(9,10,13)];Z=train_adlt[,c(9,10)];Y=train_adlt[,13]

mdl2<-brnn_extended(x=as.matrix(X),y=Y,z=as.matrix(Z),neurons1=2,neurons2=2,
              normalize=TRUE,epochs=1000,mu=0.005,mu_dec=0.1,mu_inc=10,
              mu_max=1e10,min_grad=1e-10,change = 0.001, cores=1,verbose =FALSE)

# sıralı veri oluştur
ad<-adlt #dört sınıf elde etmek için
ad$income <- cut(ad$capital_gain, breaks = c(0, 100,10000, 40000, 99999),
                 labels = c(1, 2, 3, 4),include.lowest = T)

#Brnn_ordinal işlevi, Sıralı veriler için Bayesian Düzenlileştirilmiş Sinir Ağına uyar
mdl3<-brnn_ordinal(x=as.matrix(ad[,-13]),y=as.numeric(ad[,13]),neurons=2,
             normalize=TRUE,epochs=1000,mu=0.005,mu_dec=0.1,mu_inc=10,
             mu_max=1e10,min_grad=1e-10,change_F=0.01,change_par=0.01,
             iter_EM=1000,verbose=FALSE)


p1<-predict(object=mdl1,newdata=test_adlt[,-13])
p2<-predict(object=mdl2,newdata=test_adlt[,-13])
p3<-predict(object=mdl3,newdata=ad[1:1000,-13])
```

#  caTools (87)

Aşağıdakiler de dahil olmak üzere çeşitli temel yardımcı işlevler
içerir: hareketli (dönen, çalışan) pencere istatistik işlevleri, GIF ve
ENVI ikili dosyaları için okuma/yazma, AUC'nin hızlı hesaplanması,
LogitBoost sınıflandırıcı, base64 kodlayıcı/kod çözücü, yuvarlama
hatasız toplam ve toplam toplamı , vesaire.
--TAHMİNDE OLDUKÇA FAZLA <NA> DEĞERLER ÇIKIYOR---
```{r}
library(caTools)

t<-sample.split(Y=adlt$income, SplitRatio = .7, group = NULL )
train<-adlt[t,]
test<-adlt[!t,]

#Zayıf öğrenenler olarak karar kütüklerini (tek düğümlü karar ağaçları) kullanarak logitboost sınıflandırma algoritmasını eğitin.
md<-LogitBoost(xlearn=train[,-13], ylearn=train[,13],
           nIter=ncol(train[,-13]))

p<-predict.LogitBoost(object=md,xtest=test[,-13],type="class", nIter = 12)
table(p, test$income)
table(p,useNA = "always")
```

#  class (88)

K-en yakın komşu, Öğrenme Vektörü Nicelemesi ve Kendi Kendini Düzenleyen
Haritalar dahil olmak üzere sınıflandırmaya yönelik çeşitli işlevler
--TAHMİN YAPAMAZ----
```{r}
library(class)
knn(train=train_adlt[,-13], test=test_adlt[,-13], cl=train_adlt[,13], k = 1,
    l = 0, prob = FALSE, use.all = TRUE)
knn.cv(train=train_adlt[,-13], cl=train_adlt[,13], k = 1, l = 0, prob = FALSE,
       use.all = TRUE)
knn1(train=train_adlt[,-13], test=test_adlt[,-13], cl=train_adlt[,13])
```


#  kerndwd (89)

Doğrusal mesafe ağırlıklı ayrımcılığı ve çekirdek mesafe ağırlıklı
ayrımcılığı çözen yeni bir uygulama. Sınıflandırma için çekirdek Hilbert
uzaylarının yeniden üretilmesinde doğrusal genelleştirilmiş DWD ve
çekirdek genelleştirilmiş DWD'yi çözmek için son derece yeni ve etkili
prosedürler. Algoritma, tüm çözüm yolunu belirli bir düzenlileştirme
parametreleri ızgarasında hesaplamak için çoğunluk minimizasyonu (MM)
ilkesine dayanmaktadır. ----ÇALIŞMASI UZUN SÜRÜYOR---

```{r}
library(kerndwd)

#Doğrusal genelleştirilmiş mesafe ağırlıklı ayrım (DWD) modelini ve genelleştirilmiş DWD'yi Çekirdek Hilbert uzayının çoğaltılması üzerine yerleştirin. Çözüm yolu, lambda ayarlama parametresinin değerlerinin bir tablosunda hesaplanır.
md<-kerndwd(x=scale(train_adlt[,-13]), y=ifelse(train_adlt$income==1,-1,1),
        kern= rbfdot(sigma=.1), lambda=10^(seq(3, -3, length.out=10)),
        qval=1, wt=NULL, eps=1e-03, maxit=1e+03)

p<-predict(object=md, kern=rbfdot(sigma=.1), x=train_adlt[,-13],
        newx=test_adlt[,-13], type="class") #"link"
```


#  KRLS (90)

Paket, doğrusallık veya toplanabilirlik varsayımlarına dayanmadan
regresyon ve sınıflandırma sorunları için çok boyutlu işlevler y=f(x)'e
uyacak bir makine öğrenme yöntemi olan Çekirdek Tabanlı
Düzenlileştirilmiş En Küçük Kareler'i (KRLS) uygular. KRLS, Gauss
çekirdeklerini radyal temel fonksiyonlar olarak kullanarak Tikhonov
düzenlileştirme probleminin kare kaybını en aza indirerek en uygun
fonksiyonu bulur. --ÇALIŞMASI ÇOOOOK UZUN SÜRÜYOR--

```{r}
library(KRLS)
mdl<-krls(X = train_adlt[1:1000,-13], y = train_adlt[1:1000,13],
          whichkernel = "gaussian", lambda = NULL,sigma = NULL,
          derivative = TRUE, binary= TRUE, vcov=TRUE, print.level = 1,
          L=NULL,U=NULL,tol=NULL,eigtrunc=NULL)


p<-predict(object=mdl, newdata=test_adlt[1:100,-13], se.fit = FALSE)
Metrics::accuracy(test_adlt[1:100,13],ifelse(p$fit<1.5,1,2))
```
0.82


#  LogicReg (91)

lojistik regresyon tahminleri --- tüm gözlem değerlerinin 0-1 olması
gerekiyor, yanıt değişekni sürekli olabilir
--AŞIRI TEKNİK DURUYOR---
```{r}
library(LogicReg)
data("logreg.testdat")
logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], #sep=NULL,
       type=2, select=1, ntrees=2, nleaves=1)# wgt=NULL, cens=NULL,
       #penalty=2, seed=1, kfold=10, nrep=10, oldfit=NULL,
       #anneal.control=logreg.anneal.control(start=-1, end=-4, iter=500,
      #                                       earlyout=0,update=100),
       #tree.control=logreg.tree.control(treesize=8, opers=1,minmass=0, n1=500),
       #mc.control=logreg.mc.control(nburn=1000, niter=25000, hyperpars=0,
      #                              update=0,output=4))

#predict(object, msz, ntr, newbin, newsep)
```

type of model to be fit: (1) classification, (2) regression, (3)
logistic regression, (4) proportional hazards model (Cox regression),
(5) exponential survival model, or (0) your own scoring function. If
type = 0, the code needs to be recompiled, uncompiled type = 0 results
in a constant score of 0, which may be useful to generate a sample from
the prior when select = 7 (Monte Carlo Logic Regression)

select; (1) fit a single model, (2) fit multiple models, (3)
cross-validation, (4) null-model permutation test, (5) conditional
permutation test, (6) a greedy stepwise algorithm, or (7) Monte Carlo
Logic Regression (using MCMC).

#  msaenet (92)

Yüksek boyutlu regresyonlarda özellik seçimi için çok adımlı
uyarlanabilir elastik ağ (MSAENet) algoritması, çok adımlı uyarlanabilir
MCP-Net (MSAMNet) ve çok adımlı uyarlanabilir SCAD-Net ( MSASNet)
yöntemler. ---GLMNET ALT KOPYASI, HATA VERİYOR---
ncvreg paketinin çakma kopyası, sonuçlar tamamen tesadüf
```{r}
library(msaenet)
#Adaptive Elastic-Net
m1<-aenet(x=as.matrix(beton[,-9]), y=beton[,9],
      family = "gaussian", #"binomial", "poisson","cox",
      init = "enet",# "ridge"),
      alphas = seq(0.05, 0.95, 0.05),
      tune = "cv",# "ebic", "bic", "aic"),
      nfolds = 5L, rule = "lambda.min",# "lambda.1se"),
      ebic.gamma = 1, scale = 1,
      lower.limits = -Inf, upper.limits = Inf,
      penalty.factor.init = rep(1, 8),
      seed = 1001, parallel = FALSE, verbose = FALSE)

#Adaptive MCP-Net
m2<-amnet(x=as.matrix(train_adlt[,-13]), y=train_adlt[,13], family = "binomial",
    init = c("mnet", "ridge"), gammas = 3, alphas = seq(0.05, 0.95, 0.05),
    tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
    ebic.gamma = 1, scale = 1, eps = 1e-04, max.iter = 10000L,
    penalty.factor.init = rep(1, 12), seed = 1001,
    parallel = FALSE, verbose = FALSE)

#Adaptive SCAD-Net
m3<-asnet(x=as.matrix(train_adlt[,-13]), y=train_adlt[,13], family ="binomial",
    init = c("snet", "ridge"), gammas = 3.7, alphas = seq(0.05, 0.95, 0.05),
    tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
    ebic.gamma = 1, scale = 1, eps = 1e-04, max.iter = 10000L,
    penalty.factor.init = rep(1, 12), seed = 1001,
    parallel = FALSE, verbose = FALSE)

#Multi-Step Adaptive Elastic-Net
m4<-msaenet(x=as.matrix(train_adlt[,-13]),y=train_adlt[,13],family ="binomial",
    init = c("enet", "ridge"), alphas = seq(0.05, 0.95, 0.05),
    tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
    rule = c("lambda.min", "lambda.1se"), ebic.gamma = 1, nsteps = 2L,
    tune.nsteps = c("max", "ebic", "bic", "aic"), ebic.gamma.nsteps = 1,
    scale = 1, lower.limits = -Inf, upper.limits = Inf,
    penalty.factor.init = rep(1, 12), seed = 1001,
    parallel = FALSE, verbose = FALSE)

#Multi-Step Adaptive MCP-Net
m5<-msamnet(x=as.matrix(train_adlt[,-13]),y=train_adlt[,13],family = "binomial",
    init = c("mnet", "ridge"), gammas = 3, alphas = seq(0.05, 0.95,
    0.05), tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
    ebic.gamma = 1, nsteps = 2L, tune.nsteps = c("max", "ebic", "bic",
    "aic"), ebic.gamma.nsteps = 1, scale = 1, eps = 1e-04,
    max.iter = 10000L, penalty.factor.init = rep(1, 12),
    seed = 1001, parallel = FALSE, verbose = FALSE)

#Multi-Step Adaptive SCAD-Net
m6<-msasnet(x=as.matrix(train_adlt[,-13]),y=train_adlt[,13],family = "binomial",
    init = c("snet", "ridge"), gammas = 3.7, alphas = seq(0.05, 0.95, 0.05),
    tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L, ebic.gamma = 1,
    nsteps = 2L, tune.nsteps = c("max", "ebic", "bic", "aic"),
    ebic.gamma.nsteps = 1, scale = 1, eps = 1e-04, max.iter = 10000L,
    penalty.factor.init = rep(1, 12), seed = 1001, parallel = FALSE,
    verbose = FALSE)

p1<-predict(object=m1, newx = as.matrix(beton[1:100,-9]))
p2<-predict(object=m2, newx = as.matrix(test_adlt[,-13]))
p3<-predict(object=m3, newx = as.matrix(test_adlt[,-13]))
p4<-predict(object=m4, newx = as.matrix(test_adlt[,-13]))
p5<-predict(object=m5, newx = as.matrix(test_adlt[,-13]))
p6<-predict(object=m6, newx = as.matrix(test_adlt[,-13]))


msaenet.mae(yreal=beton[1:100,9], ypred=p1)
msaenet.mae(yreal=test_adlt[,13], ypred=p2)
msaenet.mae(yreal=test_adlt[,13], ypred=p3)
msaenet.mae(yreal=test_adlt[,13], ypred=p4)
msaenet.mae(yreal=test_adlt[,13], ypred=p5)
msaenet.mae(yreal=test_adlt[,13], ypred=p6)
```
 12.63654
 2.867731
 2.867731
 2.861906
 2.86916
 2.869077


#  robustDA (93)

Bouveyron ve Girard, 2009'da önerilen sağlam karışım ayrıştırma analizi
(RMDA), etiket gürültüsü içeren verileri öğrenerek sağlam bir denetimli
sınıflandırıcı oluşturmaya olanak tanır. Önerilen yöntemin fikri,
tutarsızlıkları tespit etmek için öğrenme verilerinin etiketleri
tarafından taşınan denetimli bilgilerle verilerin denetimsiz
modellemesini karşılaştırmaktır. Yöntem daha sonra etiketlerde tespit
edilen tutarsızlıkları dikkate alarak sağlam bir sınıflandırıcı
oluşturabilir. MASS VE mclust çakması gibi

```{r}
library(robustDA)

m<-rmda(X=train_adlt[,c(1,11)],
     cls=train_adlt[,13], K = 3, model = "VEV")

p<-predict(object=m, X=test_adlt[,c(1,11)])
table(p$cls, test_adlt[,13])

#rmda(X=iris[,1:2], cls=as.numeric(factor(iris[,5])), K = 3, model = "VEV")
#lda(iris[,1:2],iris[,5])#MASS
#MclustDA(iris[,1:2],iris[,5])#mclust
```


#  rqPen (94)

Grup cezaları da dahil olmak üzere LASSO, elastik ağ, SCAD ve MCP ceza fonksiyonlarıyla cezalandırılmış niceliksel regresyon gerçekleştirir.
--SONUÇLAR BİRAZ YUHAF--
```{r}
library(rqPen)
#Grup cezalandırılmış amaç fonksiyonu kullanan niceliksel regresyon modellerine uyar.
m1<-rq.group.pen(
  x=as.matrix(beton[,-9]),
  y=beton[,9],
  tau = 0.5,
  groups = c(2,1,1,1,1,2,3,4),
  penalty = "gLASSO", #"gAdLASSO", "gSCAD", "gMCP"
  lambda = NULL,
  nlambda = 100,
  eps = ifelse(nrow(beton) < ncol(beton[,-9]), 0.05, 0.01),
  alg = "huber", #c("huber", "br", "qicd"),
  a = NULL,
  norm = 2,
  group.pen.factor = NULL,
  tau.penalty.factor = 1,#rep(1, length(tau)),
  scalex = TRUE,
  coef.cutoff = 1e-08,
  max.iter = 500,
  converge.eps = 1e-04,
  gamma = IQR(beton[,9])/10,
  lambda.discard = TRUE,
  weights = NULL)

#Cezalandırılmış bir nicelik kaybı fonksiyonunu kullanarak bir niceliksel regresyon modelini yerleştirin.
m2<-rq.pen( x=as.matrix(beton[,-9]), y=beton[,9])
  #tau = 0.5,
  #lambda = NULL,
  #penalty = "LASSO", #"Ridge", "ENet", "aLASSO", "SCAD", "MCP"
  #a = NULL,
  #nlambda = 100,
  #eps = ifelse(nrow(beton) < ncol(beton[,9]), 0.05, 0.01),
  #penalty.factor = rep(1, ncol(beton[,-9])),
  #alg = "huber", #"br", "QICD", "fn"
  #scalex = TRUE,
  #tau.penalty.factor = rep(1, length(0.5)),
  #coef.cutoff = 1e-08,
  #max.iter = 10000,
  #converge.eps = 1e-07,
  #lambda.discard = TRUE,
  #weights = NULL)

predict(
  object=m1,
  newx=as.matrix(beton[1:25,-9]),
  tau = NULL,
  a = NULL,
  lambda = NULL,
  modelsIndex = NULL,
  lambdaIndex = NULL)
```



#  sdwd (95)

Yüksek boyutlu sınıflandırma için seyrek mesafe ağırlıklı ayrımcılığı (SDWD) formüle eder ve çözüm yolunu L1, elastik ağ ve uyarlanabilir elastik ağ cezalarıyla hesaplamak için çok hızlı bir algoritma uygular.
--SONUÇ SAÇMA--
```{r}
library(sdwd)
nobs=nrow(train_adlt);nvars=ncol(train_adlt[,-13]);dfmax=nvars + 1
m<-sdwd(x=train_adlt[,-13], y=ifelse(train_adlt[,13]==1,-1,1), nlambda=100,
  lambda.factor=ifelse(nobs < nvars, 0.01, 1e-04),
  lambda=NULL, lambda2=1, pf=rep(1, nvars),
  pf2=rep(1, nvars), exclude=NULL, dfmax=nvars + 1,
  pmax=min(dfmax * 1.2, nvars), standardize=TRUE,
  eps=1e-8, maxit=1e6, strong=TRUE)

p<-predict(object=m, newx=test_adlt[,-13], s=c(0.01, 0.005), type="class")
```



#  snn (96)

K-en yakın komşu sınıflandırıcısını, ağırlıklı en yakın komşu sınıflandırıcısını, torbalı en yakın komşu sınıflandırıcısını, optimal ağırlıklı en yakın komşu sınıflandırıcısını ve stabilize edilmiş en yakın komşu sınıflandırıcısını uygulayın ve bunlar için 5 kat çapraz doğrulama yoluyla model seçimi yapın. Bu paket aynı zamanda bir sınıflandırma prosedürünün sınıflandırma hatasını ve sınıflandırma istikrarsızlığını hesaplamaya yönelik işlevler de sağlar.
--ÇOK ESKİ ve ÇOOOOK UZUN SÜRÜYOR--
```{r}
library(snn)
##parametre öğren ve tahminde kullan
#cv.tune(train=as.matrix(train_adlt[1:500,]), numgrid = 20, classifier = "snn")#lambda
#cv.tune(train=as.matrix(train_adlt[1:500,]), numgrid = 20, classifier = "knn")#K
#cv.tune(train=as.matrix(train_adlt[1:500,]), numgrid = 20, classifier = "bnn")#ratio
#cv.tune(train=as.matrix(train_adlt[1:500,]), numgrid = 20, classifier = "ownn")#K

p1<-mybnn(train=train_adlt[1:1000,], test=as.matrix(test_adlt[1:500,-13]),
          ratio=0.07)
p2<-myknn(train=train_adlt[1:1000,], test=as.matrix(test_adlt[1:500,-13]),
          K=28)
p3<-myownn(train=train_adlt[1:1000,], test=as.matrix(test_adlt[1:500,-13]),
           K=28)
p4<-mysnn(train=train_adlt[1:1000,], test=as.matrix(test_adlt[1:500,-13]),
          lambda=3.6)
#p5<-mywnn(train=train_adlt[1:1000,], test=as.matrix(test_adlt[1:500,-13]),
#          weight=rep(1,1000)) HATA VERİYOR

Metrics::accuracy(actual=test_adlt[1:500,13], predicted=p1)#clasification
Metrics::accuracy(actual=test_adlt[1:500,13], predicted=p2)
Metrics::accuracy(actual=test_adlt[1:500,13], predicted=p3)
Metrics::accuracy(actual=test_adlt[1:500,13], predicted=p4)

#Metrics::mae(actual, predicted)#regression
```
0.79
0.77
0.79
0.79

#  sparsediscrim (97)

Küçük örnekli, yüksek boyutlu veri kümelerine yönelik seyrek ve düzenli diskriminant analizi yöntemlerinden oluşan bir koleksiyon.
--SONUÇLAR OLDUKÇA TUHAF--
```{r}
library(sparsediscrim)
#Diyagonal Doğrusal Diskriminant Analizi (DLDA)
m1<-lda_diag(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL)

#Moore-Penrose Ters (MDMP) sınıflandırıcısını kullanan Minimum Uzaklık Kuralı
m2<-lda_eigen(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL,
          eigen_pct = 0.95)

#Minimum Mesafe Ampirik Bayes Tahmincisi (MDEB) sınıflandırıcısı
m3<-lda_emp_bayes(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL)

#Değiştirilmiş Ampirik Bayes (MDMEB) sınıflandırıcısını kullanan Minimum Uzaklık Kuralı
m4<-lda_emp_bayes_eigen(x=train_adlt[,-13], y=factor(train_adlt[,13]),
                        prior = NULL, eigen_pct = 0.95)

#Moore-Penrose PseudoInverse ile Doğrusal Diskriminant Analizi (LDA)
m5<-lda_pseudo(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL,
           tol = 1e-08)

#Schafer-Strimmer Kovaryans Matrisi Tahmincisi ile Doğrusal Diskriminant Analizi
m6<-lda_schafer(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL)

#Büzülmeye Dayalı Diyagonal Doğrusal Diskriminant Analizi (SDLDA)
#lda_shrink_cov(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL,
#               num_alphas = 101)

#Büzülme ortalamasına dayalı Diyagonal Doğrusal Diskriminant Analizi (SmDLDA)
m7<-lda_shrink_mean(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL)

#Thomaz-Kitani-Gillies Kovaryans Matrisi Tahmini Kullanılarak Doğrusal Diskriminant Analizi
m8<-lda_thomaz(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL)

#Diyagonal Kuadratik Diskriminant Analizi (DQDA)
m9<-qda_diag(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL)

#Büzülmeye Dayalı Diyagonal Kuadratik Diskriminant Analizi (SDQDA)
#qda_shrink_cov(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior = NULL,
#               num_alphas = 101)

#Büzülme-ortalama tabanlı Diyagonal Kuadratik Diskriminant Analizi (SmDQDA)
m10<-qda_shrink_mean(x=train_adlt[,-13], y=factor(train_adlt[,13]), prior =NULL)

p1<-predict(object= m1, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p1)

p2<-predict(object= m2, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p2)

p3<-predict(object= m3, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p3)

p4<-predict(object= m4, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p4)

p5<-predict(object= m5, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p5)

p6<-predict(object= m6, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p6)

p7<-predict(object= m7, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p7)

p8<-predict(object= m8, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p8)

p9<-predict(object= m9, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p9)

p10<-predict(object= m10, newdata = test_adlt[,-13], type = "class") #"prob","score"
Metrics::accuracy(actual = test_adlt[,13], predicted=p10)
```
.66
.24
.24
.24
.67
.67
.66
.24
.81
.81









## YARDIMCI VEYA ÇOK SIKINTILI PAKETLER

#impute????
```{r}
adult<-read.csv('adult.csv', sep = ',', fill = F, strip.white = T,
        col.names= c('age', 'workclass','fnlwgt', 'educatoin', 'educatoin_num',
                   'marital_status', 'occupation', 'relationship', 'race',
                   'sex', 'capital_gain', 'capital_loss', 'hours_per_week',
                   'native_country', 'income'),
                stringsAsFactors = T)

##convert ? to NA and renew levels
#method1
sapply(sapply(adult, grep, pattern = "\\?"),length)# nerede kaç tane ? var
# ? faktör içinde olduğu için bu şekilde değiştir
levels(adult$workclass)[levels(adult$workclass)=="?"] <- NA
levels(adult$occupation)[levels(adult$occupation)=="?"] <- NA
levels(adult$native_country)[levels(adult$native_country)=="?"] <- NA

#method2
i<-c(2,4,6:10,14,15) #categorical columns
a=adult
a[ , i] <- apply(a[ , i], 2, function(x) {
             ifelse(as.character(x) == "?",
             NA,as.character(x))})#convert ? to na
a[,i]<-lapply(a[,i],as.factor)

#setdiff(a,adult) #birbirinden fark varmı

###### IMPUTE compare #####

##method 1 impute factors >> PROFFECIONAL.........en yüksek tahmin doğruluğu
adlt1<-missForest::missForest(adult, maxiter = 3, ntree = 100)$ximp

##method 2 e1071 >>BASIC
adlt2<-adult
i<-c(2,4,6:10,14:15) #categorical columns to numeric value for e1071
adlt2[ , i] <- apply(adlt2[ , i], 2, function(x) as.numeric(factor(x)))
adlt2 <- e1071::impute(adlt2, what = "median")
adlt2<-data.frame(adlt2)

## method 3 >> PROFESSIONAL
adlt3<-randomForestSRC::impute(data=adult,ntree = 100, nodesize = 3,
                               nsplit = 10, nimpute = 2)

##method 4 randomForest >>BASIC/ PROFESSIONAL
adlt4a<-randomForest::na.roughfix(adult)#Impute Missing Values by median/mode.
adlt4b<-randomForest::rfImpute(income~.,adult)#impute missing value forest method.

#compare impute
dim(adult[!complete.cases(adult),])
#c(2,7,14)#"workclass","occupation","native_country"
m1<-adlt1[which(!complete.cases(adult)),c(2,7,14)]#.869 accuracy
m2<-adlt2[which(!complete.cases(adult)),c(2,7,14)]#.860
m3<-adlt3[which(!complete.cases(adult)),c(2,7,14)]#.863
m4a<-adlt4a[which(!complete.cases(adult)),c(2,7,14)]#.864
m4b<-adlt4b[which(!complete.cases(adult)),c(3,8,15)]#.865

w<-cbind(m1$workclass,m2$workclass,m3$workclass,m4a$workclass,m4b$workclass)
o<-cbind(m1$occupation,m2$occupation,m3$occupation,m4a$occupation,
         m4b$occupation)
n<-cbind(m1$native_country,m2$native_country,m3$native_country,
         m4a$native_country,m4b$native_country)

table(n[,1],n[,2])
table(n[,1],n[,3])
table(n[,1],n[,4])
table(n[,1],n[,5])
table(n[,2],n[,3])#simetrik
table(n[,2],n[,4])#simetrik
table(n[,2],n[,5])
table(n[,3],n[,4])#
table(n[,3],n[,5])
table(n[,4],n[,5])
```



# glmertree*98

(Genelleştirilmiş) doğrusal karma modellere dayalı özyinelemeli
bölümleme (GLMM'ler) 'lme4'ten lmer()/glmer() ve 'partykit'ten
lmtree()/glmtree()'yi birleştiriyor. ------BAŞKA PAKETLERDEN DERLEME
OLDUĞU İÇİN ÇOK PROBLEMLİ, ÇALIŞMIYOR

```{}
library('glmertree')
#Karışık efektli beta regresyona dayalı model tabanlı özyinelemeli bölümleme.
betamertree(formula=csMPa~cement|water+slag+flyash+age, data=beton,
    family = binomial(link = "logit"), weights = NULL, cluster = NULL,
    ranefstart = NULL, offset = NULL, REML = TRUE, joint = TRUE,
    abstol = 0.001, maxit = 100, dfsplit = TRUE, verbose = FALSE,
    plot = FALSE, glmmTMB.control = glmmTMB::glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

#(Genelleştirilmiş) doğrusal karma modellere dayalı model tabanlı özyinelemeli bölümleme.1
lmertree(formula=csMPa~cement|water+slag+flyash+age, data=beton, weights = NULL,
         cluster = NULL, ranefstart = NULL, offset = NULL, joint = TRUE,
         abstol = 0.001, maxit = 100, dfsplit = TRUE, verbose = FALSE,
         plot = FALSE, REML = TRUE, lmer.control = lmerControl())

#(Genelleştirilmiş) doğrusal karma modellere dayalı model tabanlı özyinelemeli bölümleme.2
glmertree(formula=csMPa~cement|water+slag+flyash+age, data=beton,
          family = "binomial", weights = NULL, cluster = NULL,ranefstart = NULL,
          offset = NULL, joint = TRUE, abstol = 0.001, maxit = 100,
          dfsplit = TRUE, verbose = FALSE, plot = FALSE, nAGQ = 1L,
          glmer.control = glmerControl())

```

FAMILY: binomial(link = "logit") gaussian(link = "identity") Gamma(link
= "inverse") inverse.gaussian(link = "1/mu\^2") poisson(link = "log")
quasi(link = "identity", variance = "constant") quasibinomial(link =
"logit") quasipoisson(link = "log")

# mpath*99

Uygulamalar, sağlam (cezalandırılmış) genelleştirilmiş doğrusal
modelleri ve sağlam destek vektör makinelerini içerir. -----BİLGİSAYARI
KİLİTLİYOR ÇALIŞMIYOR-----

```{}
library('mpath')
adlt[,14]<-adlt[,14]-1
#glmreg :fit a GLM with lasso (or elastic net), snet or mnet regularization
glmreg(formula=income~., data=data.frame(adlt), family="binomial", weights=NULL,
       offset=NULL, contrasts=NULL, x.keep=FALSE, y.keep=TRUE)

#glmregNB :fit a negative binomial model with lasso (or elastic net), snet and mnet regularization
#irglm :fit a robust generalized linear models
#irglmreg :Fit a robust penalized generalized linear models
#irsvm :fit case weighted support vector machines with robust loss functions
#loss2 :Composite Loss Value
#loss3 :Composite Loss Value for GLM
#ncl :fit a nonconvex loss based robust linear model
#zipath :Fit zero-inflated count data linear model with lasso (or elastic net),snet or mnet regularization

predict()
```

# RPMM*100

Beta ve Gauss Karışımları için Yinelemeli Bölümlenmiş Karışım Modeli.
Bu, hiyerarşik kümelemeye benzer ancak aynı zamanda sonlu karışım
modellerine benzer şekilde sınıfların hiyerarşisini döndüren model
tabanlı bir kümeleme algoritmasıdır. ---SORUNLU----

```{}
library('RPMM')

#Özyinelemeli olarak bölümlenmiş karışım modelini kullanarak beta gizli sınıf modellemesi gerçekleştirir
mdl1 <- blcTree(x = scale(life[,-1]),
                initFunctions = list(blcInitializeSplitFanny()), weight = 10,
                index = 1:167, wthresh = 1e-08, nodename = "root",
                maxlevel = Inf, verbose = 2, nthresh = 5, level = 0, env = NULL,
                unsplit = NULL, splitCriterion = blcSplitCriterionBIC)

#Özyinelemeli olarak bölümlenmiş karışım modelini kullanarak Gauss gizli sınıf modellemesini gerçekleştirir
mdl2<-glcTree(x=life[,-1], initFunctions = list(glcInitializeSplitFanny(nu=1.5)),
        weight = NULL,index = NULL, wthresh = 1e-08,nodename = "root",
        maxlevel = Inf, verbose = 2, nthresh = 5, level = 0,env = NULL,
        unsplit = NULL, splitCriterion = glcSplitCriterionBIC)

predict(object=mdl1, newdata=life[1:5,-1], nodelist=NULL, type="weight")
predict(object=mdl2, newdata=life[1:5,-1], nodelist=NULL, type="weight")
```

# islasso*101

Model katsayıları üzerinde tahmin ve çıkarım yapılmasına olanak sağlamak
için kement düzenleme modellerine yönelik uyarılmış yumuşatma (IS)
fikrinin uygulanması (şu anda yalnızca hipotez testi). Çeşitli bağlantı
fonksiyonlarıyla doğrusal, lojistik, Poisson ve gama regresyonları
uygulanır.

----ÇALIŞMIYOR-----

```{}
library('islasso')
z<-cv.glmnet(x=makeX(data.frame(train_adlt[,-14])),y=train_adlt[,14],
             alignment = "lambda")

mdl<-islasso(formula=income~., data = data.frame(train_adlt),
             family = gaussian(), lambda=mean(z$lambda), alpha = 1,
             weights=NULL, subset=NULL, offset=NULL, unpenalized=NULL,
             contrasts = NULL, control = is.control())

predict(object=mdl, newdata = data.frame(test_adlt[,-14]),
        type = "response",#c("link", "coefficients", "class", "terms"),
        se.fit = FALSE, ci = NULL, type.ci = "wald", level = .95,
        terms = NULL, na.action = na.pass)
```

# Rborist*102

Breiman tarafından tanımlandığı gibi sınıflandırma ve regresyon
ormanlarının ölçeklenebilir uygulaması ----HATA VERİYOR----

```{}
library('Rborist')
df <- preformat(train_adlt[,-13])
target<-factor(train_adlt[,13])
# eski komut Rborist(x, y, ...)  Rborist(train_adlt[,-13],train_adlt[,13])

#rfArb:Rastgele Orman algoritmasının hızlandırılmış uygulaması. Çok çekirdekli ve GPU donanımı için ayarlandı.
mdl<-rfArb(x= df, y=target,autoCompress = 0.25,
      ctgCensus = "votes",classWeight = "balance",impPermute = 0,
      indexing = FALSE,maxLeaf = 0,minInfo = 0.01,
      minNode = 10,#if (is.factor(target)) 2 else 3,
      nLevel = 20,nSamp = 0,nThread = 0,nTree = 500,noValidate = FALSE,
      predFixed = 2,predProb = 0.0,predWeight = NULL,quantVec = NULL,
      quantiles = !is.null(NULL),#quantVec),
      regMono = NULL,rowWeight = NULL,splitQuant = NULL,
      thinLeaves = is.factor(target) && !FALSE,#indexing,
      trapUnobserved = FALSE,treeBlock = 1,verbose = FALSE,withRepl = TRUE)

p<-predict(object=mdl, newdata=test_adlt[,-13], yTest=NULL,
        keyedFrame = FALSE, quantVec=seq(0.1, 1.0, by = 0.10), quantiles = !is.null(NULL),
        ctgCensus = "votes", indexing = FALSE, trapUnobserved = FALSE,
        bagging = FALSE, nThread = 0, verbose = FALSE)
table(p$yPred, test_adlt$income)
```

# BDgraph*103

Sürekli, sıralı/ayrık/sayılı ve karma veriler için yönlendirilmemiş
grafik modellerde Bayes yapısının öğrenimine yönelik istatistiksel
araçlar. ---hata veriyor---

```{}
library('BDgraph')
bdgraph( data=adlt, n = 10, method = "gcgm", #"ggm"
         algorithm = "bdmcmc", #rjmcmc
         iter = 5000, burnin = 5000 / 2, not.cont = NULL, g.prior = 0.2,
         df.prior = 3, g.start = "empty", jump = NULL, save = FALSE,
         cores = NULL, threshold = 1e-8, verbose = TRUE, nu = 1 )

# bdgraph.dw :Search algorithm for Gaussian copula graphical models for count data
# bdgraph.mpl :Search algorithm in graphical models using marginal pseudo likehlihood
# bdgraph.npn :Nonparametric transfer
# bdw.reg :Bayesian estimation of (zero-inflated) Discrete Weibull regression
```

# joinet*104

R paket birleşimi, yığılmış genellemeyi kullanarak çok değişkenli sırt
ve kement regresyonunu uygular. Bu çok değişkenli regresyon, ilişkili
sonuçları tahmin etmede tipik olarak tek değişkenli regresyondan daha
iyi performans gösterir. Yüksek boyutlu ortamlarda tahmine dayalı ve
yorumlanabilir modeller sağlar. ---GLMNET ALT KOPYASI, HATA VERİYOR----

```{}
library('joinet')
#Multivariate Elastic Net Regression
joinet(Y=beton[,9],X=beton[,-9],
       family = "gaussian",nfolds = 8,foldid = NULL, type.measure = "mse",
       alpha.base = 1,alpha.meta = 1,weight = NULL, sign = NULL,)
predict(object, newx, type = "response")
```

# plsdof*105

PLS için model seçimi çeşitli bilgi kriterlerine (aic, bic, gmdl) veya
çapraz doğrulamaya dayanmaktadır. PLS regresyon katsayılarının
ortalaması ve kovaryansına ilişkin tahminler mevcuttur. Yaklaşık güven
aralıklarının oluşturulmasına ve test prosedürlerinin uygulanmasına izin
verirler. Ayrıca Ridge Regresyonu ve Temel Bileşenler Regresyonuna
yönelik çapraz doğrulama prosedürleri mevcut. ----TAHMİN UYGULAMASI
YOK----

```{}
library(plsdof)
#Bu fonksiyon Kısmi En Küçük Kareler uyumunu hesaplar. Bu algoritma esas olarak gözlem sayısına göre ölçeklenir
kernel.pls.fit(X=as.matrix(beton[,-9]),y=beton[,9],m = ncol(beton[,-9]),
               compute.jacobian = FALSE, DoF.max = min(ncol(beton[,-9]) + 1,
                                                      nrow(beton[,-9]) - 1)
               )

#Bu fonksiyon Kısmi En Küçük Kareler çözümünü ve regresyon katsayılarının birinci türevini hesaplar. Bu uygulama çoğunlukla değişken sayısına göre ölçeklenir
linear.pls.fit(X=as.matrix(beton[,-9]),y=beton[,9],m = ncol(beton[,-9]),
               compute.jacobian = FALSE, DoF.max = min(ncol(beton[,-9]) + 1,
                                                      nrow(beton[,-9]) - 1)
               )

#Bu fonksiyon Temel Bileşenler Regresyon (PCR) uyumunu hesaplar.
pcr(X=as.matrix(beton[,-9]),y=beton[,9], scale = TRUE,
    m = min(ncol(beton[,-9]), nrow(beton[,-9]) - 1),
    eps = 1e-06, supervised = FALSE)

#Bu fonksiyon Kısmi En Küçük Kareler uyumunu hesaplar.
pls.model(X=as.matrix(beton[,-9]), y=beton[,9], m = ncol(beton[,-9]),
             Xtest = NULL, ytest = NULL, compute.DoF = FALSE,
             compute.jacobian = FALSE,use.kernel = FALSE,method.cor = "pearson")

#Bu işlev, çapraz doğrulamaya dayalı olarak optimal sırt regresyon modelini hesaplar.
ridge.cv(X=as.matrix(beton[,-9]),y=beton[,9], lambda = NULL, scale = TRUE,
         k = 10, plot.it = FALSE, groups = NULL, method.cor = "pearson",
         compute.jackknife = TRUE)
```
son
