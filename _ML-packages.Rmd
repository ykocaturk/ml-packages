---
title: "ML Packages"
output:
  pdf_document: default
  html_document: default
date: "2023-12-05"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Makine öğrenimi alanındaki kütüphaneler irdelenmiştir.

İçinde barındırdığı fonksiyonu en fazla olana göre sıralanmıştır.

Veri 1: Beton Basınç Dayanımı Veri Seti(regresyon, randomforest,
decision tree) Beton basınç dayanımı, yaş ve bileşenlerin oldukça
doğrusal olmayan bir fonksiyonudur. Örnek sayısı 1030, Öznitelik Sayısı
9. Öznitelik dökümü 8 nicel girdi değişkeni ve 1 nicel çıktı değişkeni.

İsim -- Veri Türü -- Ölçüm -- Açıklama

-   Çimento (bileşen 1) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişken Yüksek Fırın

-   Cürufu (bileşen 2) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişken

-   Uçucu Kül (bileşen 3) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişken

-   Su (bileşen 4) -- kantitatif -- m3 karışımında kg -- Giriş Değişkeni

-   Süperakışkanlaştırıcı (bileşen 5) -- kantitatif -- m3 karışımında kg
    -- Giriş Değişkeni

-   Kaba Agrega (bileşen 6) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişkeni

-   İnce Agrega (bileşen 7) -- kantitatif -- m3 karışımında kg -- Giriş
    Değişkeni

-   Yaş -- kantitatif -- Gün (1\~365) -- Giriş Değişkeni

-   Beton basınç dayanımı -- kantitatif -- MPa -- Çıkış Değişkeni

    <https://www.kaggle.com/datasets/maajdl/yeh-concret-data/download?datasetVersionNumber=1>

Veri 2: Yetişkin bireylerin gelir düzeylerine yönelik anket bilgilerini
içerir. arules paketi AdultUCI veri seti, 48842 ile bir veri çerçevesi
içerir. Aşağıdaki 15 değişken üzerinde gözlemler.

-   age : yaş - sayısal bir vektör.

-   workclass : işçi sınıfı - düzeyleri olan bir faktör

-   education : eğitim - düzeyleri olan sıralı bir faktör

-   education-num : eğitim-num - Sayısal bir vektör.

-   marital-status : medeni durum - ile bir faktör düzeyleri

-   occupation :meslek - düzeyleri olan bir faktör

-   relationship : ilişki - düzeyleri olan bir faktör

-   race : ırk - bir faktör düzeyleri ile

-   sex : cinsiyet - düzeyleri olan bir faktör Female/Male

-   capital-gain : sermaye kazancı - Sayısal bir vektör.

-   capital-loss : sermaye kaybı - Sayısal bir vektör.

-   fnlwgt : Sayısal bir vektör.

-   hours-per-week : haftalık saat - Sayısal bir vektör.

-   native-country : anavatanı

-   income : gelir - sıralı bir faktör seviyeleri. small\<large

```         
#read.table('<https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data>'
```

Vei 3: Bu veride 167 ülkenin 9 farklı sosyo ekonomik bilgilerine
yerverilmiştir.

country : Ülkenin adı

child_mort : 1000 canlı doğumda 5 yaş altı çocuk ölümü

exports : Kişi başına mal ve hizmet ihracatı. Kişi başına düşen
GSYİH'nin yüzdesi olarak verilmiştir

health : Kişi başına düşen toplam sağlık harcaması. Kişi başına düşen
GSYİH'nin yüzdesi olarak verilmiştir

imports : Kişi başına mal ve hizmet ithalatı. Kişi başına düşen
GSYİH'nin yüzdesi olarak verilmiştir

Income : Kişi başına net gelir

Inflation : Toplam GSYİH'nın yıllık büyüme oranının ölçümü

life_expec : Mevcut ölüm oranlarının aynı kalması durumunda yeni doğan
bir çocuğun yaşayacağı ortalama yıl sayısı

total_fer : Mevcut yaş-doğurganlık oranları aynı kalırsa her kadının
doğacak çocuk sayısı gdpp : Kişi başına düşen GSYİH. Toplam GSYİH'nın
toplam nüfusa bölünmesiyle hesaplanır.

\#<https://rpubs.com/adellop/731487>

```{r, datasets}
beton<-read.csv(unzip("Concrete.zip","Concrete_Data_Yeh.csv"))##regression
adult<-read.csv('adult.csv', sep = ',', fill = F, strip.white = T,
                col.names= c('age', 'workclass','fnlwgt', 'educatoin',
                             'educatoin_num', 'marital_status', 'occupation',
                             'relationship', 'race', 'sex', 'capital_gain',
                             'capital_loss', 'hours_per_week', 'native_country',
                             'income'),
                stringsAsFactors = T
                )##classification
life <- read.csv(unzip("archive.zip","Country-data.csv")) ##clustiring

lung<-survival::lung#data(cancer, package = 'survival')
lung<-na.omit(lung)

#-----data manipulation-----
sapply(adult,function(x)levels(x))
adult<-adult[,-c(3,4)]#clear id column
adult <- replace(adult,adult=="?",NA)#replace ? to NA
adult <- droplevels(adult)#update levels
#impute NA cells
##method 1 impute factors >> missForest::missForest(adult)
adlt<-adult
i<-c(2,4:8,12:13) #categorical columns to numeric value for e1071
adlt[ , i] <- apply(adlt[ , i], 2, function(x) as.numeric(factor(x)))
adlt <- e1071::impute(adlt, what = "median")
adlt<-data.frame(adlt)

indx<-sample(nrow(adlt), round(0.7*nrow(adlt)))
train_adlt<-adlt[indx, ]
test_adlt<-adlt[-indx, ]
rm(i,indx)
#i<-c(2,4,6:10,14,15) #categorical
#a=adult
#a[ , i] <- apply(a[ , i], 2, function(x) {
#             ifelse(as.character(x) == "?",
#             NA,as.character(x))})#convert ? to na
#a<-na.omit(a)#na delete
```

kontrol et; formula(model.frame(income\~.,data=train_adlt))

# 1. mlpack

C++ ile yazılmış, en ileri makine öğrenimi algoritmalarının hızlı,
genişletilebilir uygulamalarını sağlamayı amaçlayan hızlı, esnek bir
makine öğrenimi kitaplığı. 33 methods

\#<https://cran.r-project.org/web/packages/mlpack/mlpack.pdf>
\#<https://www.mlpack.org/doc/stable/r_documentation.html#gmm_train>

```{r}
library('mlpack')

#Verileri eğitim ve test veri kümesine etiketiyle birlikte bölme
adlt1 <- preprocess_split(adlt[,-13], input_labels =  data.frame((adlt$income)),
  no_shuffle = FALSE, seed = NA, test_ratio = .25, stratify_data = FALSE,
  verbose = FALSE)

#Veriyi ölçeklendirme ve bölme
btn<-data.frame(scale(beton,F,F))
btn <- preprocess_split(btn[,-9], input_labels = data.frame(btn$csMPa),
                        test_ratio = .25)

#sayısal verileri al
life <- life[,-1]
```

list2env(btn, .GlobalEnv) : veri kolonlarını ortam değişkeni olarak atar

```{r}
#classification************************sınıflandırma
#etiketli veriler üzerinde bir AdaBoost modelini eğitmek veya yeni noktaların sınıflarını tahmin etmek için mevcut bir AdaBoost modelini kullanmak için kullanılabilir
c1 <- adaboost(input_model = NA, iterations = 1000,
               labels = adlt1$training_labels, test = NA , tolerance = 1e-10,
               training = adlt1$training, verbose = T,
               weak_learner = "decision_stump")
prd<- adaboost(input_model = c1$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd); Metrics::auc(adlt1$test_labels, prd)

#Sayısal veya kategorik özelliklere sahip etiketli veriler verildiğinde, bir karar ağacı eğitilebilir ve kaydedilebilir; veya yeni noktalara göre sınıflandırma yapmak için mevcut bir karar ağacı kullanılabilir.
c2 <- decision_tree(input_model = NA, labels = adlt1$training_labels,
                    maximum_depth = 0, minimum_gain_split = 1e-07,
                    minimum_leaf_size = 20, print_training_accuracy = FALSE,
                    print_training_error = FALSE, test = NA, test_labels = NA,
                    training = adlt1$training, verbose = FALSE, weights = NA)
prd<- decision_tree(input_model = c2$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd); Metrics::auc(adlt1$test_labels, prd)

#Çok sınıflı sınıflandırma için doğrusal SVM'nin bir uygulaması. Etiketli veriler verildiğinde, bir model gelecekte kullanılmak üzere eğitilip kaydedilebilir; veya yeni noktaları sınıflandırmak için önceden eğitilmiş bir model kullanılabilir.
c3 <-linear_svm(delta = 1, epochs = 50, input_model = NA,
                labels = adlt1$training_labels, lambda = 0.0001,
                max_iterations = 10000, no_intercept = FALSE, num_classes = 0,
                optimizer = 'lbfgs', seed = 0, shuffle = FALSE,step_size = 0.01,
                test = NA, test_labels = NA, tolerance = 1e-10,
                training = adlt1$training, verbose = FALSE)
prd<- linear_svm(input_model = c3$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd)
Metrics::auc(adlt1$test_labels, prd)

#İki sınıflı sınıflandırma için L2-düzenlenmiş lojistik regresyonun bir uygulaması. Etiketli veriler verildiğinde, bir model gelecekte kullanılmak üzere eğitilip kaydedilebilir; veya yeni noktaları sınıflandırmak için önceden eğitilmiş bir model kullanılabilir.
c4<-logistic_regression(batch_size = 64, decision_boundary = 0.5,
                        input_model = NA, labels = adlt1$training_labels,
                        lambda = 0, max_iterations = 10000, optimizer = 'lbfgs',
                        step_size = 0.01, test = NA, tolerance = 1e-10,
                        training = adlt1$training, verbose = FALSE)
prd <- logistic_regression(input_model = c4$output_model,
                           test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd); Metrics::auc(adlt1$test_labels, prd)

#Sınıflandırma için kullanılan Naive Bayes Sınıflandırıcının bir uygulaması. Etiketli veriler verildiğinde, bir NBC modeli eğitilip kaydedilebilir veya sınıflandırma için önceden eğitilmiş bir model kullanılabilir.
c5 <- nbc(incremental_variance = FALSE, input_model = NA, verbose = FALSE,
        labels = adlt1$training_labels, test = NA, training = adlt1$training)
prd <- nbc(input_model = c5$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd); Metrics::auc(adlt1$test_labels, prd)

#Sınıflandırma için bir algılayıcının (tek seviyeli bir sinir ağı) uygulanması. Etiketli veriler verildiğinde, bir algılayıcı gelecekte kullanılmak üzere eğitilip kaydedilebilir; veya yeni noktalara göre sınıflandırma yapmak için önceden eğitilmiş bir algılayıcı kullanılabilir.
c6<-perceptron(input_model = NA, labels = adlt1$training_labels,verbose = FALSE,
               max_iterations = 1000, test = NA, training = adlt1$training)
prd<-perceptron(input_model = c6$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd); Metrics::auc(adlt1$test_labels, prd)

#Leo Breiman'ın sınıflandırma için standart rastgele orman algoritmasının bir uygulaması. Etiketli veriler verildiğinde, rastgele bir orman eğitilip gelecekte kullanılmak üzere kaydedilebilir; veya sınıflandırma için önceden eğitilmiş bir rastgele orman kullanılabilir.
c7<-random_forest(input_model = NA, labels = adlt1$training_labels,
                  maximum_depth = 0, minimum_gain_split = 0,
                  minimum_leaf_size = 1, num_trees = 10,
                  print_training_accuracy = FALSE, seed = 2024,
                  subspace_dim = 0, test = NA, test_labels = NA,
                  training = adlt1$training,verbose = FALSE, warm_start = FALSE)
prd<-random_forest(input_model = c7$output_model, test = adlt1$test)$predictions
Metrics::accuracy(adlt1$test_labels, prd); Metrics::auc(adlt1$test_labels, prd)

#Lojistik regresyonun çok sınıflı bir genellemesi olan sınıflandırma için softmax regresyonunun bir uygulaması
c8<-softmax_regression(input_model = NA, labels = adlt1$training_labels,
                       lambda = 0.0001,max_iterations = 10,no_intercept = FALSE,
                       number_of_classes = 0, test = NA, test_labels = NA,
                       training = scale(adlt1$training,center = F),
                       verbose = FALSE)

prd<- softmax_regression(input_model = c8$output_model,
                        test = scale(adlt1$test,center = F))$predictions
Metrics::accuracy(adlt1$test_labels, prd);Metrics::auc(adlt1$test_labels, prd)


#REGRESSION*************************regresyon
#Bayes doğrusal regresyonunun bir uygulaması
r1 <- bayesian_linear_regression(center = FALSE,input = btn$training,
                           input_model = NA, responses = btn$training_labels,
                           scale = FALSE, test = btn$test, verbose = FALSE)
prd<- bayesian_linear_regression(input_model = r1$output_model, test = btn$test)
Metrics::mae(btn$test_labels, prd$predictions)
Metrics::mse(btn$test_labels, prd$predictions)


#LARS olarak da bilinen En Küçük Açı Regresyonunun (Aşamalı/laSso) bir uygulaması. Bu, bir LARS/LASSO/Elastic Net modelini eğitebilir ve bir test seti için regresyon tahminlerinin çıktısını almak üzere bu modeli veya önceden eğitilmiş bir modeli kullanabilir.
r2<-lars(input = btn$training, input_model = NA, lambda1 = 0, lambda2 = 0,
         no_intercept = FALSE,no_normalize =FALSE,responses=btn$training_labels,
         test = NA, use_cholesky = FALSE, verbose = FALSE)
prd<-lars(input_model = r2$output_model, test = btn$test)
Metrics::mae(btn$test_labels, prd$output_predictions[,1])
Metrics::mse(btn$test_labels, prd$output_predictions[,1])

#Sıradan en küçük kareler kullanılarak basit doğrusal regresyon ve sırt regresyonunun bir uygulaması. önceden eğitilmiş bir model, bir test seti için regresyon tahminlerinin çıktısını almak üzere kullanılabilir
r3<-linear_regression(input_model = NA, lambda = 0, test = NA,
                      training = btn$training,
                      training_responses = btn$training_labels,
                      verbose = FALSE)
prd<-linear_regression(input_model = r3$output_model,
                       test = btn$test)
Metrics::mae(btn$test_labels, prd$output_predictions[,1])
Metrics::mse(btn$test_labels, prd$output_predictions[,1])



##clustering****************kümeleme
#Bir veri kümesi verildiğinde, bu, söz konusu veri kümesinin kümelenmesini hesaplayabilir ve döndürebilir.
clu1 <- dbscan(input=life, epsilon = 1, min_size = 5, naive = TRUE,
             selection_type = 'ordered', single_mode = TRUE,
             tree_type = "kd", verbose = FALSE)

#Gauss karışım modellerinin (GMM'ler) eğitimi için EM algoritmasının bir uygulaması. Bir veri kümesi göz önüne alındığında, bu, GMM'yi gelecekte diğer araçlarla kullanmak üzere eğitebilir
clu2<-gmm_train(gaussians=10, input=life, diagonal_covariance = FALSE,
                input_model = NA, kmeans_max_iterations = 1000,
                max_iterations = 250, no_force_positive = FALSE, noise = 5,
                percentage = 0.02, refined_start = TRUE, samplings = 100,
                seed = 0, tolerance = 1e-10, trials = 1, verbose = FALSE)

#Bir veri kümesi ve bir k değeri verildiğinde, bu, söz konusu veriler üzerinde bir k-ortalama kümelemesini hesaplar ve döndürür.
clu3<-kmeans(clusters=5, input=life, algorithm = 'naive',
             allow_empty_clusters = FALSE, in_place = FALSE,
             initial_centroids = NA, kill_empty_clusters = FALSE,
             kmeans_plus_plus = FALSE, labels_only = FALSE,
             max_iterations = 1000, percentage = 0.02, refined_start = FALSE,
             samplings = 10, seed = 0, verbose = FALSE)

#İkili ağaç aralık aramasını kullanarak ortalama kaydırmalı kümelemenin hızlı bir uygulaması
clu4<-mean_shift(input=life, force_convergence = FALSE, in_place = FALSE,
                 labels_only = FALSE, max_iterations = 1000, radius = 0,
                 verbose = FALSE)

#Önceden eğitilmiş GMM'ler için örnek oluşturucu
clu5<-gmm_generate(input_model=clu2$output_model, samples=9, seed = 0,
                   verbose = FALSE)

#GMM'ler için bir olasılık hesaplayıcısı. Önceden eğitilmiş bir GMM ve bir dizi nokta göz önüne alındığında, bu, her bir noktanın verilen GMM'den olma olasılığını hesaplayabilir.
clu6<-gmm_probability(input=clu5$output, input_model=clu2$output_model,
                      verbose = FALSE)

#Sınıflandırmaya yönelik bir akış karar ağacı biçimi olan Hoeffding ağaçlarının bir uygulaması.
clu7<-hoeffding_tree(batch_mode = FALSE, bins = 10, confidence = .95,
                     info_gain = FALSE, input_model = NA, labels = NA,
                     max_samples = 150, min_samples = 10,
                     numeric_split_strategy = 'domingos',
                     observations_before_binning = 50, passes = 1, test = NA,
                     test_labels = NA, training = life, verbose = FALSE)



##AĞAÇ*********************
#Yaklaşık en uzak komşu araması -UYGUN VERİ YOK-
#ag1<-approx_kfn(algorithm = "ds", calculate_error = FALSE, exact_distances = NA,
#           input_model = NA, k = 0, num_projections = 5, num_tables = 5,
#           query = NA, reference = NA, verbose = FALSE)

#Hızlı Öklid Minimum Yayılan Ağaç
ag2<-emst(input=beton, leaf_size = 1, naive = FALSE, verbose = FALSE)

#Tek ağaçlı ve çift ağaçlı hızlı maksimum çekirdek arama (FastMKS) algoritmasının bir uygulaması.
ag3<-fastmks(bandwidth = 1,base = 2,degree = 2,input_model = NA,k = 10,
        kernel = "linear",naive = FALSE,offset = 0,query = NA,
        reference = btn$training, scale = 1, single = FALSE,verbose = FALSE)

#Yerelliğe duyarlı karma (LSH) ile yaklaşık k-en yakın komşu aramasının bir uygulaması
ag4<-lsh(bucket_size = NA,hash_width = NA,input_model = NA,k = 10,
         num_probes = NA, projections = NA,query = NA,reference = btn$training,
         second_hash_size = NA, seed = NA,tables = NA,true_neighbors = NA,
         verbose = FALSE)

#Tek ağaç ve çift ağaç algoritmaları kullanan k-en yakın komşu aramasının bir uygulaması
ag5<-knn(algorithm = "dual-tree",epsilon = 0,input_model = NA,k = 10,
         leaf_size = 20,query = NA, random_basis = FALSE,
         reference = btn$training,rho = .7,seed = 0,tau = 0,tree_type = "kd",
         true_distances = NA,true_neighbors = NA,verbose = FALSE)

#Tek ağaçlı ve çift ağaçlı algoritmaları kullanan k-en uzak komşu aramasının bir uygulaması.
ag6<-kfn(algorithm = "dual_tree",epsilon = 0,input_model = NA,k = 10,
         leaf_size = 20, percentage = 1,query = NA,random_basis = FALSE,
         reference = btn$training,seed = 0,tree_type = "kd",true_distances = NA,
         true_neighbors = NA, verbose = FALSE)

#Tek ağaç ve çift ağaç algoritmalarını kullanan, yaklaşık k-en yakın komşu aramasının (kRANN) bir uygulaması.
ag7<-krann(alpha = .95,first_leaf_exact = FALSE,input_model = NA,k = 10,
      leaf_size = 20,naive = FALSE,query = NA,random_basis = FALSE,
      reference = btn$training,sample_at_leaves = FALSE,seed = 0,
      single_mode = FALSE,single_sample_limit = 20,tau = 5,
      tree_type = "kd",verbose = FALSE)



#transformation***********************dönüşüm/boyut azaltma
#Çekirdek Temel Bileşenler Analizi(KPCA), belirli bir veri kümesinde doğrusal olmayan boyutluluk azaltma veya ön işleme gerçekleştirmek için
tr1<-kernel_pca(input=cbind(matrix(unlist(adlt1$test), ncol = 12, byrow = TRUE),
                            matrix(unlist(adlt1$test_labels), ncol = 1,
                                   byrow = TRUE)),
                kernel='linear', bandwidth = 1, center = FALSE, degree = 1,
                kernel_scale = 1, new_dimensionality = 0,
                nystroem_method = FALSE, offset = 0, sampling = "kmeans",
                verbose = FALSE)
#kernel=gaussian,polynomial,hyptan,laplacian, epanechnikov,cosine

#Bir uzaktan öğrenme tekniği olan Büyük Marjlı En Yakın Komşuların (LMNN) bir uygulaması. Etiketli bir veri kümesi verildiğinde, bu, k-en yakın komşunun performansını artıran veri dönüşümünü öğrenir; bu bir ön işleme adımı olarak faydalı olabilir.
tr2<-lmnn(input=adlt1$training, batch_size = 50, center = FALSE,
          distance = matrix(integer(), 0, 0), k = 1,labels = adlt1$training_labels,
          linear_scan = FALSE, max_iterations = 10000, normalize = FALSE,
          optimizer = "amsgrad", passes = 50, print_accuracy = FALSE, range = 1,
          rank = 0, regularization = 0.5, seed = 0, step_size = 0.01,
          tolerance = 1e-07, verbose = FALSE)

#Ön işleme için kullanılabilecek bir uzaktan öğrenme tekniği olan komşuluk bileşenleri analizinin bir uygulaması.Etiketli bir veri kümesi göz önüne alındığında, bu, k-en yakın komşu sınıflandırmasını iyileştirmeyi amaçlar
tr3<-nca(input=adlt1$training, armijo_constant = 0.0001, batch_size = 50,
         labels = adlt1$training_labels, linear_scan = FALSE,
         max_iterations = 500000, max_line_search_trials = 50, max_step = 1e+20,
         min_step = 1e-20, normalize = FALSE, num_basis = 5, optimizer = "sgd",
         seed = 0, step_size = 0.01, tolerance = 1e-07, verbose = FALSE,
         wolfe = 0.9)

#Bir veri kümesi ve istenen yeni boyutluluğa göre PCA tarafından belirlenen doğrusal dönüşümü kullanarak verinin boyutluluğunu azaltabilir.
tr4<-pca(input=adlt1$training, decomposition_method = "exact",
         new_dimensionality = 10, scale = FALSE, var_to_retain = 1,
         verbose = FALSE)

#bağımsız bileşen analizi (ICA) için bir yöntem
tr5<-radical(input=adlt1$training, angles = 150, noise_std_dev = 0.175,
             objective = FALSE, replicates = 10, seed = 0, sweeps = 0,
             verbose = FALSE)

#Sözlük Öğrenimi ile Seyrek Kodlamanın bir uygulaması
tr6<-sparse_coding(atoms = 9,initial_dictionary = NA,input_model = NA,
                   lambda1 = 0, lambda2 = 0, max_iterations = 0,
                   newton_tolerance = 1e-06, normalize = FALSE,
                   objective_tolerance = 0.01,seed = 0,test = NA,
                   training = scale(life),verbose = FALSE)
```

SORUNLU FONKSİYONLAR

```         
# DİĞER***************************
#Tavsiye sistemleri için çeşitli işbirlikçi filtreleme (CF) tekniklerinin uygulanması.
cf(algorithm = "NMF",all_user_recommendations = FALSE,input_model = NA,
   interpolation = "average",iteration_only_termination = FALSE,
   max_iterations = 1000, min_residue = 1e-05,neighbor_search = "euclidean",
   neighborhood = 5,normalization = "none", query = NA,rank = 0,
   recommendations = 5,seed = 0,test = NA,training = life, verbose = FALSE)

#Yoğunluk Tahmin Ağaçları ile Yoğunluk Tahmini
det(folds = 10,input_model = NA,max_leaf_size = 10,min_leaf_size = 5,
    path_format = "lr",skip_pruning = FALSE,test = NA,training = life,
    verbose = FALSE)

#------------ÇALIŞMIYORLAR----
hava<-read.csv("golf_train.csv", fileEncoding = "latin5",check.names = F)[,-1]
hava1<-hava[,1:2]
hava1[,1]<-as.numeric(factor(hava1[,1],levels = c("soğuk","orta","sıcak")))
hava1[,2]<-as.numeric(factor(hava1[,2],levels = c("güneşli","bulutlu","yağmurlu")))
#Gizli Markov Modelleri (HMM'ler) için eğitim algoritmalarının uygulanması
mdl<-hmm_train(input_file=hava, batch = FALSE, gaussians = 2,
          input_model = NA, labels_file = "", seed = 0, states = 0,
          tolerance = 1e-05, type =list("gaussisn"), verbose = FALSE)

#Önceden eğitilmiş bir Gizli Markov Modelinden (HMM) rastgele diziler oluşturmaya yönelik bir yardımcı program. İstenilen dizinin uzunluğu belirtilebilir ve rastgele bir gözlem dizisi döndürülür.
hmm_generate(length, model=mdl, seed = NA, start_state = NA, verbose = FALSE)

#Gizli Markov Modelleri (HMM'ler) için bir dizinin log olasılığını hesaplamaya yönelik bir yardımcı program
hmm_input<-hmm_loglik(input=hava, input_model="mdl", verbose = FALSE)

#Gizli Markov Modelleri (HMM'ler) için en olası gizli durum sırasını hesaplamaya yönelik bir yardımcı program.
hmm_viterbi(input=hava, input_model="mdl", verbose = FALSE)
#-----------

#Çift ağaç algoritmalarıyla çekirdek yoğunluğu tahmininin bir uygulaması.
kde(abs_error = 0,algorithm = "dual-tree",bandwidth = 1,
    initial_sample_size = 100, input_model = NA,kernel = "gaussian",
    mc_break_coef = .4,mc_entry_coef = 3, mc_probability = .95,
    monte_carlo = FALSE,query = NA,reference = NA, rel_error = NA,tree = NA,
    verbose = FALSE)

#bir giriş veri kümesini iki düşük dereceli, negatif olmayan bileşene ayırmak için kullanılabilir.
nmf(input,rank,initial_h = NA,initial_w = NA,max_iterations = NA,
    min_residue = NA, seed = NA,update_rules = NA,verbose = FALSE)
    #Non-negative Matrix Factorization
```

# 2. kernlab

Sınıflandırma, regresyon, kümeleme, yenilik tespiti, niceliksel
regresyon ve boyutluluğun azaltılması için çekirdek tabanlı makine
öğrenimi yöntemleri. Diğer yöntemlerin yanı sıra 'kernlab', Destek
Vektör Makinelerini, Spektral Kümelemeyi, Çekirdek PCA'yı, Gauss
Süreçlerini ve bir QP çözücüyü içerir.

```{r}
library('kernlab')

#Gausspr, sınıflandırma ve regresyon için Gauss süreçlerinin bir uygulamasıdır.
mdl1<-gausspr(x=train_adlt, y=train_adlt[,13],data=NULL, scaled = FALSE,
              type= "classification", kernel="anovadot", kpar="automatic",
              var=0.001, variance.model = FALSE, tol=0.0001, cross=0, fit=TRUE,
              subset, na.action = na.omit)
 #type->"classification" or "regression"



#Ünlü k-ortalamalar algoritmasının ağırlıklı çekirdek versiyonu.
mdl2<-kkmeans(x=train_adlt, centers=9, kernel = "rbfdot",
              kpar =  list(sigma = 0.1),#"automatic"
              alg="kkmeans", p=1, na.action = na.omit)

#Çekirdek Maksimum Ortalama Farklılığı kmmd, parametrik olmayan bir dağıtım testi gerçekleştirir.
mdl3<-kmmd(train_adltincome~., data=train_adlt, kernel="rbfdot",
           kpar="automatic", alpha = 0.05,asymptotic = FALSE, replace = TRUE,
           ntimes = 150, frac = 1)

#Çekirdek Temel Bileşenler Analizi, temel bileşen analizinin doğrusal olmayan bir şeklidir.
mdl4<-kpca(x=train_adlt, kernel = "rbfdot", kpar = list(sigma = 0.1),
  features = 9, th = 1e-4, na.action = na.omit)

#Çekirdek Kantil Regresyon algoritması kqr, parametrik olmayan Kantil Regresyon gerçekleştirir.
mdl5<-kqr(train_adltincome~., data=train_adlt, scaled = TRUE, tau = 0.5,
          C = 0.1, kernel = "rbfdot", kpar = "automatic", reduced = FALSE,
          rank = dim(x)[1]/6, fit = TRUE, cross = 0, na.action = na.omit)

#ksvm, iyi bilinen C-svc, nu-svc, (sınıflandırma) tek sınıf-svc (yenilik) eps-svr, nu-svr (regresyon) formülasyonlarının yanı sıra yerel çok sınıflı sınıflandırma formülasyonlarını ve sınırlı kısıtlama SVM formülasyonlarını destekler.
mdl6<-ksvm(train_adltincome~., data=train_adlt , scaled = F, type = "kbb-svc",
           kernel ="rbfdot", kpar = "automatic", C = 1, nu = 0.2, epsilon = 0.1,
           prob.model = FALSE, class.weights = NULL, cross = 0, fit = TRUE,
           cache = 40,tol = 0.001,shrinking = TRUE, subset, na.action = na.omit)

#lssvm, csi işlevi tarafından hesaplanan çekirdek matrisinin ayrıştırılmasını kullanan En Küçük Kareler SVM'nin küçültülmüş bir sürümünü içerir.
mdl7<-lssvm(train_adltincome~., data=train_adlt, scaled = TRUE,
            kernel = "rbfdot", kpar = "automatic",type = NULL, tau = 0.01,
            reduced = TRUE, tol = 0.0001, rank = floor(dim(x)[1]/3), delta = 40,
            cross = 0, fit = TRUE, subset, na.action = na.omit)

#İlgililik Vektör Makinesi, aynı fonksiyonel formun destek vektör makinesine regresyonu ve sınıflandırılması için bir Bayes modelidir. rvm işlevi şu anda yalnızca regresyonu desteklemektedir.
mdl8<-rvm(train_adltincome~., data=train_adlt, type="regression",kernel="rbfdot",
    kpar="automatic",alpha= ncol(as.matrix(x)), var=0.1, var.fix=FALSE,
    iterations=100, verbosity = 0, tol = .Machine$double.eps, minmaxdiff = 1e-3,
    cross = 0, fit = TRUE, subset, na.action = na.omit)

#Bir spektral kümeleme algoritması. Kümeleme, verilerin bir ilgi matrisinin özvektörlerinin alt uzayına yerleştirilmesiyle gerçekleştirilir.
mdl9<-specc(x=train_adlt, centers,kernel = "rbfdot", kpar = "automatic",
            nystrom.red = FALSE, nystrom.sample = dim(x)[1]/6,iterations = 200,
            mod.sample = 0.75, na.action = na.omit)

predict(mdl1,ndata=test_adlt[,-13],type = "response", coupler = "minpair")
predict(mdl5,ndata=test_adlt[,-13])
predict(mdl6,ndata=test_adlt[,-13],type = "response", coupler = "minpair")
#K <- as.kernelMatrix(x)
#kha kfa inchol csi ranking
```

KERNEL • rbfdot Radial Basis kernel function "Gaussian" • polydot
Polynomial kernel function • vanilladot Linear kernel function • tanhdot
Hyperbolic tangent kernel function • laplacedot Laplacian kernel
function • besseldot Bessel kernel function • anovadot ANOVA RBF kernel
function • splinedot Spline kernel

KPAR • sigma inverse kernel width for the Radial Basis kernel function
"rbfdot" and the Laplacian kernel "laplacedot". • degree, scale, offset
for the Polynomial kernel "polydot" • scale, offset for the Hyperbolic
tangent kernel function "tanhdot" • sigma, order, degree for the Bessel
kernel "besseldot". • sigma, degree for the ANOVA kernel "anovadot".

# 3. gbm

Freund ve Schapire'nin AdaBoost algoritmasının ve Friedman'ın gradyan
artırma makinesinin uzantılarının bir uygulaması. En küçük kareler,
mutlak kayıp, t-dağılımı kaybı, niceliksel regresyon, lojistik, çok
terimli lojistik, Poisson, Cox orantılı tehlikeler kısmi olasılığı,
AdaBoost üstel kaybı, Huberleştirilmiş menteşe kaybı ve Sıralamayı
Öğrenme ölçümleri için regresyon yöntemlerini içerir

```{r}
library('gbm')
#Genelleştirilmiş güçlendirilmiş regresyon modellerine uyar
ft<-gbm(formula = csMPa~., distribution = "tdist", data = beton, weights=NULL,
    var.monotone = NULL,n.trees = 100,interaction.depth = 1,n.minobsinnode = 10,
    shrinkage = 0.1,bag.fraction = 0.5,train.fraction = 1,cv.folds = 0,
    keep.data = FALSE,verbose = FALSE,class.stratify.cv = NULL,n.cores = NULL)


#Bir GBM nesnesi için en uygun yükseltme yineleme sayısını tahmin eder ve isteğe bağlı olarak çeşitli performans önlemlerini çizer : "OOB","test","cv"
gbm.perf(ft, plot.it = TRUE, oobag.curve = FALSE, overlay = TRUE, method="OOB")

predict(ft, newdata=beton[sample(1:nrow(beton), 50),], n.trees=100,
        type = "link", single.tree = FALSE)
```

DISTRIBUTION:"gaussian" (squared error), "laplace" (absolute loss),
"tdist" (t-distribution loss), "bernoulli" (logistic regression for 0-1
outcomes), "huberized" (huberized hinge loss for 0-1 outcomes), classes,
"adaboost" (the AdaBoost exponential loss for 0-1 outcomes), "poisson"
(count outcomes), "coxph" (right censored observations), "quantile", or
"pairwise" (ranking measure using the LambdaMart algorithm).

# 4. e1071

Gizli sınıf analizine yönelik işlevler, kısa zamanlı Fourier dönüşümü,
bulanık kümeleme, destek vektör makineleri, en kısa yol hesaplaması,
torbalı kümeleme, saf Bayes sınıflandırıcısı, genelleştirilmiş k-en
yakın komşu.

```{r}
library('e1071')

#gknn, genel mesafe ölçümlerinden yararlanan k-en yakın komşular algoritmasının 
#bir uygulamasıdır. Bir formül arayüzü sağlanmıştır
m1<-gknn(income~., data = adult, subset=NULL, na.action = na.pass,
         scale = TRUE)
m1_1<-gknn(x=adult[,-13], y=adult[,13], k = 1, method = NULL,
           scale = TRUE, use_all = TRUE,FUN = mean)

#Bayes kuralını kullanarak bağımsız tahmin değişkenleri verildiğinde kategorik
# bir sınıf değişkeninin koşullu arka olasılıklarını hesaplar
m2<-naiveBayes(income~., data = adult, laplace = 0, subset=NULL,
               na.action = na.pass)
m2_1<-naiveBayes(x=adult[,-13], y=adult[,13], laplace = 0)

#svm, bir destek vektör makinesini eğitmek için kullanılır
##------data manuplation: soru işareti yerine kategorik değer atamak için
i<-c(2,4:8,12,13) #categorical
adlt[ , i] <- apply(adlt[ , i], 2, function(x) {
             ifelse(as.character(x) == "?",
             NA,as.character(x))})

adlt[ , i] <- apply(adlt[ , i], 2, function(x) as.numeric(factor(x)))
adlt <- impute(adlt, what = "median")
#-------
m3<-svm(income~., data = adlt, subset=NULL, na.action =na.omit, scale = TRUE)
m3_1<-svm(x=adlt[,-14], y=adlt[,14], scale = FALSE, type = "C-classification",
    kernel ="radial", degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),
    coef0 = 0, cost = 1, nu = 0.5, class.weights = NULL, cachesize = 40,
    tolerance = 0.001, epsilon = 0.1, shrinking = TRUE, cross = 0,
    probability = FALSE, fitted = TRUE, subset=NULL, na.action = na.omit)

#Kmeans gibi bir bölümleme kümesi algoritması, orijinal verilerden alınan önyükleme örnekleri üzerinde tekrar tekrar çalıştırılır. Ortaya çıkan küme merkezleri daha sonra hiyerarşik küme algoritması hclust kullanılarak birleştirilir
m4<-bclust(x=adlt, centers=2, iter.base=10, minsize=0,dist.method="manhattan", #"euclidean",
       hclust.method="average", base.method="kmeans",base.centers=20,
       verbose=TRUE,final.kmeans=FALSE, docmdscale=FALSE,resample=TRUE,
       weights=NULL, maxcluster=100)
m4_1<-hclust.bclust(object=m4, x=adlt, centers=2, dist.method=m4$dist.method,
              hclust.method=m4$hclust.method, final.kmeans=FALSE,
              docmdscale = FALSE, maxcluster=m4$maxcluster)

#Bilinen kmeans kümeleme algoritmasının bulanık versiyonunun yanı sıra çevrimiçi bir varyantı (Denetimsiz Bulanık Rekabetçi öğrenme)
m5<-cmeans(x=adlt, centers=2, iter.max = 100, verbose = FALSE,dist ="euclidean",
       method = "cmeans", m = 2,rate.par = NULL, weights = 1, control = list())

#C-kabuk kümeleme algoritması, bulanık kmeans kümeleme yönteminin kabuk prototip tabanlı versiyonu (halka prototipleri)
#cshell(x=adult[,c(1,11)], centers=2, iter.max=100, verbose=FALSE, dist="euclidean",
#           method="cshell", m=1, radius = 0.2)

predict(m1, adlt[sample(1:nrow(adult), 50),])#m2
predict(m3, adlt[sample(1:nrow(adlt), 50),])
```

# 5. caret

Sınıflandırma ve regresyon modellerini eğitmek ve çizmek için çeşitli
işlevler.

BU PAKET train FONKSİYONU İLE BAŞKA PAKETLERİ KULLANARAK MODELLEME YAPMA
YETENEĞİNE SAHİP (112 adet)
<https://topepo.github.io/caret/using-your-own-model-in-train>

```{r}
library('caret')
##------data manuplation: soru işareti yerine kategorik değer atamak için
tmp<-adult
i<-c(2,4:8,12,13) #categorical
tmp[ , i] <- apply(tmp[ , i], 2, function(x) as.numeric(factor(x)))
tmp1 <- preProcess(tmp, method="medianImpute")
adlt <- try(predict(tmp1, tmp), silent = TRUE)
x<-createDataPartition(y=adlt$income,times = 1,p = 0.7,list = TRUE,
                    groups = min(5, length(adlt$income)))
train_adlt<-adlt[x[[1]], ]
test_adlt<-adlt[-x[[1]], ]
rm(tmp,tmp1,i,x)
#--------

#Model Ortalamasını Kullanan Sinir Ağları
mdl<-avNNet(x=train_adlt[,-13], y=factor(train_adlt[,13]), repeats = 5, bag = T,
            allowParallel = T, seeds = sample.int(1e+05, 5), size = 25,
            linout = TRUE, trace = FALSE)

prd<-predict(mdl, test_adlt[,-13], type="class")
confusionMatrix(table(Prediction=as.numeric(prd),Referance=test_adlt[,13]))

#Bu fonksiyon, bir eğitim seti için sınıf ağırlık merkezlerini ve kovaryans matrisini hesaplar.
#Örneklerin her sınıf merkezine olan Mahalanobis mesafeleri.
md<-classDist(x=train_adlt[,-13], y=factor(train_adlt[,13]), groups = 5,
              pca = T, keep = NULL)
splom(predict(md, newdata=test_adlt[,-13], trans = log), grp=test_adlt[,13])

#Genetik algoritmalar kullanılarak denetlenen özellik seçimi
gafs(train_adlt[,-13], train_adlt[,13],
     gafsControl = gafsControl(functions = rfGA,method = "boot",number = 3))

#Bağımsız bileşenleri kullanarak doğrusal bir regresyon modeli takın
icr(formula=income~., data=train_adlt, weights, subset, na.action,
    contrasts = NULL)
predict(m,test_adlt[,-13])

#knn classification method
m<-knn3(x=train_adlt[,-13], y=factor(train_adlt[,13]), subset, na.action, k = 5)

#knn regression method
md<-knnreg(x=train_adlt[,-13], y=train_adlt[,13], subset, na.action, k = 5)

#PCA'yı bir veri kümesinde çalıştırın, ardından bunu bir sinir ağı modelinde kullanın
m<-pcaNNet(x=train_adlt[,-13], y=train_adlt[,13],thresh = 0.99, size=12,
           linout = TRUE, trace = FALSE)

predict(object=m, newdata=test_adlt[,-13], type = "raw")#"raw", "class", "prob"

#plsda, sınıflandırma için standart PLS modellerine uyacak şekilde kullanılırken splsda, seyrek PLS gerçekleştirir.özellik seçimi ve düzenlemeyi aynı amaç için yerleştirir.
md<-plsda(x=train_adlt[,-13], y=factor(train_adlt[,13]), ncomp = 2,
      probMethod = "bayes", prior = NULL)
confusionMatrix(predict(object=md, newdata = test_adlt[,-13], ncomp = NULL,
                        type = "class"), factor(test_adlt[,13]))

#Bu işlev, eğitim verilerine basit, filtre tabanlı özellik seçimi uygulandığında modeller için yeniden örnekleme tahminleri elde etmek için kullanılabilir.
sbf(train_adlt[,-13], train_adlt[,13],
     sbfControl = sbfControl(functions = rfSBF,method = "boot"))
```

PREPROCESS: "BoxCox", "YeoJohnson", "expoTrans", "center", "scale",
"range", "knnImpute", "bagImpute", "medianImpute", "pca", "ica" and
"spatialSign"

# 6. klaR

Sınıflandırma ve görselleştirmeye yönelik çeşitli işlevler, örn. düzenli
diskriminant analizi, sknn() çekirdek yoğunluğu saf Bayes, denetimli
sınıflandırma için 'svmlight' ve stepclass() sarmalayıcı değişken seçimi
için bir arayüz, sınıflandırma kurallarının partimat()
görselleştirilmesi ve küme sonuçlarının shardsplot() yanı sıra kmodes()
için bir arayüz kategorik veriler için kümeleme, corclust() değişken
kümeleme, farklı değişken kümeleme modellerinden değişken çıkarma ve
kanıt ağırlığı ön işleme.

SAĞLIKLI ÇALIŞMASI İÇİN svmlight PAKETİ GEREKLİ, ANCAK R'IN BU
VERSİYONUNA KURULAMIYOR. ADAMLAR svm HESAPLAMASI İÇİN e1071'i
ÖNERİYORLAR.

```{r, warning=FALSE}
library('klaR')
#Kategorik veriler üzerinde k-mod kümelemesi gerçekleştirin.Clustering
cl1<-kmodes(data=life[,-1], modes=35, iter.max = 10, weighted = FALSE,
            fast = TRUE)

#Doğrusal Diskriminant Analizinin yerelleştirilmiş bir versiyonu.
loc1<-loclda(x=train_adlt[,-14], grouping = factor(train_adlt[,14]), subset,
             na.action=na.omit)


#Alt sınıflarda ikili değişken seçimi gerçekleştirir.
subclass_class <- matrix(c("setosa","versicolor","virginica",
                           "mavi","mavi","yesil"),ncol=2, byrow = T)

cls5<-locpvs(x=iris[,-5], subclasses=iris$Species,
             subclass.labels=subclass_class, prior=NULL, method="lda",
             vs.method = c("stepclass"),#"ks.test" , "greedy.wilks"),
             niveau=0.05, fold=10, impr=0.1, direct="both", out=FALSE)

#Doğrudan sınıflandırma hatasını en aza indiren doğrusal boyut azaltma için bilgisayar yoğun yöntem.
cls6<-meclight(Species ~ ., data = iris, subset=NULL, na.action = na.omit)

#Bayes kuralını kullanarak bağımsız tahmin değişkenleri verildiğinde kategorik bir sınıf değişkeninin koşullu arka olasılıklarını hesaplar.
cl2<-NaiveBayes(x=train_adlt[,-14], grouping = factor(train_adlt[,14]),
                prior=NULL, usekernel = FALSE, fL = 0)

#En yakın ortalama sınıflandırma işlevi.
cls1<-nm(x=train_adlt[,-14], grouping = factor(train_adlt[,14]), gamma = 0)

#pvs Pairwise variable selection for classification(sınıf 2den fazla olmalı)
cls4<-pvs(x=iris[,-5], grouping = factor(iris[,5]), prior=NULL,
    method="lda",vs.method=c("ks.test"),#"stepclass","greedy.wilks"),
    niveau=0.05,fold=NULL, impr=NULL, direct=NULL, out=FALSE)

#rda Regularized Discriminant Analysis (RDA)
cls2<-rda(x=train_adlt[,-14], grouping = factor(train_adlt[,14]), prior = NULL,
          gamma = 0,lambda = 1, regularization = c(gamma = 0, lambda = 1),
          crossval = TRUE, fold = 10, train.fraction = 0.5,
          estimate.error = TRUE, output = FALSE, startsimplex = NULL,
          max.iter = 100, trafo = TRUE, simAnn = FALSE, schedule = 2,
          T.start = 0.1, halflife = 50, zero.temp = 0.01, alpha = 2,K = 100)

#sknn Simple k nearest Neighbours
cls3<-sknn(x=train_adlt[,-14], grouping = factor(train_adlt[,14]), kn = 3,
           gamma=0)

#Sınıflandırma Çoklu grup sınıflandırması, verilerin bire karşı geri kalanına bölünmesiyle yapılır.
##svmlight(Species ~ ., data = iris, subset=NULL, na.action = na.omit)

#İkili sınıflandırma için faktör değişkenlerinin kanıt dönüşümünün ağırlığını hesaplar.
cls7<-woe(x=adult, grouping = adult$income, weights = NULL, zeroadj = 0,
    ids = NULL, appont = TRUE)

# model başarım kriterleri - AC (accuracy)
test_iris<- iris[sample(nrow(iris), round(0.2*nrow(iris))),]

ucpm(predict(cl2,test_adlt[,-14])$posterior, factor(test_adlt[,14])) #0,59
ucpm(predict(cls1,test_adlt[,-14])$posterior, factor(test_adlt[,14])) #0,56
ucpm(predict(cls2,test_adlt[,-14])$posterior, factor(test_adlt[,14])) #0,48
ucpm(predict(cls3,test_adlt[,-14])$posterior, test_adlt[,14]) #0,60
ucpm(predict(cls4,test_iris[,-5])$posterior, test_iris[,5]) #0,89
##ucpm(predict(cls5,test_iris[,-5], quick = T,
##             return.subclass.prediction = TRUE)$posterior, test_iris[,5])
ucpm(predict(cls6,test_iris[,-5])$posterior, test_iris[,5]) #0,91
predict(cls7,adult[1:100,], replace = T) #only translate matrix
```

# 7. abess

En iyi alt küme seçimi problemini çözmek için son derece etkili araç
seti. Paket, doğrusal model için polinom zamanlarında tam destek
kurtarmayı ve küresel olarak en uygun çözümü garanti etmek için yeni bir
sıralama ve birleştirme tekniğinden yararlanan şekilde tasarlanan
algoritmaları uygular ve genelleştirir. Ayrıca lojistik regresyon,
Poisson regresyon, Cox orantılı risk modeli, Gama regresyon, çoklu yanıt
regresyon, çok terimli lojistik regresyon, sıralı regresyon, (sıralı)
temel bileşen analizi ve sağlam temel bileşen analizi için en iyi alt
küme seçimini destekler. Grup seçiminin en iyi alt kümesi ve kesin
bağımsızlık taraması gibi diğer değerli özellikler de sağlanmaktadır.

```{r}
library('abess')

# Uyarlanabilir en iyi alt küme seçimi (genelleştirilmiş doğrusal model için)
mdl<-abess(x=train_adlt[,-14],y=train_adlt[,14],family = c("binomial"),
 # "gaussian", "poisson", "cox", "mgaussian","multinomial","gamma", "ordinal"
 tune.path = c("sequence", "gsection"),tune.type = c("cv"),#gic,ebic,bic,aic
 weight = NULL,normalize = NULL,fit.intercept = TRUE,
 beta.low = -.Machine$double.xmax, beta.high = .Machine$double.xmax,c.max = 2,
 support.size = NULL,gs.range = NULL, lambda = 0,always.include = NULL,
 group.index = NULL,init.active.set = NULL, splicing.type = 2,
 max.splicing.iter = 20,screening.num = NULL,important.search = NULL,
 warm.start = TRUE,nfolds = 5,foldid = NULL,cov.update = FALSE,
 newton = c("exact", "approx"),newton.thresh = 1e-06,max.newton.iter = NULL,
 early.stop = FALSE,ic.scale = 1,num.threads = 0,seed = 1,subset="fnlwgt")

# Temel bileşen analizi için uyarlanabilir en iyi alt küme seçimi
ml1<-abesspca(x=adlt[,-14],type = c("predictor", "gram"),
              sparse.type = c("fpc", "kpc"),cor = FALSE, kpc.num = NULL,
              support.size = NULL,gs.range = NULL,
              tune.path = c("sequence", "gsection"),
              tune.type = c("gic", "aic", "bic", "ebic", "cv"),
              nfolds = 5,foldid = NULL,ic.scale = 1,c.max = NULL,
              always.include = NULL, group.index = NULL,screening.num = NULL,
              splicing.type = 1, max.splicing.iter = 20,warm.start = TRUE,
              num.threads = 0)


predict(mdl, test_adlt[,-14], type = "response")
plot(ml1)
extract(mdl)
```

y yanıt değişkeni: -"binom" iki seviyeye sahip olmalıdır. -'poisson'
için y pozitif tam sayıya sahip bir vektör olmalıdır. -"cox" için y,
hayatta kalma paketi tarafından döndürülen bir Surv nesnesi (önerilir)
veya "time" ve "status" adlı sütunları olan iki sütunlu bir matris
olmalıdır. -"mgaussian" için, y niceliksel yanıtların bir matrisi
olmalıdır. -"multinomial" veya "ordinal" için, y en az üç seviyeli bir
faktör olmalıdır. "Binom", "ordinal" veya "multinomial" için, eğer y
sayısal bir vektör olarak sunulursa, bir faktöre dönüştürüleceğini
unutmayın.

# 8. rms

ikili veya sıralı regresyon modelleri, Cox regresyonu, hızlandırılmış
başarısızlık süresi modelleri, sıradan doğrusal modeller, Buckley-James
modeli, seri veya uzamsal korelasyon için genelleştirilmiş en küçük
kareler ile çalışmak üzere yazılmıştır.

```{r}
library('rms')

#bj, Buckley-James dağıtımından bağımsız en küçük kareler çoklu regresyon modeline muhtemelen sağdan sansürlü bir yanıt değişkenine uyar
#bj(formula, data, subset=NULL, na.action=na.delete,link="log",
#   control=list(iter.max=20,trace=T,max.cycle=30), method='fit',
#   x=FALSE, y=FALSE,time.inc=30)

#rms Version of glm
csMPa=beton$csMPa
cement=beton$cement
slag=beton$slag
flyash=beton$flyash
water=beton$water
superplasticizer=beton$superplasticizer
coarseaggregate=beton$coarseaggregate
fineaggregate=beton$fineaggregate
age=beton$age
formula=csMPa~cement+slag+flyash+water+superplasticizer+coarseaggregate+
  fineaggregate+age

fit1<-Glm(formula=formula, data=beton[,-9], family = gaussian(), weights=NULL,
          subset=NULL, na.action = na.delete, start = NULL, offset = NULL,
          control = glm.control(epsilon=2,maxit=25), model = TRUE,
          method = "glm.fit", x = FALSE, y = TRUE, contrasts = NULL)

#Fit Linear Model Using Generalized Least Squares
#Gls(model, data, correlation, weights, subset, method, na.action=na.omit,
#    control, verbose, B=0, dupCluster=FALSE, pr=FALSE, x=FALSE)

formul=csMPa~cement+coarseaggregate+fineaggregate+age #sıkıntılı veriyi çıkardım

#Logistic Regression Model
fit2<-lrm(formula=formul, data=beton[,-c(2,3,4,5,9)])

#Linear Model Estimation Using Ordinary Least Squares
fit3<-ols(formula=formul, data=beton[,-c(2,3,4,5,9)])

#Ordinal Regression Model
fit4<-orm(formula=formul, data=beton[,-c(2,3,4,5,9)])


#Parametric Survival Model
#psm(formula=survival::Surv(time=lung$time, event=lung$status-1)~age, data=lung,
#    weights,subset, na.action=na.delete,
#    dist="weibull",init=NULL, scale=0,control=survreg.control(),parms=NULL,
#    model=FALSE, x=FALSE, y=TRUE, time.inc)

#Cox Proportional Hazards Model and Extensions ÇİFT HEDEF DEĞİŞKENİ
#cph(formula=survival::Surv(time=lung$time, event=lung$status-1)~age, data=lung,
#    weights, subset,na.action=na.delete,
#    method=c("efron","breslow","exact","model.frame","model.matrix"),
#    singular.ok=FALSE, robust=FALSE,model=FALSE, x=FALSE, y=FALSE, se.fit=FALSE,
#    linear.predictors=TRUE, residuals=TRUE, nonames=FALSE,eps=1e-4, init,
#iter.max=10,tol=1e-9, surv=FALSE, time.inc,type=NULL, vartype=NULL, debug=FALSE)

#Cox Survival Estimates
#survest(fit, newdata, linear.predictors, x, times,fun, loglog=FALSE, conf.int=0.95,
#        type, vartype,conf.type=c("log", "log-log", "plain", "none"), se.fit=TRUE,
#        what=c('survival','parallel'),individual=FALSE, ...)

#Cox Predicted Survival
#survfit(formula, newdata, se.fit=TRUE, conf.int=0.95,individual=FALSE, type=NULL,
#        vartype=NULL,conf.type=c('log', "log-log", "plain", "none"), id, ...)

Metrics::mae(beton[,9],
  predict(object=fit2, beton[,-c(2,3,4,5,9)], type=c("lp"),
        # "fitted","fitted.ind", "mean", "x", "data.frame","terms", "cterms",
        #"ccterms", "adjto","adjto.data.frame","model.frame"),
        se.fit=FALSE, codes=FALSE)
)
```

# 9. CORElearn

Tahmine dayalı modeller, örneğin isteğe bağlı yapıcı tümevarımlı
sınıflandırma ve regresyon ağaçlarını ve yapraklardaki modelleri,
rastgele ormanları, kNN'yi, naiveBayes'i ve yerel ağırlıklı regresyonu
içerir.

```{r}
library('CORElearn')

trainIdxs <- sample(x=nrow(iris), size=0.7*nrow(iris), replace=FALSE)
testIdxs <- c(1:nrow(iris))[-trainIdxs]

#Build a classification or regression model
mdlr<-CoreModel(formula=Species ~ ., data=iris[trainIdxs,], model="rf",
          #"rfNear","tree","knn","knnKernel","bayes","regTree"
          selectionEstimator="MDL",minNodeWeightRF=5,rfNoTrees=100,maxThreads=1)

Metrics::accuracy(iris[["Species"]][testIdxs],
                   predict(mdlr, iris[testIdxs,])$class
                   )
```

# 10. grf

Ormana dayalı istatistiksel tahmin ve çıkarım. GRF, tümü eksik ortak
değişkenleri destekleyen, heterojen tedavi etkileri tahmini (isteğe
bağlı olarak sağ sansürlü sonuçlar, çoklu tedavi kolları veya sonuçları
veya araçsal değişkenler kullanılarak) ve ayrıca en küçük kareler
regresyonu, niceliksel regresyon ve hayatta kalma regresyonu için
parametrik olmayan yöntemler sağlar.

```{r}
library('grf')
mdlg<-boosted_regression_forest(X=train_adlt[,-14], Y=train_adlt[,14],
  num.trees = 2000, sample.weights = NULL,clusters = NULL,
  equalize.cluster.weights = FALSE,sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(train_adlt[,-14])) + 20),ncol(train_adlt[,-14])),
  min.node.size = 5,honesty = TRUE,honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,alpha = 0.05,imbalance.penalty = 0,
  ci.group.size = 2,tune.parameters = "none",tune.num.trees = 10,
  tune.num.reps = 100,tune.num.draws = 1000,boost.steps = NULL,
  boost.error.reduction = 0.97,boost.max.steps = 5,
  boost.trees.tune = 10,num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max))

prd<-predict(mdlg, test_adlt[,-14])

Metrics::auc(test_adlt[,14], ifelse(prd<1.5,1,2))

#causal_forest :Causal forest
#causal_survival_forest :Causal survival forest
#instrumental_forest :Intrumental forest
#ll_regression_forest :Local linear forest
#lm_forest :LM Forest
#multi_arm_causal_forest :Multi-arm/multi-outcome causal forest
#multi_regression_forest :Multi-task regression forest
#quantile_forest :Quantile forest
#regression_forest :Regression forest
#survival_forest :Survival forest

```

# 11. evclass

Dempster-Shafer kütle fonksiyonları biçiminde çıktılar sağlayan farklı
kanıtsal sınıflandırıcılar. Yöntemler şunlardır: kanıtsal K-en yakın
komşu kuralı, kanıtsal sinir ağı, radyal temel fonksiyonlu sinir ağları,
lojistik regresyon, ileri beslemeli sinir ağları.

```{r}
library('evclass')
#Ana işlevler şunlardır:
#  EK-NN sınıflandırıcısının başlatılması, eğitimi ve değerlendirilmesi için EkNNinit, EkNNfit ve EkNNval;
#  kanıtsal sinir ağı sınıflandırıcısı için proDSinit, proDSfit ve proDSval;
#  karar verme kararı; RBF sınıflandırıcısı için RBFinit, RBFfit ve RBFval;
#  Eğitimli lojistik regresyon veya çok katmanlı sınıflandırıcılardan çıktı kütle fonksiyonlarını hesaplamak için calcAB ve calcm

#EkNN sınıflandırıcısı için parametrelerin başlatılması
int<-EkNNinit(x=train_adlt[,-14],y=train_adlt[,14], alpha = 0.95)
#EkNN sınıflandırıcının eğitimi
mdl1<-EkNNfit(x=train_adlt[,-14],y=train_adlt[,14],K=5,param = int,alpha = 0.95,
        lambda = 1/max(as.numeric(c('1','2'))),optimize = TRUE,
        options = list(maxiter = 300, eta = 0.1, gain_min = 1e-06, disp = TRUE))
#Bir test setinin EkNN sınıflandırıcı tarafından sınıflandırılması
EkNNval(xtrain=train_adlt[,-14], ytrain=train_adlt[,14], xtst=test_adlt[,-14],
        K=5, ytst = test_adlt[,14], param = mdl1$param)


#kanıtsal sinir ağı sınıflandırıcısı için başlangıç parametre değerlerini döndürür.
int<-proDSinit(x=train_adlt[,-14],y=train_adlt[,14], nproto=2, nprotoPerClass = T, crisp = T)
#kanıtsal sinir ağı sınıflandırıcısı için parametre optimizasyonu gerçekleştirir
mdl2<-proDSfit(x=train_adlt[,-14],y=train_adlt[,14],param=int,
         lambda = 1/max(as.numeric(c('1','2'))),mu = 0,optimProto = TRUE,
         options = list(maxiter = 500, eta = 0.1, gain_min = 1e-04, disp = 10))
#kanıta dayalı sinir ağı sınıflandırıcısını kullanarak bir test kümesindeki örnekleri sınıflandırır.
proDSval(x=test_adlt[,-14],y=test_adlt[,14], param=mdl2$param)


#RBFinit, Radyal Temel Fonksiyon sınıflandırıcısı için başlangıç parametre değerlerini döndürür.
int<-RBFinit(x=train_adlt[,-14],y=train_adlt[,14], nproto=5)
#RBFfit, radyal temel fonksiyon (RBF) sınıflandırıcısı için parametre optimizasyonu gerçekleştirir.
mdl3<-RBFfit(x=train_adlt[,-14],y=train_adlt[,14],param=int,lambda = 0,
       control = list(fnscale = -1, trace = 2, maxit = 1000),optimProto = TRUE)
#Bir test setinin radyal temel fonksiyon sınıflandırıcısı ile sınıflandırılması
RBFval(x=test_adlt[,-14],y=test_adlt[,14], param=mdl3$param, calc.belief = TRUE)
```

# 12. partykit

şunları içerir ('rpart', 'RWeka', 'PMML','party':cforest,ctree,mob)

```{r}
library('partykit')

#Temel öğreniciler olarak koşullu çıkarım ağaçlarını kullanan rastgele orman ve torbalama topluluğu algoritmalarının uygulanması
md1<-cforest(formula=income~., data=data.frame(train_adlt), weights=NULL,
      subset=NULL, offset=NULL, cluster=NULL, strata=FALSE, na.action = na.omit,
      control = ctree_control(teststat = "quad", testtype = "Univ",
                              mincriterion = 0, saveinfo = FALSE),
        ytrafo = NULL, scores = NULL, ntree = 500L,
        perturb = list(replace = FALSE, fraction = 0.632),
        mtry = ceiling(sqrt(13)), applyfun = NULL, cores = NULL,trace = FALSE)

md2<-ctree(formula=income~., data=data.frame(train_adlt), subset=NULL,
           weights=NULL, na.action = na.pass, offset=NULL, cluster=NULL,
           control = ctree_control(teststat = "quad", testtype = "Univ",
                              mincriterion = 0, saveinfo = FALSE),
           ytrafo = NULL,converged = NULL, scores = NULL, doFit = TRUE)

md3<-glmtree(formula=income~., data=data.frame(train_adlt), subset=NULL,
             na.action=na.omit, weights=NULL, offset=NULL, cluster=NULL,
             family = gaussian, epsilon = 1e-8, maxit = 25, method = "glm.fit")

md4<-lmtree(formula=income~., data=data.frame(train_adlt), subset=NULL,
            na.action=na.omit, weights=NULL, offset=NULL, cluster=NULL)

md5<-mob(formula=income~., data=data.frame(train_adlt), subset=NULL,
         na.action=na.omit, weights=NULL, offset=NULL, cluster=NULL,fit=FALSE,
         control = mob_control())

predict()
```

# 27. party

Özyinelemeli bölümleme için hesaplamalı bir araç kutusu. Paketin özü,
ağaç yapılı regresyon modellerini iyi tanımlanmış bir koşullu regresyon
teorisine yerleştiren koşullu çıkarım ağaçlarının bir uygulaması olan
çıkarım prosedürleri ctree()'dir.Bu parametrik olmayan regresyon
ağaçları sınıfı, nominal, sıralı, sayısal, sansürlü ve ayrıca çok
değişkenli yanıt değişkenleri ve ortak değişkenlerin keyfi ölçüm
ölçekleri dahil olmak üzere her türlü regresyon problemine
uygulanabilir.

```{r}
library('party')

#Yanıt veya Giriş Değişkenlerinin Dönüşümleri
ptrafo<-ptrafo(data=adult[,-3],
       numeric_trafo = function(x) scale(x),
       factor_trafo = function(x) factor(x),
       ordered_trafo = NULL, surv_trafo = NULL, var_trafo = NULL)

#Temel öğreniciler olarak koşullu çıkarım ağaçlarını kullanan rastgele orman ve torbalama topluluğu algoritmalarının uygulanması
m1<-cforest(formula=income~., data = data.frame(train_adlt), subset = NULL,
            weights = NULL, controls = cforest_unbiased(),xtrafo = ptrafo,
            ytrafo = ptrafo, scores = NULL)
table(test_adlt[,14], predict(m1,test_adlt[,-14]), OOB = TRUE)


#Koşullu çıkarım çerçevesinde sürekli, sansürlü, sıralı, nominal ve çok değişkenli yanıt değişkenleri için yinelemeli bölümleme
m2<-ctree(formula=income~., data = data.frame(train_adult), subset = NULL,
          weights = NULL, controls = ctree_control(), xtrafo = ptrafo,
          ytrafo = ptrafo, scores = NULL)
table(ifelse(predict(m2,data.frame(test_adlt)) <1.5,1,2), test_adlt[,14])

#MOB, her terminal düğümüyle ilişkili uygun modellere sahip bir ağaç sağlayan, model tabanlı özyinelemeli bölümlemeye yönelik bir algoritmadır.
m3<-mob(formula=income~age+workclass+educatoin+educatoin_num+marital_status+
          occupation+relationship+race+sex | capital_gain+capital_loss |
          hours_per_week+native_country,
          data = data.frame(train_adlt), weights=NULL, na.action = na.omit,
          model = glinearModel, control = mob_control(minsplit = 50))
table(ifelse(predict(m3,data.frame(test_adlt)) <1.5,1,2), test_adlt[,14])

```

# 13. RWeka

Weka, Java'da yazılmış, veri ön işlemeye yönelik araçlar içeren, veri
madenciliği görevleri için bir makine öğrenme algoritmaları
koleksiyonudur. sınıflandırma, regresyon, kümeleme, birliktelik
kuralları ve görselleştirme.

```{r}
library('RWeka')

#R interfaces to Weka regression and classification function learners.
LinearRegression(formula=csMPa~., data=beton, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)
Logistic(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,
  control = Weka_control(), options = NULL)
SMO(formula=Species~., data=iris, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)

#R interfaces to Weka lazy learners.k-nn
IBk(formula=income~., data=data.frame(train_adlt),subset=NULL,na.action=na.omit,
  control = Weka_control(), options = NULL)
#LBR(formula=income~., data=data.frame(train_adlt), subset=NULL, na.action=na.omit,
#  control = Weka_control(), options = NULL)

#R interfaces to Weka meta learners.
AdaBoostM1(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,
  control = Weka_control(), options = NULL)
Bagging(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,
  control = Weka_control(), options = NULL)
LogitBoost(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,# additive logistic regression
  control = Weka_control(), options = NULL)
#MultiBoostAB(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,
#  control = Weka_control(), options = NULL)
Stacking(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,
  control = Weka_control(), options = NULL)
#CostSensitiveClassifier(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,
#  control = Weka_control(), options = NULL)

#R interfaces to Weka rule learners.
JRip(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,#"Repeated Incremental Pruning to Produce
                                      #Error Reduction” (RIPPER)
  control = Weka_control(), options = NULL)
M5Rules(formula=csMPa~., data=beton, subset=NULL, na.action=na.omit,
  control = Weka_control(), options = NULL)
OneR(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,
  control = Weka_control(), options = NULL)
PART(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,
  control = Weka_control(), options = NULL)

#R interfaces to Weka regression and classification tree learners.
J48(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,#unpruned or pruned C4.5 decision trees
  control = Weka_control(), options = NULL)
LMT(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit,#“Logistic Model Trees”
  control = Weka_control(), options = NULL)
M5P(formula=income~., data=data.frame(train_adlt), subset=NULL,
    na.action=na.omit, control = Weka_control(), options = NULL)
DecisionStump(formula=income~., data=data.frame(train_adlt), subset=NULL,
              na.action=na.omit, control = Weka_control(), options = NULL)

#predict()

#Takılan kümeleyicilerden sınıf kimliklerini veya üyelikleri tahmin etmek için bir tahmin yöntemi
Cobweb(x=adult, control = NULL) #cluster
FarthestFirst(x=adult, control = NULL)#en uzak ilk geçiş algoritmasını
SimpleKMeans(x=adult, control = NULL)#k-means modellenen hızlı, basit yaklaşık bir kümeleyici
#XMeans(x=adult, control = NULL)#k-means extended by an “Yapıyı İyileştirme bölümü”
#DBScan(x=adult, control = NULL)#yoğunluk tabanlı kümeleme algoritmasını

#Weka_associators R/Weka Associators
x <- read.arff(system.file("arff", "contact-lenses.arff",
                           package = "RWeka"))
Apriori(x, control = NULL)
#Tertius(x, control = NULL)

```

# 14. mboost

Genelleştirilmiş doğrusal, toplamsal ve etkileşim modellerini potansiyel
olarak yüksek boyutlu verilere uydurmak için temel öğreniciler olarak
bileşen bazında (cezalandırılmış) en küçük kareler tahminlerini veya
regresyon ağaçlarını kullanan genel risk fonksiyonlarını optimize etmeye
yönelik fonksiyonel gradyan iniş algoritması (artırma).

```{r}
library('mboost')
# gamboost for boosted (generalized) additive models,
m1<-mboost(formula= income ~ btree(age)+bols(capital_gain)+bbs(hours_per_week),
       data = data.frame(train_adlt), na.action = na.omit, weights = NULL,
       offset = NULL, family = Gaussian(), control = boost_control(),
       oobweights = NULL, baselearner = c("btree")) #"bss", "bns" "bbs", "bols"


m2<-gamboost(formula=income~., data = adlt, na.action = na.omit, weights = NULL,
         offset = NULL, family = Binomial(type = "adaboost",link = "logit"),
         control = boost_control(), oobweights = NULL,
         baselearner ="bols",# c("bbs", "bols", "btree"),
         dfbase = 4)

# glmboost :Temel öğreniciler olarak bileşen bazında doğrusal modellerin kullanıldığı keyfi kayıp fonksiyonlarını optimize etmek için gradyan artırma.
m3<-glmboost(age ~ csMPa, data = beton[,8:9], weights = NULL,
         offset = NULL, family = Gaussian(),
         na.action = na.pass, contrasts.arg = NULL, center = FALSE,
         control =  boost_control(mstop = 2000), oobweights = NULL)

# blackboost for boosted trees.
m4<-blackboost(age ~ csMPa, data = beton[,8:9], weights = NULL,na.action = na.pass,
           offset = NULL, family = Gaussian(),
           control = boost_control(), oobweights = NULL,
           tree_controls = partykit::ctree_control(teststat = "quad",
                                                   testtype = "Teststatistic",
                                                   mincriterion = 0,
                                                   minsplit = 10,
                                                   minbucket = 4,
                                                   maxdepth = 2,
                                                   saveinfo = FALSE))

predict(m1, adlt[1:100,-14])
predict(m2, adlt[1:100,-14])


varimp(m1)
print(m1)
coef(m1)
```

FAMILY: AdaExp() AUC() Binomial(type = c("adaboost", "glm"), link =
c("logit", "probit", "cloglog", "cauchit", "log"), ...) GaussClass()
GaussReg() Gaussian() Huber(d = NULL) Laplace() Poisson()
GammaReg(nuirange = c(0, 100)) CoxPH() QuantReg(tau = 0.5, qoffset =
0.5) ExpectReg(tau = 0.5) NBinomial(nuirange = c(0, 100))
PropOdds(nuirange = c(-0.5, -1), offrange = c(-5, 5)) Weibull(nuirange =
c(0, 100)) Loglog(nuirange = c(0, 100)) Lognormal(nuirange = c(0, 100))
Gehan() Hurdle(nuirange = c(0, 100)) Multinomial() Cindex(sigma = 0.1,
ipcw = 1) RCG(nuirange = c(0, 1), offrange = c(-5, 5))

# 15. survival

Aşağıdakiler dahil temel hayatta kalma analizi rutinlerini içerir: Surv
nesnelerinin tanımı, Kaplan-Meier ve Aalen-Johansen (çok durumlu)
eğrileri, Cox modelleri, ve parametrik hızlandırılmış arıza süresi
modelleri

```{r}
library('survival')
data(cancer, package = 'survival')

#hayatta kalma nesnesi oluşturun.
S<-Surv(time=lung$time, event=lung$status-1, #time2,
     type=c('right'),origin=0)#'left','interval','counting','interval2','mstate'

#Bu işlev, bir formülden, önceden takılmış bir Cox modelinden veya önceden takılmış hızlandırılmış arıza süresi modelinden hayatta kalma eğrileri oluşturur.
s1 <- survfit(S ~ 1, data = lung) ##Surv(time, status)

#Her katman için bir eğri olacak şekilde bir hayatta kalma eğrileri grafiği oluşturulur
plot(s1, conf.int=.95, mark.time=FALSE, pch=3, col=1, lty=1, lwd=1, cex=1,
     log=FALSE, xscale=1, yscale=1, ylim=1, fun="S",#, xmax=FALSE, xlim=FALSE
     xlab = "Days", ylab = "Overall survival probability", xaxs="r",
     conf.times=1, conf.cap=.005,conf.offset=.012,
     conf.type = c("log"),# "log-log", "plain", "logit", "arcsin"),
     mark=NULL, noplot="(s0)", cumhaz=FALSE)#,firstx, ymin=0

#bir yıl hayatta kalma olasılığı
summary(s1, times = 365.25)
summary(s1, times = c(1,30,60,90*(1:10)))

#iki veya daha fazla hayatta kalma eğrisi arasında fark olup olmadığını test eder
survdiff(formula= S ~ sex, data=lung)#,
#subset=c("inst","age","ph.ecog","ph.karno","pat.karno","meal.cal","wt.loss"),#c(1,4,6:10),
#na.action=na.omit, rho=0, timefix=FALSE)

#regresyon hesabı-Cox Orantılı Tehlikeler Modeli
coxph(S ~ sex, data = lung)



#Aalen'in sansürlenmiş veriler için eklemeli regresyon modeli
aareg(formula=S~sex, data=lung, weights=NULL, subset=NULL,
      na.action=NULL,qrtol=1e-07, nmin=3, dfbeta=FALSE,
      taper=1,test = c('aalen'),# 'variance', 'nrisk'),
      cluster=NULL,model=FALSE, x=FALSE, y=FALSE)



#Parametrik Hayatta Kalma Modeli için Regresyon
s2<-survreg(formula=S~sex + ph.ecog, data=lung, weights=NULL,
        subset=NULL,na.action=na.omit, dist="weibull", init=NULL, scale=0,
        control=survreg.control(),parms=NULL,model=FALSE, x=FALSE,y=TRUE,
        robust=FALSE, cluster=NULL, score=FALSE)

predict(object=s2, newdata = lung[1:25, c(3,5:6)],
        type=c("response"),#"link","lp","linear","terms","quantile","uquantile"
        se.fit=FALSE, terms=NULL, p=c(0.1, 0.9), na.action=na.pass)
```

# 17. BART

Bayesian Toplamalı Regresyon Ağaçları (BART), sürekli, ikili, kategorik
ve olaya kadar geçen süre sonuçları için ortak değişkenlerin esnek
parametrik olmayan modellemesini sağlar.

```{r}
library('BART')

y.train=ifelse(train_adlt[,14]==1,0,1)
ntype=as.integer(
    factor('pbart', levels=c('wbart', 'pbart', 'lbart')))
mdl<-gbart(x.train = train_adlt[,-14], y.train = y.train, x.test=matrix(0,0,0),
           type='pbart',  ntype= ntype, sparse=FALSE, theta=0, omega=1,
           a=0.5, b=1, augment=FALSE, rho=NULL, xinfo=matrix(0,0,0),
           usequants=FALSE, rm.const=TRUE, sigest=NA, sigdf=3, sigquant=0.90,
           k=2, power=2, base=0.95, lambda=NA, tau.num=c(NA, 3, 6)[ntype],
           offset=NULL, w=rep(1, length(y.train)),
           ntree=c(200L, 50L, 50L)[ntype], numcut=100L, ndpost=1000L,
           nskip=100L, keepevery=c(1L, 10L, 10L)[ntype], printevery=100L,
           transposed=FALSE, hostname=FALSE,
           mc.cores = 1L, ## mc.gbart only
           nice = 19L, ## mc.gbart only
           seed = 99L ## mc.gbart only
           )

predict(mdl,test_adlt[,-14])

#abart :AFT Olaya kadar geçen süre sonuçları için BART
#gbart :Sürekli ve ikili sonuçlar için genelleştirilmiş BART
#lbart :Lojistik latentlerle ikili sonuçlar için Logit BART
#mbart :Daha az kategoriye sahip kategorik sonuçlar için çok terimli BART
#mbart2 :Daha fazla kategoriye sahip kategorik sonuçlar için çok terimli BART
#pbart :Normal latentlerle ikili sonuçlar için Probit BART
#surv.bart :BART ile hayatta kalma analizi
#wbart :Sürekli sonuçlar için BART
```

sürekli için 'wbart', probit için 'pbart', logit için 'lbart'

# 18. bst

Temel öğreniciler olarak bileşen bazında doğrusal, düzleştirici eğriler
ve ağaç modelleri ile kayıp fonksiyonlarını optimize etmek için gradyan
artırma.

```{r}
library('bst')
y<-ifelse(train_adlt[,14]<2,-1,1)
mdl<-bst(x=train_adlt[,-14], y=y, cost = 0.5, family = "gaussian",
#c( "hinge", "hinge2", "binom", "expo", "poisson", "tgaussianDC", "thingeDC",
#"tbinomDC", "binomdDC", "texpoDC", "tpoissonDC","huber", "thuberDC","clossR",
#"clossRMM", "closs", "gloss", "qloss", "clossMM","glossMM", "qlossMM", "lar"),
    ctrl = bst_control(mstop=100), control.tree = list(maxdepth = 1),
    learner = c("ls"))#, "sm", "tree"))

predict(object=mdl, newdata=test_adlt[,-14], newy=NULL, mstop=100,
        type=c("response", "all.res", "class", "loss", "error"))

#plot(x, type = c("step", "norm"),...)
coef(object=mdl, which=mdl$ctrl$mstop)

#mada :One-vs-all multi-class AdaBoost
#mbst :Boosting for Multi-Classification
#mhingebst :Boosting for Multi-class Classification
#mhingeova :Multi-class HingeBoost
#rbst :Robust Boosting for Robust Loss Functions
```

# 19. frbs

Sınıflandırma ve regresyon görevleriyle ilgilenmek için bulanık kural
tabanlı sistemlere (FRBS'ler) dayalı çeşitli öğrenme algoritmalarının
uygulanması. ------ÇOK HATALI TAHMİN YAPIYOR-----

```{r}
library('frbs')

range.data<- matrix(apply(train_adlt[,-14],2,range),ncol = 13)
train_adlt_n<-norm.data(train_adlt[,-14], range.data= range.data, min.scale = 0,
                        max.scale = 1)
train_adlt_n <-cbind(train_adlt_n, train_adlt[,14])
test_adlt_n<-norm.data(test_adlt[,-14], range.data= range.data, min.scale = 0,
                       max.scale = 1)

# model building
mdl<-frbs.learn(data.train = train_adlt_n, range.data = range.data,
                method.type = "FRBCS.W", control = list(num.labels=2,
                                                type.mf="SIGMOID",
                                                type.tnorm="MIN",
                                                type.implication.func= "ZADEH",
                                                name="Sim-0"))
prd<-predict(mdl, test_adlt_n)

#• WM:
list(num.labels, type.mf, type.tnorm, type.defuz,type.implication.func, name)
#• HYFIS:
list(num.labels, max.iter, step.size, type.tnorm,type.defuz,
     type.implication.func, name)
#• ANFIS and FIR.DM:
list(num.labels, max.iter, step.size,type.tnorm, type.implication.func , name)
#• SBC:
list(r.a, eps.high, eps.low, name)
#• FS.HGD:
list(num.labels, max.iter, step.size, alpha.heuristic,type.tnorm,
     type.implication.func, name)
#• FRBCS.W and FRBCS.CHI:
list(num.labels, type.mf, type.tnorm,type.implication.func, name)
#• DENFIS method:
list(Dthr, max.iter, step.size, d, name)
#• GFS.FR.MOGUL:
list(persen_cross, max.iter, max.gen, max.tune,persen_mutant, epsilon, name)
#• GFS.THRIFT method:
list(popu.size, num.labels, persen_cross,max.gen, persen_mutant, type.tnorm,
     type.defuz,type.implication.func, name)
#• GFS.GCCL:
list(popu.size, num.class, num.labels, persen_cross,max.gen, persen_mutant,name)
#• FH.GBML:
list(popu.size, max.num.rule, num.class, persen_cross,
max.gen, persen_mutant, p.dcare, p.gccl, name)
#• SLAVE:
list(num.class, num.labels, persen_cross, max.iter,
max.gen, persen_mutant, k.lower, k.upper, epsilon, name)
#• GFS.LT.RS:
list(popu.size, num.labels, persen_mutant, max.gen,mode.tuning, type.tnorm, type.implication.func,type.defuz, rule.selection, name)
```

• "WM": Wang ve Mendel'in regresyon görevlerini yerine getirme tekniği.
• "SBC": regresyon görevlerini gerçekleştirmek için çıkarıcı kümeleme
yöntemi. • "HYFIS": regresyon görevlerini yerine getirmek için hibrit
sinirsel bulanık çıkarım sistemleri • "ANFIS": regresyon görevlerini
yerine getirmek için uyarlanabilir nöro-bulanık çıkarım sistemleri. •
"FRBCS.W": sınıflandırma görevlerini yerine getirmek için Ishibuchi'nin
yöntemini temel alan ağırlık faktörüne sahip bulanık kural tabanlı
sınıflandırma sistemleri. • "FRBCS.CHI": sınıflandırma görevlerini
yerine getirmek için Chi'nin yöntemini temel alan bulanık kural tabanlı
sınıflandırma sistemleri. • "DENFIS": regresyon görevlerini yerine
getirmek için dinamik gelişen nöro-bulanık çıkarım sistemleri. •
"FS.HGD": Regresyon görevlerini yerine getirmek için sezgisel ve gradyan
iniş yöntemini kullanan bulanık sistem. • "FIR.DM": regresyon
görevlerini yerine getirmek için iniş yöntemiyle bulanık çıkarım
kuralları. • GFS.FR.MOGUL": regresyon görevlerini yerine getirmek
amacıyla MOGUL metodolojisini temel alan bulanık kural öğrenimine
yönelik genetik bulanık sistemler. • "GFS.THRIFT": Thrift'in regresyon
görevlerini yerine getirmek için genetik algoritmalara dayanan tekniği.
• "GFS.GCCL": Ishibuchi'nin genetik işbirliği-rekabete dayanan yöntemi
Sınıflandırma görevlerini yerine getirmeyi öğrenme. • "FH.GBML":
Ishibuchi'nin genetik işbirlikçi rekabetçi öğrenme ile sınıflandırma
görevlerini yürütmek için Pittsburgh'un melezleştirilmesine dayanan
yöntemi. • "SLAVE": sınıflandırma görevlerini yerine getirmek için
belirsiz ortamda yapısal öğrenme algoritması. • "GFS.LT.RS": yanal
ayarlama ve kural seçimi için genetik algoritma.

# 20. ipred

Dolaylı sınıflandırma yoluyla geliştirilmiş tahmin modelleri ve
sınıflandırma, regresyon ve hayatta kalma sorunları için torbalamanın
yanı sıra tahmin hatasının yeniden örneklemeye dayalı tahmin edicileri.

```{r}
library('ipred')
## S3 method for class 'factor'
m1<-ipredbagg(y=adult[,15], X=adult[,-c(3,15)], nbagg=25,
          control=rpart::rpart.control(minsplit=2, cp=0, xval=0),
          comb=NULL, coob=FALSE, ns=length(adult[,15]), keepX = TRUE,
          na.action=na.omit)

## S3 method for class 'numeric'
m2<-ipredbagg(y=adlt[,14], X=adlt[,-14], nbagg=25,
              control=rpart::rpart.control(xval=0), comb=NULL, coob=FALSE,
              ns=length(adlt[,14]), keepX = TRUE)

## S3 method for class 'Surv'
ipredbagg(y, X=NULL, nbagg=25, control=rpart.control(xval=0),
          comb=NULL, coob=FALSE, ns=dim(y)[1], keepX = TRUE)

## S3 method for class 'data.frame'
m3<-bagging(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit)


predict(m1,test_adlt[,-14])
```

# 21. VGAM

VGAM, vektör genelleştirilmiş doğrusal ve toplam modelleri (VGLM'ler ve
VGAM'ler) ve ilgili modelleri (İndirgenmiş dereceli VGLM'ler, İkinci
Dereceden RR-VGLM'ler, İndirgenmiş dereceli VGAM'ler) uydurmak için
işlevler sağlar. Bu paket, maksimum olasılık tahmini (MLE) veya
cezalandırılmış MLE'ye göre birçok modele ve dağıtıma uyar. Ayrıca
ekolojideki kısıtlı ikinci dereceden koordinasyon (CQO) gibi kısıtlı
koordinasyon modellerine de uyar. ------ÇOK FAZLA RAM İSTİYOR \~50 GB

```{r}
library('VGAM')

control <- vglm.control(checkwz = TRUE, Check.rank = TRUE, Check.cm.rank = TRUE,
                        criterion = names(.min.criterion.VGAM),epsilon = 1e-07,
                        half.stepsizing = TRUE, maxit = 30, noWarning = FALSE,
                        stepsize = 1, save.weights = FALSE, trace = FALSE,
                        wzepsilon = .Machine$double.eps^0.75, xij = NULL)

#vektör genelleştirilmiş toplam modeli (VGAM)
vgam(formula=csMPa~.,family = cratio(), data = beton,
     weights = NULL, subset = NULL,na.action = na.fail, etastart = NULL,
     mustart = NULL, coefstart = NULL, control = control,offset = NULL,
     method = "vgam.fit", model = FALSE,x.arg = TRUE, y.arg = TRUE,
     contrasts = NULL,constraints = NULL, extra = list(), form2 = NULL,
     qr.arg = FALSE, smart = TRUE)

#İndirgenmiş dereceli vektör genelleştirilmiş doğrusal model (RR-VGLM)
rrvglm(formula, family = stop("argument 'family' needs to be assigned"),
       data = list(), weights = NULL, subset = NULL,na.action = na.fail,
       etastart = NULL, mustart = NULL, coefstart = NULL,
       control = rrvglm.control(...), offset = NULL, method = "rrvglm.fit",
       model = FALSE, x.arg = TRUE, y.arg = TRUE,contrasts = NULL,
       constraints = NULL, extra = NULL,qr.arg = FALSE, smart = TRUE, ...)

#vektör genelleştirilmiş doğrusal modeller (VGLM)
vglm(formula=csMPa~.,family = cratio(), data = beton,
     weights = NULL, subset = NULL, na.action = na.fail, etastart = NULL,
     mustart = NULL, coefstart = NULL, control = control, offset = NULL,
     method = "vglm.fit", model = FALSE, x.arg = TRUE, y.arg = TRUE,
     contrasts = NULL, constraints = NULL, extra = list(),form2 = NULL,
     qr.arg = TRUE, smart = TRUE)

#Kısıtlı bir ikinci dereceden koordinasyon (CQO; daha önce kanonik Gauss koordinasyonu veya CGO olarak adlandırılıyordu) modeli, ikinci dereceden indirgenmiş sıralı vektör genelleştirilmiş doğrusal model (QRR-VGLM) çerçevesi kullanılarak takılır.
cqo(formula, family = stop("argument 'family' needs to be assigned"),data = list(),
    weights = NULL, subset = NULL,na.action = na.fail, etastart = NULL,
    mustart = NULL, coefstart = NULL, control = qrrvglm.control(...),
    offset = NULL,method = "cqo.fit", model = FALSE, x.arg = TRUE, y.arg = TRUE,
    contrasts = NULL, constraints = NULL, extra = NULL,smart = TRUE, ...)

#Goodman'ın RC ilişkilendirme modelini (GRC) bir sayım matrisine ve daha genel olarak satır sütun etkileşimine uyar
grc(y, Rank = 1, Index.corner = 2:(1 + Rank),str0 = 1, summary.arg = FALSE,
    h.step = 1e-04, ...)

rcim(y, family = poissonff, Rank = 0, M1 = NULL, weights = NULL, which.linpred = 1,
     Index.corner = ifelse(is.null(str0), 0, max(str0)) + 1:Rank,rprefix = "Row.",
     cprefix = "Col.", iprefix = "X2.", offset = 0, str0 = if (Rank) 1 else NULL,
     summary.arg = FALSE, h.step = 0.0001,rbaseline = 1, cbaseline = 1,
     has.intercept = TRUE, M = NULL, rindex = 2:nrow(y), cindex = 2:ncol(y),
     iindex = 2:nrow(y), ...)

#Kısıtlı bir toplamsal düzenleme (CAO) modeli, azaltılmış dereceli vektör genelleştirilmiş toplamsal model (RR-VGAM) çerçevesi kullanılarak takılır.
cao(formula, family = stop("argument 'family' needs to be assigned"),data = list(),
    weights = NULL, subset = NULL, na.action = na.fail,etastart = NULL,
    mustart = NULL, coefstart = NULL,control = cao.control(...), offset = NULL,
    method = "cao.fit", model = FALSE, x.arg = TRUE, y.arg = TRUE,
    contrasts = NULL, constraints = NULL,extra = NULL, qr.arg = FALSE,
    smart = TRUE, ...)

predictvglm(object, newdata = NULL,type = c("link", "response", "terms"),
            se.fit = FALSE, deriv = 0, dispersion = NULL,untransform = FALSE,
            type.fitted = NULL, percentiles = NULL, ...)
```

# 22. mda

Karışım ve esnek diskriminant analizi, çok değişkenli uyarlamalı
regresyon eğrileri (MARS), BRUTO ve vektör tepkisi yumuşatma eğrileri.

```{r}
library(mda)
#----manipule test data, ? to NA and omit
ad <- adult[,-3]#clear id column
ad <- replace(ad,ad=="?",NA)#replace ? to NA
ad <- droplevels(ad)#update levels
ad <- na.omit(ad)
ad$income <- cut(ad$capital_gain, breaks = c(0, 18000, 40000, 99999),
                 labels = c("<18K", "18K-40K", ">40K"),include.lowest = T)
#create 3 class


#bruto Fit an Additive Spline Model by Adaptive Backfitting
m1<-bruto(x=beton[,-9], y=beton[,9], w=1, wp=1/9, dfmax=50, cost=2,
      maxit.select=20, maxit.backfit=20,thresh = 0.0001, trace.bruto = FALSE,
      start.linear = TRUE)#, fit.object)

#fda Flexible Discriminant Analysis
m2<-fda(formula=income~., data=ad[1:20000,])


#mda Mixture Discriminant Analysis
m3<-mda(formula=income~., data=ad[1:20000,])


#gen.ridge Penalized Regression:Cezalandırılmış diskriminant analizinde kullanıldığı gibi, cezalandırılmış bir regresyon gerçekleştirin
m4<-gen.ridge(x=adlt[,-14], y=adlt[,14])

#mars Çok değişkenli uyarlanabilir regresyon eğrileri.
m5<-mars(x=adlt[,-14], y=adlt[,14])

#polinom regression
m6<-polyreg(x=adlt[,-14], y=adlt[,14])

(p1<-predict(m1,as.matrix(beton[,-9]), type = "terms"))#"fitted"
#p2<-predict(m2,ad[20000:30161,-14], type = "class")
#p3<-predict(m3,ad[20000:30161,-14], type = "class")
#p4<-predict(m5,ad[20000:30161,-14])
#confusion(p2, adult[,15])#fda or mda
#confusion(p3, adult[,15])
```

# 23. rpart

Sınıflandırma, regresyon ve hayatta kalma ağaçları için yinelemeli
bölümleme

```{r}
library('rpart')

control=rpart.control(minsplit = 20, minbucket = round(20/3), cp = 0.01,
maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,
surrogatestyle = 0, maxdepth = 30)

mdl<-rpart(formula=income~., data=adult, weights=NULL, subset=fnlwgt,
      na.action = na.rpart, method='class',#"anova", "poisson", "class" or "exp"
      model = FALSE, x = FALSE, y = TRUE, parms=NULL, control=control)# cost

predict(object=mdl, newdata=adult[sample(nrow(adult),200,replace = F),],
        type = "class",na.action = na.pass)#c("vector", "prob", "class", "matrix"),
        
```

method : If y is a survival object, then method ="exp" is assumed, if y
has 2 columns then method = "poisson" is assumed, if y is a factor then
method = "class" is assumed, otherwise method = "anova" is assumed.

# 24. adabag

Applies Multiclass AdaBoost.M1, SAMME and Bagging

```{r}
library('adabag')
#Torbalama algoritmasını bir veri kümesine uygular
mdl1<-bagging(formula=income~., data=adult[,-3], mfinal = 100, control=control,
              par=FALSE)

#AdaBoost.M1 ve SAMME algoritmalarını bir veri kümesine uygular
mdl2<-boosting(formula=income~., data=adult[1:5000,-3], boos = TRUE, mfinal = 100,
               coeflearn = 'Breiman', control=control)#’Freund’ ’Zhu’

p1<-predict(object=mdl1, newdata=adult[sample(nrow(adult),200,replace = F),],
            newmfinal=length(mdl1$trees))
p2<-predict(object=mdl2, newdata=adult[sample(nrow(adult),200,replace = F),],
            newmfinal=length(mdl2$trees))
p1$confusion;p2$confusion
```

# 26. grpreg

Gruplandırılmış cezalarla doğrusal regresyon, GLM ve Cox regresyon
modellerinin düzenlileştirme yolunu uydurmak için etkili algoritmalar.
Bu, grup kementi, grup MCP ve grup SCAD gibi grup seçim yöntemlerinin
yanı sıra grup üstel kement, bileşik MCP ve grup köprüsü gibi iki
düzeyli seçim yöntemlerini içerir.

```{r}
library('grpreg')
#gBridge :Grup köprüsü regresyon yolunu sığdır
gr<-gBridge(X=adlt[,-14], y=adlt[,14], group=1:13, family="binomial")
#  c("gaussian","poisson"), nlambda=100, lambda, lambda.min={if (nrow(X) > ncol(X)) .001
#  else .05}, lambda.max, alpha=1, eps=.001, delta=1e-7, max.iter=10000,
#  gamma=0.5, group.multiplier, warn=TRUE, returnX=FALSE, ...)

#grpreg :Düzenlileştirme parametresi lambda için bir değerler tablosu üzerinde gruplandırılmış cezalara sahip modeller için düzenleme yollarını sığdırın. Doğrusal ve lojistik regresyon modellerine uyar.
mdl1<-grpreg(X=adlt[,-14], y=adlt[,14], group=gr$group, #1:ncol(X),
       penalty="grLasso", #"grMCP", "grSCAD","gel", "cMCP"
       family=gr$family,#c("gaussian", "binomial", "poisson"),
       nlambda=100, lambda=gr$lambda,
       lambda.min={if (nrow(X) > ncol(X)) 1e-4 else .05},
       log.lambda = TRUE, alpha=1, eps=1e-4, max.iter=10000, dfmax=5, gmax=13, #length(unique(group)),
       gamma=3,#ifelse(penalty == "grSCAD", 4, 3),
   tau = 1/3, group.multiplier=gr$group.multiplier, warn=TRUE, returnX = FALSE)

#grpsurv :Düzenlileştirme parametresi lambda için bir değerler tablosu üzerinde gruplandırılmış cezalarla Cox modelleri için düzenlileştirme yollarını sığdırın.
data(Lung)
mdl2<-grpsurv(X=Lung$X, y=Lung$y, group=Lung$group,#1:ncol(X),
              penalty="grLasso",#"grMCP", "grSCAD","gel", "cMCP"
              gamma=3,#ifelse(penalty=="grSCAD", 4, 3),
              alpha=1, nlambda=100, lambda=0.02,
              lambda.min={if (nrow(X) > ncol(X)) 0.001 else .05},
              eps=.001, max.iter=10000, dfmax=5, gmax=6,#length(unique(group)),
            tau=1/3, group.multiplier=c(1,1,1,1,1,1), warn=TRUE, returnX=FALSE)

predict(mdl1, X=adlt[1:100,-14], type="class")
#c("link", "response", "class","coefficients", "vars", "groups", "nvars", "ngroups", "norm")

```

# 28. randomForestSRC

Tek değişkenli, çok değişkenli, denetimsiz, hayatta kalma, rekabet eden
riskler, sınıf dengesizliği sınıflandırması ve niceliksel regresyon için
Breiman'ın rastgele ormanlarının hızlı OpenMP paralel hesaplanması. Yeni
Mahalanobis'in ilişkili sonuçlar için bölünmesi. Aşırı rastgele ormanlar
ve rastgele bölme. Eksik veriler için atama yöntemleri paketi. Alt
örneklemeyi kullanan hızlı rastgele ormanlar. Değişken önemi için güven
bölgeleri ve standart hatalar. Yeni geliştirilmiş bekletme önemi. Duruma
özel önem. Minimum derinlik değişkeninin önemi.

```{r}
library('randomForestSRC')
#veri hazırlama
adlt<-adult[,-3]#clear id column
adlt <- replace(adlt,adlt=="?",NA)#replace ? to NA
adlt <- droplevels(adlt)#update levels
#impute NA cells
adlt <- impute(formula=NULL, data=adlt,
              ntree = 100, nodesize = 3, nsplit = 10,
              nimpute = 2, fast = FALSE, blocks=NULL,
              mf.q=0.5, max.iter = 10, eps = 0.01,
              ytry = NULL, always.use = NULL, verbose = TRUE)


#rfsrc Hayatta Kalma, Regresyon ve Sınıflandırma için Hızlı Birleşik Rastgele Ormanlar
md<-rfsrc(formula=income~., data=adlt, ntree = 500, mtry = 5, ytry = 1,
          nodesize = 5, nodedepth = NULL, splitrule = NULL, nsplit = 10,
          importance = "none",#c(FALSE, TRUE, "none", "anti", "permute", "random"),
          block.size = if (any(is.element(as.character("none"),
                                      c("none", "FALSE")))) NULL else 10,
      bootstrap = "none",#c("by.root", "none", "by.user"),
      samptype = "swr",#c("swor", "swr"),
      samp = NULL, membership = FALSE,
      sampsize = if (samptype == "swor") function(x){x * .632} else function(x){x},
      na.action = "na.omit",#c("na.omit", "na.impute"),
      nimpute = 1, ntime = 150, cause=1,
      perf.type = "none", proximity = FALSE, distance = FALSE, forest.wt = FALSE,
      xvar.wt = NULL, yvar.wt = NULL, split.wt = NULL,case.wt = NULL,forest = TRUE,
      save.memory = FALSE,var.used = FALSE,#c(FALSE, "all.trees", "by.tree"),
      split.depth = FALSE,#c(FALSE, "all.trees", "by.tree"),
      seed = NULL, do.trace = FALSE, statistics = FALSE)

p<-predict(object=md, newdata=adlt[1:100,-14], m.target = NULL,
        importance = FALSE,#c(FALSE, TRUE, "none", "anti", "permute", "random"),
        get.tree = NULL,
        block.size = if (any(is.element(as.character(FALSE),
                                        c("none", "FALSE")))) NULL else 10,
        na.action = "na.omit",#c("na.omit", "na.impute", "na.random"),
        outcome = c("train"),# "test"),
        perf.type = NULL,  proximity = FALSE,  forest.wt = FALSE,  ptn.count = 0,
        distance = FALSE, var.used = FALSE,#c(FALSE, "all.trees", "by.tree"),
        split.depth = FALSE,#c(FALSE, "all.trees", "by.tree"), seed = NULL,
        do.trace = FALSE, membership = FALSE, statistics = FALSE)

```

# 30. randomForest

Breiman'a dayalı, rastgele girdiler kullanan bir ağaç ormanına dayalı
sınıflandırma ve regresyon.

```{r}
library('randomForest')
#veri hazırlama----FAZLA RAM İSTİYOR VE ÇOK UZUN SÜRÜYOR impute kısmı---
#a<-adult[,-3]#clear id column
#a <- replace(a,a=="?",NA)#replace ? to NA
#a <- droplevels(a)#update levels
#a<-rfImpute(x=a[,-14], y=a[,14], iter=10, ntree=100)

mdl<-randomForest(formula=income~., data=adult, subset=fnlwgt, na.action=na.omit)

predict(mdl, adult[1:100,-15], type="response",norm.votes=TRUE, predict.all=FALSE,
        proximity=FALSE, nodes=FALSE)#,cutoff=forest$cutoff)

getTree(rfobj=mdl, k=1, labelVar=FALSE)#extract the structure of a tree from a randomForest object
importance(x=mdl, type=NULL, class=NULL, scale=TRUE)# extractor function for variable importance measures
```

# 31. nlme

Gauss doğrusal ve doğrusal olmayan karma etki modellerini yerleştirin ve
karşılaştırın.

```{r}
library('nlme')
#Bu fonksiyon genelleştirilmiş en küçük kareler kullanan doğrusal bir modele uyar.
mm<-gls(model=csMPa~cement+water+age, data=beton, correlation=NULL, weights=NULL,
    subset=1:8, method='ML', na.action=na.fail,control=list(), verbose=FALSE)

#Bu fonksiyon genelleştirilmiş en küçük kareler kullanan doğrusal olmayan bir modele uyar.
#gnls(model=csMPa ~ cement+water+age|sqr(coarseaggregate), data=beton,
#     params= 'coarseaggregate',
#     start=1, correlation=NULL, weights=NULL, subset=1:8, na.action=na.fail,
#     naPattern=NULL, control=list(), verbose=FALSE)

#Doğrusal Karma Efekt Modelleri
#lme(csMPa ~ fineaggregate+age, data = beton[,7:9], random = ~ 1)
#fixed, data, random, correlation, weights, subset, method,na.action, control,
#    contrasts = NULL, keep.data = TRUE)

#Doğrusal Olmayan Karma Etki Modelleri
#nlme(model, data, fixed, random, groups, start, correlation, weights,subset, method,
#     na.action, naPattern, control, verbose)


#plot(mm)
predict(mm,beton[1:100,c(1,4,8)])
#qqnorm(mm)
```

# 32. arules

İşlem verilerini ve modellerini (sık öğe kümeleri ve birliktelik
kuralları) temsil etmek, değiştirmek ve analiz etmek için altyapı
sağlar. Ayrıca Apriori ve Eclat ilişkilendirme madenciliği
algoritmalarının C uygulamalarını sağlar.

```{r}
library('arules')
##data(Adult,package="arules")

## remove attributes
adult[["fnlwgt"]] <- NULL
adult[["education-num"]] <- NULL

adult <- replace(adult,adult=="?",NA)#replace ? to NA
adult <- droplevels(adult)#update levels
#impute NA cells
adult <- randomForestSRC::impute(formula=NULL, data=adult,
              ntree = 100, nodesize = 3, nsplit = 10,
              nimpute = 2, fast = FALSE, blocks=NULL,
              mf.q=0.5, max.iter = 10, eps = 0.01,
              ytry = NULL, always.use = NULL, verbose = TRUE)

## map metric attributes
adult[["age"]] <- ordered(cut(adult[["age"]], c(15, 25, 45, 65, 100)),
                          labels = c("Young", "Middle-aged", "Senior", "Old"))
adult[["hours_per_week"]] <- ordered(cut(adult[["hours_per_week"]],
                                         c(0,25,40,60,100),
    labels = c("Part-time", "Full-time", "Over-time", "Workaholic")))
adult[["capital_gain"]] <- ordered(cut(adult[["capital_gain"]],
  c(-Inf,0,median(adult[["capital_gain"]][adult[["capital_gain"]] > 0]),
    Inf)), labels = c("None", "Low", "High"))
adult[["capital_loss"]] <- ordered(cut(adult[["capital_loss"]],
  c(-Inf,0, median(adult[["capital_loss"]][adult[["capital_loss"]] > 0]),
    Inf)), labels = c("None", "Low", "High"))
## create transactions
adult <- transactions(adult)
adult


#Apriori algoritmasını kullanarak sık öğe kümelerini, birliktelik kurallarını veya birliktelik hiper kenarlarını kazın.
m<-apriori(data=adult, parameter =  list(supp = 0.5, conf = 0.9, target = "rules"),
           appearance = NULL,#list(items = c("income=<=50K", "income=>50K", "age=Young")
           control = NULL)

confint(m, "oddsRatio", smoothCounts = .5)
crossTable(adult,measure="lift",sort=T)
DATAFRAME(head(adult))
DATAFRAME(m)
eclat(adult,parameter = list(supp = 0.1, maxlen = 5))
#fim4r(Adult, method = "fpgrowth",target = "rules", supp = .7, conf = .8)
z<-inspect(m[100:101], ruleSep = "~~>", itemSep = " + ", setStart = "",
           setEnd = "", linebreak = FALSE)
LIST(adult[1:5])


#Bir dosyadan işlem verilerini okur ve bir işlem nesnesi oluşturur.
#read.transactions(file,format = c("basket", "single"),header = FALSE,sep = "",
#cols = NULL,rm.duplicates = FALSE,quote = "\"'",skip = 0,encoding = "unknown")

#Eclat algoritmasıyla sık görülen öğe kümelerini bulun. Bu uygulama, ağırlıklı birliktelik kuralı madenciliği (WARM) uygulamak için optimize edilmiş işlem kimliği listesi birleştirmelerini ve işlem ağırlıklarını kullanır.
weclat(data=adult, parameter = NULL, control = NULL)
```

# 33. elasticnet

Elastic-Net'in tüm çözüm yolunu uydurmak için işlevler sağlar ve ayrıca
seyrek PCA yapmak için işlevler sağlar. LARS-EN algoritması sıfırdan
başlayarak tüm katsayılar ve uyum dizisini sağlar.

```{r}
library('elasticnet')
mdl<-enet(x=as.matrix(beton[,-9]), y=beton[,9], lambda=0, max.steps=100, normalize=TRUE,
     intercept=TRUE, trace = FALSE, eps = .Machine$double.eps)

predict(object=mdl, newx=as.matrix(beton[1:100,-9]), s=1, type = "fit",#c("fit", "coefficients"),
        mode = c("step"),naive=FALSE)#,"fraction", "norm", "penalty"
```

# 34. glmnet

Doğrusal regresyon, lojistik ve çok terimli regresyon modelleri, Poisson
regresyonu, Cox modeli, çok yanıtlı Gaussian vegruplandırılmış çok
terimli regresyon için tüm kement veya elastik ağ düzenleme yolunu
uydurmak için son derece etkili prosedürler.

```{r}
library('glmnet')
nvars<-8; dfmax = nvars + 1; nobs<-nrow(beton)
X = makeX(beton[,-9],na.impute = T,sparse = F)

#fit a GLM with lasso or elasticnet regularization
mdl<-glmnet(x = X, y=beton[,9],
       family = c("gaussian"),# "binomial", "poisson", "multinomial", "cox", "mgaussian"),
       weights = NULL, offset = NULL, alpha = 1, nlambda = 100,
       lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04), lambda = NULL,
       standardize = TRUE, intercept = TRUE, thresh = 1e-07, dfmax = nvars + 1,
       pmax = min(dfmax * 2 + 20, nvars), exclude = NULL,
       penalty.factor = rep(1, nvars), lower.limits = -Inf, upper.limits = Inf,
       maxit = 1e+05, type.gaussian = ifelse(nvars < 500,"covariance","naive"),
       type.logistic = c("Newton", "modified.Newton"),
       standardize.response = FALSE,
       type.multinomial = c("ungrouped", "grouped"), relax = FALSE,
       trace.it = 0)

predict(mdl,X[1:100,], type = "link")
```

# 35. glmpath

L1 düzenli genelleştirilmiş doğrusal modeller ve Cox orantılı tehlike
modeli için yol izleyen bir algoritma.

```{r}
library('glmpath')

n<-dim(train_adlt)[1]; m<-dim(train_adlt)[2]

#Fits the entire L1 regularization path for Cox proportional hazards model
#coxpath(data=train_adlt, nopenalty.subset = NULL, method ="efron",#"breslow"
#  lambda2 = 1e-5, max.steps = 10 * min(n, m), max.norm = 100 * m,
#  min.lambda = (if (m >= n) 1e-3 else 0), max.vars = Inf,
#  max.arclength = Inf, frac.arclength = 1, add.newvars = 1,
#  bshoot.threshold = 0.1, relax.lambda = 1e-7, approx.Gram = FALSE,
#  standardize = TRUE, eps = .Machine$double.eps, trace = FALSE)

#predict(object, data, s, type = c("coefficients", "loglik",
#  "lp", "risk", "coxph"), mode = c("step",
#  "norm.fraction", "norm", "lambda.fraction", "lambda"),
#  eps = .Machine$double.eps)

#Fits the entire L1 regularization path for generalized linear models
mdl<-glmpath(x=train_adlt[,-14], y=train_adlt[,14],# data=train_adlt,
        nopenalty.subset = NULL, family = binomial,
        weight = rep(1, n), offset = rep(0, n), lambda2 = 1e-5,
        max.steps = 10 * min(n, m), max.norm = 100 * m,
        min.lambda = (if (m >= n) 1e-6 else 0), max.vars = Inf,
        max.arclength = Inf, frac.arclength = 1, add.newvars = 1,
        bshoot.threshold = 0.1, relax.lambda = 1e-8,
        standardize = TRUE, eps = .Machine$double.eps,
        trace = FALSE)

predict(mdl, newx=test_adlt[,-14], newy=test_adlt[,14], type = "response")

```

# 37. ncvreg

Kement veya dışbükey olmayan cezalar kullanan doğrusal regresyon, GLM ve
Cox regresyon modelleri için düzenleme yollarına uyar; özellikle
minimaks içbükey ceza (MCP) ve yumuşak bir şekilde kırpılmış mutlak
sapma (SCAD) cezası ve ilave L2 cezaları seçenekleri ("elastik ağ"
fikri).

```{r}
library('ncvreg')
n<-dim(adlt)[1]; p<-dim(adlt)[2]

#ncvreg Fit an MCP- or SCAD-penalized regression path
mdl1<-ncvreg(X=adlt[,-14],y=adlt[,14],family ="binomial",
             # c("gaussian", "binomial", "poisson"),
       penalty ="MCP",# c("MCP", "SCAD", "lasso"),
       gamma = switch("MCP", SCAD = 3.7, 3),
       alpha = 1,lambda.min = ifelse(n > p, 0.001, 0.05),nlambda = 100,
       lambda= 0.01,
       eps = 1e-04,max.iter = 10000,convex = TRUE,dfmax = p + 1,
       penalty.factor = rep(1, ncol(adlt)-1),warn = TRUE,returnX=F)

#ncvsurv Fit an MCP- or SCAD-penalized survival model
data("Lung");n=137;p=9
mdl2<-ncvsurv(X=Lung$X, y=Lung$y, penalty ="MCP",# c("MCP", "SCAD", "lasso")
        gamma = switch("MCP", SCAD = 3.7, 3), alpha = 1,
        lambda.min = ifelse(n > p, 0.001, 0.05), nlambda = 100,
        lambda=0.01, eps = 1e-04, max.iter = 10000, convex = TRUE,
        dfmax = p, penalty.factor = rep(1, ncol(Lung$X)), warn = TRUE,returnX=F)

reg<-predict(object=mdl1, X=adlt[1:100,-14], type ="response")
# c("link", "response", "class", "coefficients", "vars", "nvars")

surv<-predict(object=mdl2,X=Lung$X[1:50,], type="response")
# c("link", "response", "survival", "median", "coefficients", "vars", "nvars"),
```

# 39. CatBoost

karar ağaçlarında gradyan artırma kullanan bir makine öğrenimi
algoritmasıdır.

```{r}
library(catboost)
pool<-catboost.load_pool(data=train_adlt[,-14],
                   label = train_adlt[,14],
                   cat_features = NULL,
                   column_description = NULL,
                   pairs = NULL,
                   delimiter = "\t",
                   has_header = FALSE,
                   weight = NULL,
                   group_id = NULL,
                   group_weight = NULL,
                   subgroup_id = NULL,
                   pairs_weight = NULL,
                   baseline = NULL,
                   feature_names = NULL,
                   thread_count = -1)

fit<-catboost.train(learn_pool=pool,
               test_pool = NULL,
               params = list(loss_function = 'Logloss',
                             iterations = 100,
                             ignored_features=4#education_num safdışı
                             )
               )
test_pool<-catboost.load_pool(data=test_adlt[,-14],
                              label = test_adlt[,14])
catboost.predict(model=fit,
                 pool=test_pool,
                 verbose=FALSE,
                 prediction_type="Class",
                 ntree_start=0,
                 ntree_end=0,
                 thread_count=-1) #(the number of threads is equal to the number of processor cores)
#catboost.get_feature_importance(model=fit,
#                                pool = NULL,
#                                type = 'FeatureImportance',
#                                thread_count = -1)

#catboost.get_object_importance(model=fit,
#                               pool=test_pool,
#                               train_pool=pool,
#                               top_size = -1,
#                               type = 'Average',
#                               update_method = 'SinglePoint',
#                               thread_count = -1)

#catboost.get_model_params(model=fit)


#------------------------------------


library(caret)#fit control and grid için

titanic_train=read.csv("./ML_GlobalAI/DecisionTrees_titanic.csv", sep = ",")
x<-titanic_train[,-1]
y<-titanic_train[,1]

fit_control <- trainControl(method = "cv",
                            number = 4,
                            classProbs = TRUE)

grid <- expand.grid(depth = c(4, 6, 8),
                    learning_rate = 0.1,
                    iterations = 100,
                    l2_leaf_reg = 1e-3,
                    rsm = 0.95,
                    border_count = 64)

report <- train(x, as.factor(make.names(y)),
                method = catboost.caret,
                logging_level = 'Verbose', preProc = NULL,
                tuneGrid = grid, trControl = fit_control)

print(report)

importance <- varImp(report, scale = FALSE)
print(importance)
```

# 40. bartMachine

Bayesian Toplamalı Regresyon Ağaçlarının veri analizi ve
görselleştirmeye yönelik genişletilmiş özelliklere sahip gelişmiş bir
uygulaması

```{r}
library('bartMachine')
##options(java.parameters = "-Xmx8000m")#java RAM problemi için

#Builds a BART model for regression or classification.
bartMachine(X = adult[,-c(3,4,15)], y = adult[,15], Xy = NULL, num_trees = 50,
            num_burn_in = 250, num_iterations_after_burn_in = 1000,
            alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3,
            prob_rule_class = 0.5, mh_prob_steps = c(2.5, 2.5, 4)/9,
            debug_log = FALSE, run_in_sample = TRUE, s_sq_y = "mse",
            sig_sq_est = NULL, print_tree_illustrations = FALSE,
            cov_prior_vec = NULL, interaction_constraints = NULL,
            use_missing_data = FALSE, covariates_to_permute = NULL,
            num_rand_samps_in_library = 10000,
            use_missing_data_dummies_as_covars = FALSE,
            replace_missing_data_with_x_j_bar = FALSE,
            impute_missingness_with_rf_impute = T,
            impute_missingness_with_x_j_bar_for_lm = TRUE,
            mem_cache_for_speed = TRUE, flush_indices_to_save_RAM = TRUE,
            serialize = FALSE, seed = NULL,verbose = TRUE)

build_bart_machine(X = adult[,-c(3,4,15)], y = adult[,15], Xy = NULL,
                   num_trees = 50, num_burn_in = 250,
                   num_iterations_after_burn_in = 1000, alpha = 0.95, beta = 2,
                   k = 2, q = 0.9, nu = 3, prob_rule_class = 0.5,
                   mh_prob_steps = c(2.5, 2.5, 4)/9, debug_log = FALSE,
                   run_in_sample = TRUE, s_sq_y = "mse", sig_sq_est = NULL,
                   print_tree_illustrations = FALSE, cov_prior_vec = NULL,
                   interaction_constraints = NULL, use_missing_data = FALSE,
                   covariates_to_permute = NULL,
                   num_rand_samps_in_library = 10000,
                   use_missing_data_dummies_as_covars = FALSE,
                   replace_missing_data_with_x_j_bar = FALSE,
                   impute_missingness_with_rf_impute = FALSE,
                   impute_missingness_with_x_j_bar_for_lm = TRUE,
                   mem_cache_for_speed = TRUE, flush_indices_to_save_RAM = TRUE,
                   serialize = FALSE, seed = NULL, verbose = TRUE)

bart_predict_for_test_data(bart_machine, Xtest, ytest, prob_rule_class = NULL)
plot()
```

# 41. BayesTree

Bu BART:Bayesian Katkısal Regresyon Ağaçlarının bir uygulamasıdır

```{r}
library('BayesTree')
#makeind(adult,all=T) #faktör değişkenleri dummy kolona çevirir

mdl<-bart(x.train=train_adlt[,-14], y.train=train_adlt[,14],
          x.test=matrix(0.0,22792,13), sigest=NA, sigdf=2, sigquant=.9, k=2.0,
          power=2.0, base=.95,binaryOffset=0, ntree=200, ndpost=1000, nskip=100,
          printevery=100, keepevery=1, keeptrainfits=TRUE, usequants=FALSE,
          numcut=100, printcutoffs=0, verbose=TRUE)

summary(mdl)
```

# 42. C50

Quinlan'ın C5.0 algoritmasını kullanarak sınıflandırma ağacı modellerine
veya kural tabanlı modellere uyun

```{r}
library('C50')
mdl<-C5.0(x=train_adlt[,-14], y=factor(train_adlt[,14]),trials = 1,
          rules = FALSE, weights = NULL, control = C5.0Control(), costs = NULL)

summary(mdl)

predict(object=mdl, newdata = test_adlt[,-14], trials = mdl$trials["Actual"],
        type = "class", na.action = na.pass)

```

# 43. cubist

Bu işlev, eğitim setindeki en yakın komşulara dayalı ek düzeltmelerle
birlikte Quinlan (1992) (diğer adıyla M5)'te açıklanan kural tabanlı
modele uyar.

```{r}
library('Cubist')
mdl<-cubist(x=beton[,-9], y=beton[,9], committees = 1,control = cubistControl(),
       weights = NULL)
predict(object=mdl, newdata = beton[1:50,-9], neighbors = 0)
```

# 44. dipm

DIPM yöntemi, belirli bir tedavi grubunda özellikle zayıf veya güçlü
performansa sahip alt grupları arayan bir sınıflandırma ağacıdır. Depth
Importance in Precision Medicine (DIPM) ----SIKINTILI----

```{r}
library('dipm')
lung<-survival::lung
lung1<-na.omit(lung)
lung1$status<- lung1$status-1
lung1$sex<- lung1$sex-1
#Bu işlev, belirli bir tedavi grubunda deneklerin özellikle iyi veya özellikle kötü performans gösterdiği alt grupları tanımlamak için tasarlanmış bir sınıflandırma ağacı oluşturur.
dipm(formula=Surv(time, status) ~ sex | ., data = lung1)
     #types = "nominal",
     ## "response", "treatment", "status", "binary", "ordinal", "nominal"
     #nmin = 5, nmin2 = 5, ntree = ceiling(min(max(sqrt(228), sqrt(10)), 1000)),
     #mtry = Inf, maxdepth = 2, maxdepth2 = 6, print = TRUE,
     #dataframe = FALSE, prune = FALSE)

#Bu işlev, belirli bir tedavi grubunda deneklerin özellikle iyi veya özellikle kötü performans gösterdiği alt grupları tanımlamak için tasarlanmış bir sınıflandırma ağacı oluşturur.
spmtree(formula=Surv(time, status) ~ sex | ., data = lung1)
        #types = NULL, nmin = 5, maxdepth = Inf, print = TRUE,
        #dataframe = FALSE, prune = FALSE)
```

# 45. earth

Friedman'ın "Çok Değişkenli Uyarlanabilir Regresyon Splineları" ve
"Hızlı MARS" makalelerindeki teknikleri kullanarak bir regresyon modeli
oluşturun

```{r}
library('earth')
mdl<- earth(formula = csMPa~., data = beton,  weights = NULL, wp = NULL,
      subset = NULL, #na.action = na.fail,
      pmethod = "backward", #c("none", "exhaustive", "forward", "seqrep", "cv"),
      keepxy = FALSE, trace = 0, glm = NULL, degree = 1, nprune = NULL,nfold=0,
      ncross=1, stratify=TRUE,varmod.method = "none", varmod.exponent = 1,
      varmod.conv = 1, varmod.clamp = .1, varmod.minspan = -3,Scale.y = NULL)

predict(object = mdl, newdata = beton[1:50,-9],
        type = "response") #c("link", "earth", "class", "terms"),
        #interval = "none", level = .95,thresh = .5, trace = FALSE)
```

# 46. evtree

optimum sınıflandırma ve regresyon ağaçlarını öğrenmek için evrimsel bir
algoritma uygular, CART alternatifidir

```{r}
library('evtree')
evtree(formula=csMPa~., data = beton, subset=NULL, na.action=na.fail,
       weights=NULL, control = evtree.control(seed = 1))
```

# 47. grplasso

Grup lasso cezasına sahip, kullanıcı tarafından belirlenen (GLM-)
modellere uyar

```{r}
library('grplasso')
train_adlt[,14]<-train_adlt[,14]-1
mdl<-grplasso(formula=income~., data = data.frame(train_adlt), nonpen = ~ 1,
              weights=NULL, subset=NULL, na.action=na.fail, lambda=20,
              coef.init=1:14, penscale = sqrt, model = LogReg(), center = TRUE,
              standardize = TRUE, control = grpl.control(), contrasts = NULL)

predict(object=mdl, newdata=test_adlt[,-14], type = "response",#"link"
        na.action = na.pass)
```

# 50. lars

Tek bir en küçük kareler uyumu maliyetiyle tüm bir kement dizisinin
yerleştirilmesi için etkili prosedürler. En küçük açı regresyonu ve
sonsuz küçük ileri aşamalı regresyon, aşağıdaki makalede açıklandığı
gibi kementle ilgilidir. Gürültülü veride hatalı sonuç verir.

```{r}
library('lars')

lars(x=scale(life[,-9]), y=life[,9], type = "lasso",#lar,forward.stagewise,stepwise
     trace = FALSE, normalize = FALSE, intercept = TRUE,
     Gram = NULL, eps = 1e-12, max.steps=9, use.Gram = FALSE)

predict(object, newx, s, type = c("fit", "coefficients"),
        mode = c("step","fraction", "norm", "lambda"), ...)
```

# 51. LiblineaR

Büyük ölçekli düzenli doğrusal sınıflandırma ve regresyonu çözmek için
basit bir kütüphane. Şu anda L2-düzenlenmiş sınıflandırmanın (lojistik
regresyon, L2-kayıp doğrusal SVM ve L1-kayıp doğrusal SVM gibi) yanı
sıra L1-düzenlileştirilmiş sınıflandırmayı (L2-kayıp doğrusal SVM ve
lojistik regresyon gibi) ve L2-düzenlenmiş destek vektörünü
desteklemektedir. regresyon (L1- veya L2 kaybıyla). LiblineaR'ın ana
özellikleri arasında çok sınıflı sınıflandırma (bire karşı geri kalanı
ve Crammer & Singer yöntemi), model seçimi için çapraz doğrulama,
olasılık tahminleri (yalnızca lojistik regresyon) veya dengesiz veriler
için ağırlıklar yer alır.

```{r}
library('LiblineaR')
md<-LiblineaR(data=beton,target=beton[,9],type = 12,cost = 1,epsilon = 0.1,
          svr_eps = 0.1,bias = 1,wi = NULL,cross = 0,verbose = FALSE,
          findC = FALSE,useInitC = TRUE)
mdl<-LiblineaR(data=train_adlt,target=train_adlt[,14],type = 0,cost = 1,
          epsilon = 0.01, svr_eps = NULL,bias = 1, wi = NULL,cross = 0,
          verbose = FALSE,findC = FALSE,useInitC = TRUE)

predict(object=md, newx=beton[1:50,], proba = FALSE, decisionValues = FALSE)
predict(object=mdl, newx=test_adlt, proba = FALSE, decisionValues = FALSE)
```

# 52. lightgbm

Yüksek Verimli Gradyan Artırma Karar Ağacı. Herkese açık veri kümeleri
üzerinde yapılan karşılaştırma deneyleri, 'LightGBM'nin, önemli ölçüde
daha düşük bellek tüketimiyle hem verimlilik hem de doğruluk açısından
mevcut güçlendirme çerçevelerinden daha iyi performans gösterebileceğini
gösteriyor.

```{r}
library('lightgbm')
mdl<-lightgbm(data=train_adlt[,-14],label = train_adlt[,14],weights = NULL,
         params = list(), nrounds = 100L, verbose = 1L, eval_freq = 1L,
         early_stopping_rounds = NULL, init_model = NULL,
         callbacks = list(), serializable = TRUE, objective = "auto",
         init_score = NULL, num_threads = 2)

predict(object=mdl, newdata=test_adlt[,-14], type = "binary",
    start_iteration = NULL,num_iteration = NULL,header = FALSE,params = list())
```

# 53. naivebayes

Naive Bayes sınıflandırıcısının bu uygulamasında aşağıdaki sınıf koşullu
dağılımları mevcuttur: Bernoulli, Kategorik, Gaussian, Poisson ve
Çekirdek Yoğunluğu Tahmini yoluyla tahmin edilen sınıf koşullu
yoğunluğunun parametrik olmayan temsili. Uygulanan sınıflandırıcılar
eksik verileri yönetir ve seyrek verilerden yararlanabilir.

```{r}
library('naivebayes')
m1<-bernoulli_naive_bayes(x=train_adlt[,-14], y=factor(train_adlt[,14]),
                          prior = NULL, laplace = 0)
m2<-gaussian_naive_bayes(x=train_adlt[,-14], y=factor(train_adlt[,14]),
                         prior = NULL)
m3<-multinomial_naive_bayes(x=train_adlt[,-14], y=factor(train_adlt[,14]),
                            prior = NULL, laplace = 0.5)
m4<-naive_bayes(x=train_adlt[,-14], y=factor(train_adlt[,14]), prior = NULL,
                laplace = 0,usekernel = FALSE, usepoisson = FALSE)
m5<-nonparametric_naive_bayes(x=train_adlt[,-14], y=factor(train_adlt[,14]),
                              prior = NULL)
m6<-poisson_naive_bayes(x=train_adlt[,-14], y=factor(train_adlt[,14]),
                        prior = NULL, laplace = 0)

predict(object=m1, newdata = test_adlt[,-14], type = "class")
predict(object=m2, newdata = test_adlt[,-14], type = "class")
predict(object=m3, newdata = test_adlt[,-14], type = "class")
predict(object=m4, newdata = test_adlt[,-14], type = "class")
predict(object=m5, newdata = test_adlt[,-14], type = "class")
predict(object=m6, newdata = test_adlt[,-14], type = "class")
```

# 54. OneR

İyileştirmelerle Tek Kurallı Makine Öğrenimi Sınıflandırma Algoritması

```{r}
library('OneR')
mdl<-OneR(formula=income~., data=data.frame(train_adlt), ties.method = "first",
     verbose = FALSE)#"chisq"

#Bir veri çerçevesindeki tüm sayısal verileri, kesme noktalarının hedef kategorilerle en iyi şekilde hizalandığı kategorik bölmelere ayırır, böylece bir faktör döndürülür. Bir OneR modeli oluştururken bu, daha fazla doğruluğa sahip daha az kuralla sonuçlanabilir.
data_bin<-optbin(formula=income~., data=adult, method = "logreg",
       na.omit = TRUE)#, "infogain","naive")
OneR(income~., data_bin)


p<-predict(object=mdl, newdata=data.frame(test_adlt[,-14]), type = "class")
eval_model(prediction=p, actual=test_adlt[,14],
           dimnames = c("Prediction", "Actual"), zero.print = "0")
```

# 55. penalizedLDA

Bu paket, özellik sayısının (p) gözlem sayısını (n) aştığı yüksek
boyutlu ortam için tasarlanmış, cezalandırılmış doğrusal diskriminant
analizi gerçekleştirir.

```{r}
library('penalizedLDA')

mdl<-PenalizedLDA(x=adlt[50:60,-13], y=adlt[50:60,13], xte=NULL,
             type = "standard", lambda=.1, K = 1, chrom =NULL, lambda2 = NULL,
             standardized = FALSE, wcsd.x = NULL, ymat = NULL, maxiter = 20,
             trace=FALSE)
#not:satır sayısı sütun sayısından az olacak ve her sütunda en az bir rakam olacak aralık seçildi

predict(object=mdl, xte=adlt[93:101,-13])

```

# 56. picasso

Bu paket, genelleştirilmiş doğrusal modeli dışbükey ve dışbükey olmayan
cezaya uydurmak için hesaplama açısından verimli araçlar sağlar.
Kullanıcılar, l1 ve ridge gibi dışbükey cezalara kıyasla önemli ölçüde
daha az tahmin hatası ve fazla uyum sağlayan SCAD ve MCP gibi dışbükey
olmayan cezaların üstün istatistiksel özelliğinden yararlanabilirler.
Hesaplama, çok aşamalı dışbükey gevşeme ve sıcak başlangıç başlatma,
aktif küme güncelleme ve hesaplamayı artırmak için koordinat ön seçimi
için güçlü kuraldan yararlanan ve benzersiz bir seyrek yerel optimuma
doğrusal bir yakınsama elde eden Yol Yönünde Kalibre Edilmiş Seyrek
Çekim algoritması (PICASSO) tarafından gerçekleştirilir. Optimum
istatistiksel özelliklere sahip. Hesaplama, seyrek matris çıktısı
kullanılarak bellek açısından optimize edilmiştir.

```{r}
library('picasso')
mdl<-picasso(X=train_adlt[,-13], Y=train_adlt[,13], lambda = NULL, nlambda = 100,
        lambda.min.ratio =0.05, family = "binomial",#gaussian,sqrtlasso,poisson
        method = "l1", # "mcp", "scad"
        type.gaussian = "naive", #"covariance".
        gamma = 3, df = NULL,standardize = TRUE, intercept = TRUE,
        prec = 1e-07,max.ite = 1000, verbose = FALSE)

predict(object=mdl, newdata=test_adlt[1:100,-13], lambda.idx = c(1:3),
        p.pred.idx = c(1:5))
```

# 57. pre

Tahmin kuralı topluluklarını (PRE'ler) türetir. Büyük ölçüde, Friedman
ve Popescu (2008)'da açıklanan PRE'lerin türetilmesi prosedürünü,
ayarlamalar ve iyileştirmelerle takip etmektedir. Ana işlev pre(),
sürekli, ikili, sayım, çok terimli ve çok değişkenli sürekli yanıtlar
için kurallardan ve/veya doğrusal terimlerden oluşan tahmin kural
topluluklarını türetir. gpe() işlevi, tahmin değişkenlerinin
kurallarından, menteşe ve doğrusal işlevlerinden oluşan genelleştirilmiş
tahmin topluluklarını türetir.

```{r}
library('pre')
#Genel Tahmin Topluluğu (gpe) Türetme
mdl1<-gpe(formula=income~., data=data.frame(train_adlt),
    base_learners = list(gpe_trees(), gpe_linear()),
    weights = rep(1, times = nrow(train_adlt)),sample_func = gpe_sample(),
    verbose = FALSE, penalized_trainer = gpe_cv.glmnet(),model = TRUE)

#Bir tahmin kuralı topluluğu türetin
mdl2<-pre(formula=income~., data=data.frame(train_adlt))#, family = "binomial")
    #use.grad = TRUE, weights=NULL, type = "both", sampfrac = 0.5, maxdepth = 3L,
    #learnrate = 0.01, mtry = Inf, ntrees = 500, confirmatory = NULL,
    #singleconditions = FALSE, winsfrac = 0.025, normalize = TRUE,
    #standardize = FALSE, ordinal = TRUE, nfolds = 10L,
    #tree.control=rpart_control(), #????
    #tree.unbiased = TRUE, removecomplements = TRUE,
    #removeduplicates = TRUE, verbose = FALSE, par.init = FALSE,
    #par.final = FALSE, sparse = FALSE)

predict(object=mdl1, newdata = data.frame(test_adlt[,-13]),type = "link",
        penalty.par.val = "lambda.1se")
predict(object=mdl2, newdata = data.frame(test_adlt[,-13]),type = "link",
        penalty.par.val = "lambda.1se")
```

# 58. quantregForest

Nicelik Regresyon Ormanları, koşullu niceliklerin tahminine yönelik ağaç
tabanlı bir topluluk yöntemidir. Özellikle yüksek boyutlu veriler için
çok uygundur. Karma sınıfların tahmin değişkenleri ele alınabilir.
Paket, 'randomForest' paketine bağlıdır.

```{r}
library('quantregForest')
m<-quantregForest(x=adult[,-13],y=adult[,13], nthreads=1, keep.inbag=FALSE)
predict(m, adult[1:100,-13])
```

# 59. ranger

Özellikle yüksek boyutlu veriler için uygun olan Rastgele Ormanların
hızlı bir uygulaması. Sınıflandırma, regresyon, hayatta kalma ve
olasılık tahmin ağaçlarından oluşan topluluklar desteklenmektedir. Genom
çapında ilişkilendirme çalışmalarından elde edilen veriler verimli bir
şekilde analiz edilebilir.

```{r}
library('ranger')
m<-ranger(formula = income~., data = train_adlt)#, num.trees = 500, mtry = NULL,
       #importance = "none", write.forest = TRUE, probability = FALSE,
       #min.node.size = 1, min.bucket = 1, max.depth = NULL,
       #replace = TRUE, sample.fraction = ifelse(replace, 1, 0.632),#
       #case.weights = NULL, class.weights = NULL, splitrule = "gini",
       #num.random.splits = 1, alpha = 0.5, minprop = 0.1,
       #split.select.weights = NULL, always.split.variables = NULL,
       #respect.unordered.factors = NULL, scale.permutation.importance = FALSE,
       #local.importance = FALSE, regularization.factor = 1,
       #regularization.usedepth = FALSE, keep.inbag = FALSE, inbag = NULL,
       #holdout = FALSE, quantreg = FALSE, time.interest = NULL,##
       #oob.error = TRUE, num.threads = NULL, save.memory = FALSE,
       #verbose = TRUE, node.stats = TRUE, seed = NULL,
       #dependent.variable.name = NULL, status.variable.name = NULL,
       #classification = TRUE)#, x = NULL, y = NULL)

(predict(object=m, data = test_adlt[,-13], predict.all = FALSE,
        num.trees = m$num.trees, type = "response", se.method = "infjack",
        quantiles = c(0.1, 0.5, 0.9), what = NULL, seed = NULL,
        num.threads = NULL, verbose = TRUE))$predictions
```

# 61. RLT

Regresyon, sınıflandırma ve hayatta kalma analizi için çeşitli ek
özelliklere sahip rastgele orman.

```{r}
library('RLT')
x=train_adlt[,-13];reinforcement = FALSE;nmin = max(1, as.integer(log(nrow(x))))
#Reinforcement Learning Trees-Takviyeli Öğrenme Ağaçları
mdl<-RLT(x=x, y=train_adlt[,13], censor = NULL,
    model = "regression",#regression, classification or survival
    print.summary = 0, use.cores = 1,
    ntrees = if (reinforcement) 100 else 500,
    mtry = max(1, as.integer(ncol(x)/3)),
    nmin = max(1, as.integer(log(nrow(x)))), alpha = 0.4, split.gen = "random",
    nsplit = 1, resample.prob = 0.9, replacement = TRUE, npermute = 1,
    select.method = "var", subject.weight = NULL, variable.weight = NULL,
    track.obs = FALSE, importance = TRUE, reinforcement = FALSE, muting = -1,
    muting.percent = if (reinforcement) MuteRate(nrow(x), ncol(x),
                                                 speed = "aggressive",
                                                 info = FALSE) else 0,
    protect = as.integer(log(ncol(x))),combsplit = 1,combsplit.th = 0.25,
    random.select = 0,embed.n.th = 4 * nmin,
    embed.ntrees = max(1, -atan(0.01 * (ncol(x) - 500))/pi * 100 + 50),
    embed.resample.prob = 0.8, embed.mtry = 1/2,
    embed.nmin = as.integer(nrow(x)^(1/3)), embed.split.gen = "random",
    embed.nsplit = 1)

predict(object=mdl, testx=test_adlt[,-13])$prediction

```

# 62. rminer

Kısa ve tutarlı bir işlevler kümesi sunarak sınıflandırma ve regresyon
(zaman serisi tahmini dahil) görevlerinde veri madenciliği
algoritmalarının kullanımını kolaylaştırır. fonksiyonları tamamen diğer
paketlerden çağırır. ----PEKÇOK PAKETİN DERLENMİŞ HALİ ANCAK HATAYA ÇOK
FAZLA AÇIK----

```{r}
library('rminer')

# train test data split (H$tr, H$ts)
H<-holdout(y=adlt[,13], ratio = 2/3, internalsplit = FALSE, mode = "stratified",
           #stratified,order,rolling,incremental,random
        iter = 1, seed = NULL, window=10, increment=1)

mdl<-fit(x=income~., data = adlt[H$tr,], model = "kknn", task = "default",
    search = "heuristic", mpar = NULL, feature = "none", scale = "default",
    transform = "none", created = NULL, fdebug = FALSE)


predict(mdl, adlt[H$ts,-13])
```

MODEL: naive, ctree, cv.glm.net, rpart, kknn, knn, ksvm, lssvm, mlp,
mlpe, randomForest, xgboost, bagging, boosting, lda, multinom(lr),
naiveBayes, qda, cubist, lm, mr, mars, pcr, plsr, cppls, rvm TASK: prob,
class, reg, "default" Not: imputation özelliği sadece sayısal verilerde
geçerli

# 63. tree

Sınıflandırma ve Regresyon Ağaçları

```{r}
library('tree')
frm <- as.formula(paste("income ~ ",
                        paste(names(train_adlt[,-13]), collapse= "+")))
#ctrl<-tree.control(nobs=NULL, mincut = 5, minsize = 10, mindev = 0.01)

mdl<-tree(formula=frm, data=train_adlt)
     #, weights=NULL, subset=NULL,na.action = na.pass,
     #control = ctrl,
     #method = "recursive.partition",#"model.frame"
     #split = c("deviance"), model = FALSE,#, "gini"
     #x = FALSE, y = TRUE, wts = FALSE)

predict(object=mdl, newdata = test_adlt[,-13], type = c("vector"),
        #"tree", "class", "where"),
        split = FALSE, nwts, eps = 1e-3)

```

# 64. xgboost

Paket, verimli doğrusal model çözücü ve ağaç öğrenme algoritmalarını
içerir. Regresyon, sınıflandırma ve sıralama dahil olmak üzere çeşitli
amaç işlevlerini destekler.

```{r}
library('xgboost')

#xgb.DMatrix nesnesini oluştur:xgb.train için
dtrain<-xgb.DMatrix(data=as.matrix(adlt[,-13]),label=adlt[,13]-1,info = NULL,
                    missing = NA,silent = FALSE,nthread = 2)

#PARAMETERS
global_param<-list(verbosity      =0,
                   use_rmm        =FALSE,
                   objective      ="binary:logistic",
                   base_score     =0.5,
                   eval_metric    =list("error","logloss"))#list("rmse","auc")

tree_param<-list(booster                ="gbtree",
                 eta                    =1,
                 gamma                  =0,
                 max_depth              =6,
                 min_chil_weight        =1,
                 subsample              =1,
                 colsample_bytree       =1,
                 lambda                 =1,
                 alpha                  =0,
                 num_parallel_tree      =1,
                 monotone_constraints   =0,
                 interaction_constraints="[[0,1],[2,3]]")

#linear_param<-list(booster="gblinear",lambda=0,alpha=0) LINEAR REGRESSION

#xgboost işlevi xgb.train için daha basit bir sarmalayıcıdır
 #matrix, dgCMatrix, xgb.DMatrix
basic<-xgboost(data = as.matrix(train_adlt[,-13]),
        label = train_adlt[,13]-1, #yanıt değişkeni 0-1 olmalı
        nrounds = 2,
        missing = FALSE,
        weight = NULL,
        params = append(global_param,tree_param),
        verbose = 1,
        print_every_n = 1L,
        early_stopping_rounds = NULL,
        maximize = NULL,
        save_period = NULL,
        save_name = "xgboost.model",
        xgb_model = NULL,
        callbacks = NULL)

#xgb.train, bir xgboost modelinin eğitimi için gelişmiş bir arayüzdür
dtest<-xgb.DMatrix(data=as.matrix(adlt[1:1000,-13]),label=adlt[1:1000,13]-1)
watchlist <- list(train=dtrain, test=dtest) #bunlar opsiyonel

prof<-xgb.train(
  params = append(global_param, tree_param),
  data = dtrain,
  nrounds = 2,
  watchlist = watchlist,#NULL,
  obj = NULL,
  feval = NULL,
  verbose = 1,
  print_every_n = 1L,
  early_stopping_rounds = NULL,
  maximize = NULL,
  save_period = NULL,
  save_name = "xgboost.model",
  xgb_model = NULL,
  callbacks = NULL#list()
)

p1<-predict(object=basic,
  newdata=as.matrix(test_adlt[,-13]),
  missing = NA,
  outputmargin = FALSE,
  ntreelimit = NULL,
  predleaf = FALSE,
  predcontrib = FALSE,
  approxcontrib = FALSE,
  predinteraction = FALSE,
  reshape = FALSE,
  training = FALSE,
  iterationrange = NULL,
  strict_shape = FALSE
)

p2<-predict(prof, as.matrix(test_adlt[,-13]))

mean(as.numeric(p1 > 0.5) != test_adlt[,13])
mean(as.numeric(p2 > 0.5) != test_adlt[,13])

##getinfo(dtest, "label")
##xgb.importance(model = basic)
##xgb.plot.importance(xgb.importance(model = basic))
##xgb.plot.tree(model = basic)
##xgb.plot.shap.summary(data=as.matrix(test_adlt[,-13]), model = basic)
##xgb.plot.shap(data=as.matrix(test_adlt[,-13]),  model = basic)
```

OBJECTIVE:reg:squarederror,reg:squaredlogerror,reg:logistic,reg:pseudohubererror
binary:logistic,binary:logitraw,binary:hinge,count:poisson,survival:cox
survival:aft,aft_loss_distribution,multi:softmax,multi:softprob,rank:pairwise
rank:ndcg,rank:map,reg:gamma,reg:tweedie

# 66. Boruta

öznitellik seçimi sarmalayıcı algoritması. Orijinal niteliklerin önemini
rastgele elde edilebilen önemle karşılaştırarak, bunların değiştirilmiş
kopyaları (shado) kullanılarak tahmin edilerek ilgili özellikleri bulur.

```{r}
library('Boruta')
ss<-Boruta(x=adlt[,-13],y=adlt[,13],pValue = 0.01,mcAdj = TRUE,maxRuns = 100,
       doTrace = 0, holdHistory = TRUE, getImp = getImpRfZ)
getImpLegacyRfZ(x=adlt[,-13],y=adlt[,13])
getImpRfZ(x=adlt[,-13],y=adlt[,13], ntree = 500, num.trees = 500)
```

# 67. DALEX

DALEX paketi, girdi değişkenleri ile model çıktısı arasındaki bağlantıyı
anlamaya yardımcı olan çeşitli yöntemler içerir. Uygulanan yöntemler,
modeli tek bir örnek düzeyinde ve tüm veri kümesi düzeyinde keşfetmeye
yardımcı olur.

```{r}
library('DALEX')
#Bu işlev, açıklamalar için işlevler tarafından daha fazla işlenebilecek bir modelin birleşik bir temsilini oluşturur.
ex<-explain(model=mdl,data = adlt[,-13],y = adlt[,13],predict_function = NULL,
        predict_function_target_column = NULL,residual_function = NULL,
        weights = NULL,label = NULL,verbose = TRUE,precalculate = TRUE,
        colorize = !isTRUE(getOption("knitr.in.progress")),model_info = NULL,
        type = "classification")

#Bu, açıklayıcı nesneler için çalışan genel bir predict() işlevidir.
model_prediction(explainer=ex, new_data=test_adlt[,-13])

#Bu genel işlev, kullanıcının model hakkındaki temel bilgileri çıkarmasına olanak tanır
model_info(model=mdl, is_multiclass = FALSE)

#Bu fonksiyon artıkların model teşhisini gerçekleştirir. Artıklar tahminlere, gerçek y değerlerine veya seçilen değişkenlere göre hesaplanır ve grafiği çizilir
model_diagnostics(explainer=ex, variables = NULL)

#bu işlev feature_importance'ı çağırır
model_parts(explainer=ex, loss_function = loss_default(ex$model_info$type),
            type = "variable_importance",N = 1000,n_sample = 1000)

#sınıflandırma ve regresyon modelleri için çeşitli performans ölçümlerini hesaplar
model_performance(explainer=ex, cutoff = 0.5)

#model yanıtını araştıran bir veri kümesi düzeyindeki açıklamaları hesaplar
model_profile(explainer=ex, variables = NULL, N = 100, groups = NULL,
              k = NULL, center = TRUE, type = "partial")

predict_diagnostics(explainer=ex, new_observation = adlt[1:100,-13])
predict_parts(explainer=ex, new_observation = adlt[1:100,-13])
predict_profile(explainer = ex, new_observation = adlt[1:100,-13])
#shap_aggregated(explainer = ex, new_observation = adlt[1:100,-13])
#variable_effect(explainer = ex, new_observation = adlt[1:100,-13])
```

işlenecek modellers: model info • class cv.glmnet and glmnet - models
created with glmnet package • class glm - generalized linear models •
class lrm - modelscreated with rms package, • class model_fit - models
created with parsnip package • class lm - linear models created with
stats::lm • class ranger - models created with ranger package • class
randomForest-random forest models created with randomForest package •
class svm -support vector machines models created with the e1071 package
• class train - models created with caret package • class gbm - models
created with gbm package

# 68. GMMBoost

Bu paket, Genelleştirilmiş karma modeller için olasılığa dayalı
güçlendirme yaklaşımları sağlar

```{r}
library('GMMBoost')
data(knee)
#Yarı parametrik bir karma modeli veya genelleştirilmiş bir yarı parametrik karma modeli takın.
bGAMM(fix=pain ~ time + th,
      add= ~ age + sex,
      rnd= list(id=~1),
      data=knee, lambda=1e+5, family =  poisson(link = log),
      control = bGAMMControl(nue=0.1,add.fix=NULL,start=NULL,q_start=NULL,
                             OPT=TRUE,nbasis=20,spline.degree=3,diff.ord=2,
                             sel.method="aic",steps=500,method="EM",
                             overdispersion=FALSE)
      )

#Doğrusal bir karma modeli veya genelleştirilmiş bir doğrusal karma modeli takın.
bGLMM(fix=pain ~ time + th + I(age^2), rnd=list(id=~1+sex), data=knee,
      family =  poisson(link = log), 
      control = bGLMMControl(nue=0.1, lin="(Intercept)", start=NULL,
                             q_start=NULL, OPT=TRUE,sel.method="aic", steps=500,
                             method="REM",overdispersion=FALSE,print.iter=TRUE)
      )

#Sıralı yanıtlı genelleştirilmiş bir doğrusal karma modeli takın
OrdinalBoost(fix=pain ~ time + th + age + sex, rnd= list(id=~1), data=knee,
             model="sequential",
             control = OrdinalBoostControl(nue=0.1, lin=NULL, katvar=NULL,
                                           start=NULL, q_start=NULL,OPT=TRUE,
                                           sel.method="aic", steps=100,
                                           method="REML", maxIter=500,
                                           print.iter.final=FALSE,
                                           eps.final=1e-5)
             )
```

# 69. MASS

birçok veri ve analiz yöntemi içerir. glmnb, glmmpql, lmgls, lmridge,
loglm, lqs ve negativeBinomial,polr, rlm modeller için çözüm sunar

```{r}
library(MASS)

#Negatif Binom genelleştirilmiş doğrusal modeli için ek parametre tetanın tahminini içerecek şekilde sistem fonksiyonu glm()'nin bir modifikasyonu.
mdl1<-glm.nb(formula = csMPa ~ cement + slag + flyash + water /(superplasticizer +
                 coarseaggregate + fineaggregate + age), 
       data = beton, weights=NULL, subset=NULL,
       na.action=na.omit,
      # start = NULL, etastart=1, mustart=1,
       control = glm.control(epsilon = 1e-8, maxit = 25, trace = FALSE),
       method = "glm.fit",model = TRUE, x = FALSE, y = TRUE, contrasts = NULL,
       init.theta=10, link = log #log, sqrt or identity
      )

#Cezalandırılmış Yarı Olabilirliği kullanarak çok değişkenli normal rastgele efektlere sahip bir GLMM modelini yerleştirin
mdl2<-glmmPQL(fixed = csMPa~cement+slag+flyash+water+superplasticizer+coarseaggregate+I(age),
        random = ~1|fineaggregate, family = "poisson", data = beton,
        correlation = NULL, weights = NULL, control = poisson(link = "log"),
        niter = 10, verbose = TRUE)

#lda Doğrusal diskriminant analizi.
mdl3<-lda(formula=Species~., data=iris, prior=c(1,1,1)/3, subset=NULL, na.action=na.pass)

#lm.gls Doğrusal modelleri Genelleştirilmiş En Küçük Karelere göre sığdır
#mdl4<-lm.gls(formula=csMPa~., data=beton, W=1030, subset=NULL, na.action=na.pass,
 #      inverse = FALSE, method = "lm.fit", model = FALSE, x = FALSE, y = FALSE,
  #     contrasts = NULL)

#lm.ridge Sırt regresyonuyla doğrusal bir model yerleştirin
mdl5<-lm.ridge(formula=csMPa~., data=beton, subset=NULL, na.action=na.omit,
         lambda = 0, model = FALSE,
         x = FALSE, y = FALSE, contrasts = NULL)

#Veri kümesindeki iyi noktalara bir regresyon uydurun, böylece yüksek kırılma noktasına sahip bir regresyon tahmincisi elde edin. lmsreg ve ltsreg uyumluluk sarmalayıcılardır.
mdl6<-lqs(formula=csMPa~., data=beton, method = "lts",#"lqs","lms","S","model.frame"
    subset=NULL, na.action=na.omit, model = TRUE,x.ret = FALSE, y.ret = FALSE,
    contrasts = NULL)


#Bir lojistik veya probit regresyon modelini sıralı bir faktör yanıtına uyar. Varsayılan lojistik durum, orantısal olasılık lojistik regresyonudur ve fonksiyon buna göre adlandırılır.
#mdl7<-polr(formula=Species~., data=iris, weights=NULL,# start="zeta", subset,
 #    na.action=na.omit, contrasts = NULL,
  #   Hess = FALSE, model = TRUE,
   #  method = "logistic")# "probit", "loglog", "cloglog", "cauchit"

#qda İkinci dereceden diskriminant analizi
mdl8<-qda(formula=factor(Species)~., data=iris, subset=NULL, na.action=na.omit)

#Bir M tahmincisi kullanarak sağlam regresyonla doğrusal bir model yerleştirin.
mdl9<-rlm(formula=csMPa~., data=beton, weights=NULL, subset=NULL, na.action=na.omit,
    method = "M",# "MM", "model.frame"
    wt.method = "inv.var", #"case"
    model = TRUE, x.ret = TRUE, y.ret = FALSE, contrasts = NULL)

#Bir dizi faktörün çoklu uygunluk analizini hesaplar.
hava<-read.csv("golf_train.csv", fileEncoding = "latin5",check.names = F,
               stringsAsFactors = T)[,-1]
mdl10<-mca(df=hava, nf = 3, abbrev = FALSE)



#predict.glmmPQL
predict(object=mdl2, newdata = beton[1:100,], type = c("link", "response"))

#predict.lda
predict(object=mdl3, newdata=iris[sample(1:150,25),-5], prior = mdl3$prior,
        dimen=3,  method = "plug-in")#, "predictive", "debiased"))

#predict.lqs
predict(object=mdl6, newdata=beton[1:100,-9], na.action = na.pass)

#predict.mca : !!!ANLAMSIZ, yanıt değişkeni yok

#predict.qda
predict(object=mdl8, newdata=iris[sample(1:150,25),-5], prior = mdl8$prior,
        method = "plug-in")# "predictive", "debiased", "looCV"
```

binomial(link = "logit") gaussian(link = "identity") Gamma(link =
"inverse") inverse.gaussian(link = "1/mu\^2") poisson(link = "log")
quasi(link = "identity", variance = "constant") quasibinomial(link =
"logit") quasipoisson(link = "log")

# 70. ordinalNet

Elastik net cezalı sıralı regresyon modellerine uyar.

```{r}
library(ordinalNet)
mdl<-ordinalNet(x=as.matrix(train_adlt[,-13]),y=factor(train_adlt[,13]),alpha = 1,
           standardize = TRUE, penaltyFactors = NULL, positiveID = NULL,
           family = "cumulative", #"sratio", "cratio", "acat"
        reverse = FALSE,link = "logit", #"probit", "cloglog", "cauchit"
        customLink = NULL,parallelTerms = TRUE,nonparallelTerms = FALSE,
        parallelPenaltyFactor = 1,lambdaVals = NULL,nLambda = 20,
        lambdaMinRatio = 0.01,includeLambda0 = FALSE,alphaMin = 0.01,
        pMin = 1e-08,stopThresh = 1e-08,threshOut = 1e-08,threshIn = 1e-08,
        maxiterOut = 100,maxiterIn = 100,printIter = FALSE,printBeta = FALSE,
        warn = TRUE,keepTrainingData = TRUE)

ordinalNetCV(x=as.matrix(train_adlt[,-13]),y=factor(train_adlt[,13]))
ordinalNetTune(x=as.matrix(train_adlt[,-13]),y=factor(train_adlt[,13]))
predict(object=mdl, newx = as.matrix(test_adlt[,-13]),whichLambda = NULL,
        criteria = c("aic", "bic"),type = "class")#"response","link"
```

# 71. ordinalForest

Sıralı orman (OF) yöntemi, yüksek boyutlu ve düşük boyutlu verilerle
sıralı regresyona izin verir. Bir eğitim veri seti kullanarak bir OF
tahmin kuralı oluşturduktan sonra, yeni gözlemler için sıralı hedef
değişkenin değerlerini tahmin etmek için kullanılabilir. -----çakma
RANGER paketi----

```{r}
library('ordinalForest')
data(hearth)
mdl<-ordfor(depvar="Class", data=hearth, nsets = 1000, ntreeperdiv = 100)#,
#            ntreefinal = 5000,importance = c("rps", "accuracy"),
#  perffunction = "equal",#"probability","proportional","oneclass","custom"
#  classimp, classweights, nbest = 10, naive = FALSE, num.threads = NULL,
#  npermtrial = 500, permperdefault = FALSE, mtry = NULL, min.node.size = NULL,
#  replace = TRUE, sample.fraction = ifelse(replace, 1, 0.632),
#  always.split.variables = NULL, keep.inbag = FALSE)

preds<-predict(mdl, hearth[1:100,])

table('true'=hearth$Class[1:100], 'predicted'=preds$ypred)
perff_equal(ytest=hearth$Class[1:100], ytestpred=preds$ypred)
perff_proportional(ytest=hearth$Class[1:100], ytestpred=preds$ypred)
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="1")
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="2")
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="3")
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="4")
perff_oneclass(ytest=hearth$Class[1:100], ytestpred=preds$ypred, categ="5")
perff_custom(ytest=hearth$Class[1:100], ytestpred=preds$ypred, classweights=c(1,2,1,1,1))
```

# 72. ada

Belirli bir veri kümesinde hem üstel hem de lojistik kayıp altında ayrı,
gerçek ve hafif bir artış gerçekleştirir. Ada paketi, küçük ve orta
büyüklükteki veri kümeleri için ideal olarak uygun, basit, iyi
belgelenmiş ve geniş bir sınıflandırma rutini sağlar.

```{r}
library(ada)
mdl<-ada(x=train_adlt[,-13], y=train_adlt[,13],test.x=NULL,test.y=NULL,
    loss="logistic",type="discrete",#"exponential" / ,"real","gentle"
    iter=50, nu=0.1, bag.frac=0.5,model.coef=TRUE,bag.shift=FALSE,max.iter=20,
    delta=10^(-10),verbose=FALSE,na.action=na.rpart)

predict(object=mdl, newdata=test_adlt[,-13], type = "both",n.iter=NULL)$class
#"vector", "probs","F"
```

# 73. arm

Regresyon ve Çok Düzeyli/Hiyerarşik Modeller Kullanarak Veri Analizi

```{r}
library(arm)

#Bayesian genelleştirilmiş doğrusal modeller.
mdl<-bayesglm (formula=formula(model.frame(income~.,data=train_adlt)),
          family = gaussian, data=train_adlt, weights=NULL, subset=NULL)#,
          #na.action=na.omit, start = NULL, etastart, mustart=20, offset=NULL,
          #control = list(maxit=10),
          #model = TRUE, method = "glm.fit", x = FALSE, y = TRUE,
          #contrasts = contr.bayes.unordered(n=2, base = 1, contrasts = TRUE),
          #drop.unused.levels = TRUE, prior.mean = 0,
          #prior.scale = NULL, prior.df = 1, prior.mean.for.intercept = 0,
          #prior.scale.for.intercept = NULL, prior.df.for.intercept = 1,
          #min.prior.scale=1e-12, scaled = TRUE, keep.order=TRUE,
          #drop.baseline=TRUE, maxit=100, print.unnormalized.log.posterior=FALSE,
          #Warning=TRUE)

#Bayes Sıralı Lojistik veya Probit Regresyon
ad<-adult #üç sınıf elde etmek için
ad$income <- cut(ad$capital_gain, breaks = c(0, 10000, 40000, 99999),
                 labels = c("<18K", "18K-40K", ">40K"),include.lowest = T)
mdl1<-bayespolr(formula=formula(model.frame(income~.,data=ad)),
          data=ad, weights = NULL, start=0, subset=NULL, na.action=na.omit,
          contrasts = NULL, Hess = TRUE, model = TRUE,
          method = "logistic", #"probit", "cloglog", "cauchit"
          drop.unused.levels=TRUE, prior.mean = 0, prior.scale = 2.5,
          prior.df = 1, prior.counts.for.bins = NULL, min.prior.scale=1e-12,
          scaled = TRUE, maxit = 100, print.unnormalized.log.posterior = FALSE)

p1<-predict(mdl, test_adlt[,-13],"link")
p2<-predict(mdl1, ad[1:1000,-13], "class")
table(ifelse(p1<1.5,1,2), test_adlt$income)
table(p2, ad[1:1000,13])
```

# 74. binda

"Binda" paketi, karşılık gelen değişkenler için ikili tahminleyicileri
kullanarak çok sınıflı diskriminant analizine yönelik işlevleri uygular.

```{r}
library(binda)

life1<-apply(life[,-1],2,function(x)scales::rescale(x)) #scale to 0-1
L<-factor(c("asia", "europe", "africa", "africa", "africa", "america", "asia", "oceania", "europe", "asia", "america", "asia", "asia", "america", "europe", "europe", "america", "africa", "asia", "america", "europe", "africa", "america", "asia", "europe", "africa", "africa", "asia", "africa", "america", "africa", "africa", "africa", "america", "asia", "asia", "africa", "africa", "africa", "america", "america", "europe", "asia", "europe", "europe", "america", "america", "africa", "america", "america", "africa", "europe", "oceania", "europe", "europe", "africa", "africa", "asia", "europe", "africa", "europe", "america", "america", "america", "africa", "africa", "america", "america", "europe", "europe", "asia", "asia", "asia", "asia", "europe", "asia", "europe", "america", "asia", "asia", "asia", "africa", "oceania", "asia", "asia", "asia", "europe", "asia", "africa", "africa", "africa", "europe", "europe", "europe", "africa", "africa", "asia", "asia", "africa", "europe", "africa", "africa", "oceania", "europe", "asia", "europe", "africa", "africa", "asia", "africa", "asia", "europe", "oceania", "africa", "africa", "europe", "asia", "asia", "america", "america", "america", "asia", "europe", "europe", "asia", "europe", "europe", "africa", "oceania", "asia", "africa", "europe", "africa", "africa", "asia", "asia", "europe", "oceania", "africa", "asia", "europe", "europe", "america", "america", "europe", "europe", "asia", "africa", "asia", "asia", "africa", "oceania", "africa", "asia", "asia", "africa", "europe", "europe", "europe", "europe", "america", "asia", "oceania", "america", "asia", "asia", "africa"))
thr = optimizeThreshold(X=life1, L)# find optimal thresholds (one for each variable)
Xb = dichotomize(X=life1, thr) #convert into binary matrix, if value is lower than threshold -> 0 otherwise -> 1
train<-Xb[1:140,];test<-Xb[141:167,]

##chances(Xb, L)#Bernoulli parametrelerini tahmin eder.
##binda.ranking(Xb, L)#İkili Diskriminant Analizi: Değişken Sıralaması

mdl<-binda(Xtrain=train, L[1:140], lambda.freqs=0, verbose=TRUE)

table(predict(object=mdl, Xtest=test, verbose=TRUE)$class, L[141:167])

```

# 75.brnn

iki katmanlı bir sinir ağına uyar. Başlangıç ağırlıklarını atamak için
Nguyen ve Widrow algoritmasını (1990) ve optimizasyonu gerçekleştirmek
için Gauss-Newton algoritmasını kullanır

```{r}
library(brnn)
#iki katmanlı bir sinir ağına uyar. Başlangıç ağırlıklarını atamak için Nguyen ve Widrow algoritmasını (1990) ve optimizasyonu gerçekleştirmek için Gauss-Newton algoritmasını kullanır
mdl1<-brnn(x=as.matrix(train_adlt[,-13]),y=train_adlt[,13],neurons=2,normalize=TRUE,
     epochs=1000, mu=0.005,mu_dec=0.1,mu_inc=10,mu_max=1e10,min_grad=1e-10,
     change = 0.001, cores=1,verbose=FALSE,Monte_Carlo = FALSE,tol = 1e-06,
     samples = 40)

#Brnn_extulated işlevi, iki katmanlı bir sinir ağına uyar. Başlangıç ağırlıklarını atamak için Nguyen ve Widrow algoritmasını (1990) ve optimizasyonu gerçekleştirmek için Gauss-Newton algoritmasını kullanır. Gizli katman, iki grup giriş değişkeni için farklı ön dağılımlar atamamıza izin veren iki grup nöron içerir.
#GELİR ve HARCAMA GRUP YAPALIM, KALANI BAŞKA GRUP
X=train_adlt[,-c(9,10,13)];Z=train_adlt[,c(9,10)];Y=train_adlt[,13]

mdl2<-brnn_extended(x=as.matrix(X),y=Y,z=as.matrix(Z),neurons1=2,neurons2=2,
              normalize=TRUE,epochs=1000,mu=0.005,mu_dec=0.1,mu_inc=10,
              mu_max=1e10,min_grad=1e-10,change = 0.001, cores=1,verbose =FALSE)

# sıralı veri oluştur
ad<-adlt #dört sınıf elde etmek için
ad$income <- cut(ad$capital_gain, breaks = c(0, 100,10000, 40000, 99999),
                 labels = c(1, 2, 3, 4),include.lowest = T)

#Brnn_ordinal işlevi, Sıralı veriler için Bayesian Düzenlileştirilmiş Sinir Ağına uyar
mdl3<-brnn_ordinal(x=as.matrix(ad[,-13]),y=as.numeric(ad[,13]),neurons=2,
             normalize=TRUE,epochs=1000,mu=0.005,mu_dec=0.1,mu_inc=10,
             mu_max=1e10,min_grad=1e-10,change_F=0.01,change_par=0.01,
             iter_EM=1000,verbose=FALSE)


p1<-predict(object=mdl1,newdata=test_adlt[,-13])
p2<-predict(object=mdl2,newdata=test_adlt[,-13])
p3<-predict(object=mdl3,newdata=ad[1:1000,-13])
```

# 76.caTools

Aşağıdakiler de dahil olmak üzere çeşitli temel yardımcı işlevler
içerir: hareketli (dönen, çalışan) pencere istatistik işlevleri, GIF ve
ENVI ikili dosyaları için okuma/yazma, AUC'nin hızlı hesaplanması,
LogitBoost sınıflandırıcı, base64 kodlayıcı/kod çözücü, yuvarlama
hatasız toplam ve toplam toplamı , vesaire

```{r}
library(caTools)

t<-sample.split(Y=adlt$income, SplitRatio = .7, group = NULL )
train<-adlt[t,]
test<-adlt[!t,]

#Zayıf öğrenenler olarak karar kütüklerini (tek düğümlü karar ağaçları) kullanarak logitboost sınıflandırma algoritmasını eğitin.
md<-LogitBoost(xlearn=train[,-13], ylearn=train[,13],
           nIter=ncol(train[,-13]))

q=predict.LogitBoost(object=md,xtest=test[,-13],type="class", nIter = 12)
table(q,test$income)
```

# 77.class

K-en yakın komşu, Öğrenme Vektörü Nicelemesi ve Kendi Kendini Düzenleyen
Haritalar dahil olmak üzere sınıflandırmaya yönelik çeşitli işlevler

```{r}
library(class)
knn(train=train_adlt[,-13], test=test_adlt[,-13], cl=train_adlt[,13], k = 1,
    l = 0, prob = FALSE, use.all = TRUE)
knn.cv(train=train_adlt[,-13], cl=train_adlt[,13], k = 1, l = 0, prob = FALSE,
       use.all = TRUE)
knn1(train=train_adlt[,-13], test=test_adlt[,-13], cl=train_adlt[,13])
```

# 78.gam

Bu paket, genelleştirilmiş eklemeli modellerin takılması ve bu
modellerle çalışmaya yönelik işlevler sağlar

```{r}
library(gam)
mdl<-gam(formula=income~.,family = gaussian,data=train_adlt)
    #weights=NULL,subset=NULL,na.action=na.omit,start = NULL,etastart,mustart,
    #control = gam.control(epsilon = 1e-07,bf.epsilon = 1e-07,maxit = 30,
    #                  bf.maxit = 30,trace = FALSE),
    #model = TRUE,method = "glm.fit",x = FALSE,y = TRUE)

predict(object=mdl,newdata=test_adlt[,-13],type = "link", #"response", "terms"
        dispersion = NULL,se.fit = FALSE,na.action = na.pass,
        terms = labels(mdl))

```

# 79.hda

Sınıfların kovaryans matrisleri eşit değilse, sınıflandırma için
boyutluluk azaltmayı gerçekleştiren işlevler

```{r}
library(hda)
#Eşit olmayan kovaryans matrislerine sahip sınıfların ayırt edilmesi için doğrusal dönüşüm yükleme matrisini hesaplar.
mdl<-hda(x=train_adlt[,-13], grouping=factor(train_adlt[,13]),
    newdim = 1:(ncol(train_adlt[,-13])-1), crule = T,reg.lamb = NULL,
    reg.gamm = NULL, initial.loadings = NULL,sig.levs = c(0.05,0.05),
    noutit = 7, ninit = 10, verbose = TRUE)

predict(object=mdl, newdata=test_adlt[,-13], alldims = FALSE, task = "c")#"dr"
```

# 80.HDclassif

Yüksek boyutlu verilerin düşük boyutlu farklı alt uzaylarda yaşadığı
varsayımına dayanan, yüksek boyutlu veriler için diskriminant analizi ve
veri kümeleme yöntemleri, boyut azaltma fikirlerini ve modeldeki
kısıtlamaları birleştiren Gauss karışım modelinin yeni bir
parametrelendirmesini önermektedir.

```{r}
library(HDclassif)

#Yüksek Boyutlu Diskriminant Analizi
md1<-hdda(data=train_adlt[,-13],cls=train_adlt[,13],model = "AkjBkQkDk",
     graph = FALSE,d_select = "Cattell",threshold = 0.2,com_dim = 2,
     show = getHDclassif.show(),scaling = FALSE,cv.dim = 1:10,
     cv.threshold = c(0.001, 0.005, 0.05, 1:9 * 0.1),cv.vfold = 10,
     LOO = FALSE,noise.ctrl = 1e-08)

#Yüksek Boyutlu Veri Kümeleme
md2<-hddc(data=train_adlt[,-13],K = 1:2,model = c("AkjBkQkDk"),threshold = 0.2,
     criterion = "bic",com_dim = 2,itermax = 10,eps = 0.001,algo = "CEM",
     d_select = "BIC",init = "kmeans",init.vector, show = getHDclassif.show(),
     mini.nb = c(5, 10),scaling = T, min.individuals =10,noise.ctrl = 1e-08,
     mc.cores = 1,nb.rep = 1, keepAllRes = TRUE,
     kmeans.control = list(iter.max=200,nstart=10,algorithm="Lloyd"),
     d_max = 100,subset = Inf)#'Hartigan-Wong', 'Lloyd', 'Forgy' or 'MacQueen'

#HD Gaussianlarla Karışım Diskriminant Analizi
md3<-hdmda(X=train_adlt[,-13],cls=train_adlt[,13],K=1:2,model='AkjBkQkDk',show=FALSE)

table(predict(md1,test_adlt[,-13])$class,test_adlt[,13])
table(predict(object=md2, data=test_adlt[,-13], cls = NULL)$class,test_adlt[,13])
table(as.numeric(predict(object=md3, X=test_adlt[,-13])$class), test_adlt[,13])
```

# 81. HiDimDA

Gözlemlerden (çok) daha fazla değişkene sahip problemler için güvenilir
kovaryans tahmin edicilerine dayalı olarak yüksek boyutlu problemlerde
doğrusal diskriminant analizi gerçekleştirir. Sınıflandırıcı eğitimi,
tahmin, çapraz doğrulama ve değişken seçimi için rutinler içerir.

```{r}
library(HiDimDA)
#Dlda: Çapraz Doğrusal Diskriminant Analizi
m1<-Dlda(data=train_adlt[,-13], grouping=factor(train_adlt[,13]),
  prior = "proportions",VSelfunct = SelectV,ldafun="classification")#"canonical"

#Slda: Küçültülmüş Doğrusal Diskriminant Analizi
m2<-Slda(data=train_adlt[,-13], grouping=factor(train_adlt[,13]),
     prior = "proportions", StddzData=TRUE,VSelfunct = SelectV,
     Trgt=c("CnstDiag","Idntty","VarDiag"),
     minp=20, ldafun="classification")#"canonical"

#Mlda: Maksimum Belirsizlik Doğrusal Diskriminant Analizi
m3<-Mlda(data=train_adlt[,-13], grouping=factor(train_adlt[,13]),
     prior = "proportions",StddzData=TRUE,VSelfunct = SelectV,
     ldafun="classification", #"canonical"
     PCAstep=FALSE)

#RFlda: Faktör-model Doğrusal Diskriminant Analizi
m4<-RFlda(data=train_adlt[,-13], grouping=factor(train_adlt[,13]), q = 1,
      prior = "proportions",CorrAp = TRUE, maxq=5, VSelfunct = SelectV,
      ldafun="classification", nstarts = 1, #"canonical"
      CVqtrials=1:3, CVqfolds=3, CVqrep=1, CVqStrt=TRUE)

#SelectV: Denetimli sınıflandırma için Yüksek Boyutlu değişken seçimi
SelectV(data=train_adlt[,-13], grouping=factor(train_adlt[,13]),
        Selmethod=c("ExpHC","HC","Fdr","Fair","fixedp"),
        NullDist=c("locfdr","Theoretical"), uselocfdr=c("onlyHC","always"),
        minlocfdrp=200, comvar=TRUE, Fdralpha=0.5,ExpHCalpha=0.5,
        HCalpha0=0.1, maxp=ncol(train_adlt), tol=1E-12)

table(predict(m1, test_adlt[,-13],grpcodes=levels(factor(train_adlt$income)))$class, test_adlt[,13])
table(predict(m2, test_adlt[,-13],grpcodes=levels(factor(train_adlt$income)))$class, test_adlt[,13])
table(predict(m3, test_adlt[,-13],grpcodes=levels(factor(train_adlt$income)))$class, test_adlt[,13])
table(predict(m4, test_adlt[,-13],grpcodes=levels(factor(train_adlt$income)))$class, test_adlt[,13])
```

# 82. kerndwd

Doğrusal mesafe ağırlıklı ayrımcılığı ve çekirdek mesafe ağırlıklı
ayrımcılığı çözen yeni bir uygulama. Sınıflandırma için çekirdek Hilbert
uzaylarının yeniden üretilmesinde doğrusal genelleştirilmiş DWD ve
çekirdek genelleştirilmiş DWD'yi çözmek için son derece yeni ve etkili
prosedürler. Algoritma, tüm çözüm yolunu belirli bir düzenlileştirme
parametreleri ızgarasında hesaplamak için çoğunluk minimizasyonu (MM)
ilkesine dayanmaktadır. ----ÇALIŞMASI UZUN SÜRÜYOR---

```{r}
library(kerndwd)

#Doğrusal genelleştirilmiş mesafe ağırlıklı ayrım (DWD) modelini ve genelleştirilmiş DWD'yi Çekirdek Hilbert uzayının çoğaltılması üzerine yerleştirin. Çözüm yolu, lambda ayarlama parametresinin değerlerinin bir tablosunda hesaplanır.
md<-kerndwd(x=scale(train_adlt[,-13]), y=ifelse(train_adlt$income==1,-1,1),
        kern= rbfdot(sigma=.1), lambda=10^(seq(3, -3, length.out=10)),
        qval=1, wt=NULL, eps=1e-03, maxit=1e+03)

p<-predict(object=md, kern=rbfdot(sigma=.1), x=train_adlt[,-13],
        newx=test_adlt[,-13], type="class") #"link"
```

# 83. kknn

Ağırlıklı k-En Yakın Komşular Sınıflandırması, Regresyon ve Spektral
Kümeleme

```{r}
library(kknn)
#fit and predict
fp<-kknn(formula = income~., train=train_adlt, test=test_adlt[,-13],
     na.action = na.omit(),k = 7, distance = 2, kernel = "optimal",
     ykernel = NULL, scale=TRUE,
     contrasts = c('unordered' = "contr.dummy", ordered = "contr.ordinal"))

#fitting
f<-train.kknn(formula = income~., data = train_adlt, kmax = 11, ks = NULL, distance = 2,
           kernel = "optimal", ykernel = NULL, scale = TRUE,
           contrasts = c('unordered' = "contr.dummy",ordered = "contr.ordinal"))

#cv.kknn(formula = income~., data = train_adlt, kcv = 10)

table(ifelse(fp$fitted.values<1.5,1,2), test_adlt$income)
```

# 84. kohonen

Kendi kendini düzenleyen haritaları (SOM'lar) eğitme işlevleri. Ayrıca
haritaların sorgulanması ve tahmin eğitilmiş haritaların kullanılması
desteklenmektedir. Kohonen paketi kendi kendini düzenleyen haritaların
(SOM'lar) çeşitli biçimlerini uygular.

```{r}
library(kohonen)
#prepare data
Xtraining <- scale(train_adlt[,-13])
Xtest <- scale(test_adlt[,-13],
               center = attr(Xtraining, "scaled:center"),
               scale = attr(Xtraining, "scaled:scale"))
trainingdata <- list(gozlemler = Xtraining, yanit = factor(train_adlt$income))
testdata <- list(gozlemler = Xtest, yanit1 = factor(test_adlt$income))

#Bir süpersom, kendi kendini düzenleyen haritaların (SOM'ler), muhtemelen farklı
#sayılara ve farklı değişken türlerine (eşit sayıda nesneye rağmen) sahip birden
#fazla veri katmanına genişletilmesidir. NA'lara izin verilir. Eğitim sırasında
#kazanan birimleri belirlemek için tüm katmanlar üzerindeki ağırlıklı mesafe
#hesaplanır.
mdl<-supersom(data=trainingdata, grid=somgrid(5, 5, "hexagonal"), rlen = 100,
         alpha = c(0.05, 0.01),radius = quantile(0, 2/3),
         whatmap = NULL, user.weights = 1, maxNA.fraction = 0L,
         keep.data = TRUE, dist.fcts = NULL,mode = "online",#"batch", "pbatch"
         cores = -1, normalizeDataLayers = TRUE)#remove init

#Som ve xyf işlevleri, sırasıyla bir ve iki katmanlı süpersomlar için basit #sarmalayıcılardır.

prd<-predict(object=mdl,newdata = testdata, unit.predictions = NULL,
             trainingdata = NULL, whatmap = "gozlemler", threshold = 0,
             maxNA.fraction = "gozlemler")

table(grc = factor(test_adlt$income), thm=prd$predictions[["yanit"]])
```

# 85. KRLS

Paket, doğrusallık veya toplanabilirlik varsayımlarına dayanmadan
regresyon ve sınıflandırma sorunları için çok boyutlu işlevler y=f(x)'e
uyacak bir makine öğrenme yöntemi olan Çekirdek Tabanlı
Düzenlileştirilmiş En Küçük Kareler'i (KRLS) uygular. KRLS, Gauss
çekirdeklerini radyal temel fonksiyonlar olarak kullanarak Tikhonov
düzenlileştirme probleminin kare kaybını en aza indirerek en uygun
fonksiyonu bulur. --ÇALIŞMASI UZUN SÜRÜYOR--

```{r}
library(KRLS)
krls(X = train_adlt[,-13], y = train_adlt[,13], whichkernel = "gaussian",
     lambda = NULL,sigma = NULL, derivative = TRUE, binary= TRUE, vcov=TRUE,
     print.level = 1,L=NULL,U=NULL,tol=NULL,eigtrunc=NULL)


predict(object, newdata, se.fit = FALSE)
```

# 86. LogicReg

lojistik regresyon tahminleri --- tüm gözlem değerlerinin 0-1 olması
gerekiyor, yanıt değişekni sürekli olabilir

```{r}
library(LogicReg)
data("logreg.testdat")
logreg(resp = logreg.testdat[,1], bin=logreg.testdat[, 2:21], #sep=NULL,
       type=2, select=1, ntrees=2, nleaves=1)# wgt=NULL, cens=NULL,
       #penalty=2, seed=1, kfold=10, nrep=10, oldfit=NULL,
       #anneal.control=logreg.anneal.control(start=-1, end=-4, iter=500, earlyout=0,
      #                                      update=100),
       #tree.control=logreg.tree.control(treesize=8, opers=1, minmass=0, n1=500),
       #mc.control=logreg.mc.control(nburn=1000, niter=25000, hyperpars=0,
      #                              update=0,output=4))

#predict(object, msz, ntr, newbin, newsep)
```

type of model to be fit: (1) classification, (2) regression, (3)
logistic regression, (4) proportional hazards model (Cox regression),
(5) exponential survival model, or (0) your own scoring function. If
type = 0, the code needs to be recompiled, uncompiled type = 0 results
in a constant score of 0, which may be useful to generate a sample from
the prior when select = 7 (Monte Carlo Logic Regression)

select; (1) fit a single model, (2) fit multiple models, (3)
cross-validation, (4) null-model permutation test, (5) conditional
permutation test, (6) a greedy stepwise algorithm, or (7) Monte Carlo
Logic Regression (using MCMC).

# 87. mgcv

Genelleştirilmiş toplamsal (karma) modeller, bunların bazı uzantıları ve
(Kısıtlı) Marjinal Olabilirlik, Genelleştirilmiş Çapraz Doğrulama ve
benzeri yoluyla çoklu yumuşatma parametresi tahminiyle veya tamamen
Bayes çıkarımı için yinelenmiş iç içe Laplace yaklaşımı kullanılarak
diğer genelleştirilmiş sırt regresyonu

```{r}
library(mgcv)
f=formula(x=csMPa~cement+ s(water)+ti(age)+te(slag)+te(flyash)+
            te(superplasticizer)+s(coarseaggregate)+s(fineaggregate))#s, te, ti and t2

c=gam.control(nthreads=1,ncv.threads=1,irls.reg=0.0,epsilon = 1e-07,
    maxit = 200,mgcv.tol=1e-7,mgcv.half=15, trace = FALSE,
    rank.tol=.Machine$double.eps^0.5,nlm=list(),
    optim=list(),newton=list(),
    idLinksBases=TRUE,scalePenalty=TRUE,efs.lspmax=15,
    efs.tol=.1,keepData=FALSE,scale.est="fletcher",
    edge.correct=FALSE)


#Çok büyük veri kümeleri için genelleştirilmiş toplamsal modeller
m1<-bam(formula=f,family=gaussian(),data=beton,weights=NULL,subset=NULL,
    na.action=na.omit, offset=NULL,method="fREML",control=c,
    select=FALSE,scale=0,gamma=1,knots=NULL,sp=NULL,min.sp=NULL,
    paraPen=NULL,chunk.size=10000,rho=0,AR.start=NULL,discrete=FALSE,
    cluster=NULL,nthreads=1,gc.level=0,use.chol=FALSE,samfrac=1,
    coef=NULL,drop.unused.levels=TRUE,G=NULL,fit=TRUE,drop.intercept=NULL,
    in.out=NULL)

#Entegre düzgünlük tahminine sahip genelleştirilmiş toplamsal modeller
m2<-gam(formula=f,family=gaussian(),data=beton,weights=NULL,subset=NULL,
    na.action=na.omit,offset=NULL,method="GCV.Cp",
    optimizer=c("outer","newton"),control=c,scale=0,
    select=FALSE,knots=NULL,sp=NULL,min.sp=NULL,H=NULL,gamma=1,
    fit=TRUE,paraPen=NULL,G=NULL,in.out,drop.unused.levels=TRUE,
    drop.intercept=NULL,nei=NULL,discrete=FALSE)


#Genelleştirilmiş Katkı Karma Modelleri
m3<-gamm(formula=f,random=NULL,correlation=NULL,family=gaussian(),
    data=beton,weights=NULL,subset=NULL,na.action=na.omit,knots=NULL,
    control=list(niterEM=0,optimMethod="L-BFGS-B",returnObject=TRUE),
    niterPQL=20,verbosePQL=TRUE,method="ML",drop.unused.levels=TRUE,
    mustart=NULL, etastart=NULL)

#Başka Bir Gibbs Katkı Modelleyicisi: mgcv için JAGS desteği.
#jagam(formula=f,family=gaussian,data=beton,file="dnm_jagam.txt",weights=NULL,
#      na.action=na.omit,offset=NULL,knots=NULL,sp=NULL,drop.unused.levels=TRUE,
#      control=gam.control(),centred=TRUE,sp.prior = "gamma",diagonalize=FALSE)


bam.prd<-predict(object=m1,newdata=beton[1:100,-9],type="link",se.fit=FALSE,
    terms=NULL, exclude=NULL,block.size=50000,newdata.guaranteed=FALSE,
    na.action=na.pass,cluster=NULL,discrete=TRUE,n.threads=1,gc.level=0)

gam.prd<-predict(object=m2,newdata=beton[1:100,-9],type="link",se.fit=FALSE,
    terms=NULL, exclude=NULL,block.size=NULL,newdata.guaranteed=FALSE,
    na.action=na.pass,unconditional=FALSE,iterms.type=NULL)

Metrics::mae(beton$csMPa[1:100], bam.prd)
Metrics::mse(beton$csMPa[1:100], gam.prd)
```

FAMILY:
Tweedie,negbin,betar,cnorm,nb,ocat,scat,tw,ziP,gfam,cox.ph,gammals,gaulss,
gevlss,gumbls,multinom,mvn,shash,twlss,ziplss

METHOD:
"GCV.Cp","GACV.Cp","NCV","QNCV","REML","P-REML","ML","P-ML","REML","ML","NCV","QNCV"

OPTIMIZER: "outer","newton","bfgs","optim","nlm","efs"

FORMULA: s:smooth, te:tensor smooth, ti:tensor product interaction,
t2:alternative tensor product smooth

BETON VERİSİNDEKİ DEĞİŞKENLERİN YANIT DEĞİŞKENİ İLE ETKİLEŞİMİ

```{r}
par(mar=c(2,2,2,2))
for(i in 1:8){plot(x=beton[,i], y=beton$csMPa,main = names(beton)[i])}
```

# 88. msaenet

Yüksek boyutlu regresyonlarda özellik seçimi için çok adımlı
uyarlanabilir elastik ağ (MSAENet) algoritması, çok adımlı uyarlanabilir
MCP-Net (MSAMNet) ve çok adımlı uyarlanabilir SCAD-Net ( MSASNet)
yöntemler. ---GLMNET ALT KOPYASI, HATA VERİYOR---

```{r}
library(msaenet)
#Adaptive Elastic-Net
aenet(x=beton[,-9], y=beton[,9],
      family = "gaussian", #"binomial", "poisson","cox",
init = "enet",# "ridge"),
alphas = seq(0.05, 0.95, 0.05),
tune = "cv",# "ebic", "bic", "aic"),
nfolds = 5L, rule = "lambda.min",# "lambda.1se"),
ebic.gamma = 1, scale = 1,
lower.limits = -Inf, upper.limits = Inf,
penalty.factor.init = rep(1, 8),
seed = 1001,
parallel = FALSE, verbose = FALSE)

#Adaptive MCP-Net
amnet(x=train_adlt[,-13], y=train_adlt[,13], family = "binomial",
init = c("mnet", "ridge"), gammas = 3, alphas = seq(0.05, 0.95,
0.05), tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
ebic.gamma = 1, scale = 1, eps = 1e-04, max.iter = 10000L,
penalty.factor.init = rep(1, 12), seed = 1001,
parallel = FALSE, verbose = FALSE)

#Adaptive SCAD-Net
asnet(x=train_adlt[,-13], y=train_adlt[,13], family =  "binomial",
init = c("snet", "ridge"), gammas = 3.7, alphas = seq(0.05, 0.95,
0.05), tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
ebic.gamma = 1, scale = 1, eps = 1e-04, max.iter = 10000L,
penalty.factor.init = rep(1, 12), seed = 1001,
parallel = FALSE, verbose = FALSE)

#Multi-Step Adaptive Elastic-Net
msaenet(x=train_adlt[,-13], y=train_adlt[,13], family = "binomial",
init = c("enet", "ridge"), alphas = seq(0.05, 0.95, 0.05),
tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
rule = c("lambda.min", "lambda.1se"), ebic.gamma = 1, nsteps = 2L,
tune.nsteps = c("max", "ebic", "bic", "aic"), ebic.gamma.nsteps = 1,
scale = 1, lower.limits = -Inf, upper.limits = Inf,
penalty.factor.init = rep(1, 12), seed = 1001,
parallel = FALSE, verbose = FALSE)

#Multi-Step Adaptive MCP-Net
msamnet(x=train_adlt[,-13], y=train_adlt[,13], family = "binomial",
init = c("mnet", "ridge"), gammas = 3, alphas = seq(0.05, 0.95,
0.05), tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
ebic.gamma = 1, nsteps = 2L, tune.nsteps = c("max", "ebic", "bic",
"aic"), ebic.gamma.nsteps = 1, scale = 1, eps = 1e-04,
max.iter = 10000L, penalty.factor.init = rep(1, 12),
seed = 1001, parallel = FALSE, verbose = FALSE)

#Multi-Step Adaptive SCAD-Net
msasnet(x=train_adlt[,-13], y=train_adlt[,13], family = "binomial",
init = c("snet", "ridge"), gammas = 3.7, alphas = seq(0.05, 0.95,
0.05), tune = c("cv", "ebic", "bic", "aic"), nfolds = 5L,
ebic.gamma = 1, nsteps = 2L, tune.nsteps = c("max", "ebic", "bic",
"aic"), ebic.gamma.nsteps = 1, scale = 1, eps = 1e-04,
max.iter = 10000L, penalty.factor.init = rep(1, 12),
seed = 1001, parallel = FALSE, verbose = FALSE)

predict(object, newx, ...)

msaenet.mae(yreal, ypred)
msaenet.mse(yreal, ypred)
msaenet.rmse(yreal, ypred)

```

# 89. partDSA

Tüm ortak değişken alanı üzerinde yoğun ve kapsamlı bir araştırmaya
dayalı, giderek daha karmaşık hale gelen tahmin edicilerin parça parça
sabit bir tahmin listesini oluşturmak için yeni bir araç.

```{r}
library(partDSA)
mdl<-partDSA(x=train_adlt[,-13], y=train_adlt[,13], wt=rep(1, 22792),
        x.test=train_adlt[,-13], y.test=train_adlt[,13],
        wt.test=rep(1, 22792), sleigh=1,
        control=DSA.control(vfold=10, minsplit = 20, minbuck=round(20/3),
        cut.off.growth=10, MPD=0.1, missing="impute.at.split",
        loss.function= "default", wt.method="KM", brier.vec=NULL,
        leafy=0, leafy.random.num.variables.per.split=4,
        leafy.num.trees=50, leafy.subsample=0, save.input=FALSE,
        boost=0, boost.rounds=100, cox.vec=NULL,IBS.wt=NULL, partial=NULL))

p<-predict(object=mdl, newdata1=test_adlt[,-13])
p1<-apply(p,1,mean)
table(prd=ifelse(p1<1.5,1,2),test_adlt$income)
```

# 90. penalized

desteklenen regresyon modelleri doğrusal, lojistik ve Poisson regresyonu
ve Cox Orantılı Tehlike modelidir

```{r}
library(penalized)

lung<-survival::lung#data(cancer, package = 'survival')
lung<-na.omit(lung)
#hayatta kalma nesnesi oluşturun. status değişkeni 0-1 olmalı
S<-Surv(time=lung$time, event=lung$status-1, #time2,
     type=c('right'),origin=0)#'left','interval','counting','interval2','mstate'

# Olabilirlik çapraz doğrulamasını kullanarak, L1 (kement veya kaynaşmış kement) ve/veya L2 (sırt) cezalarıyla genelleştirilmiş doğrusal modellerin çapraz doğrulanması

cv1<-cvl(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
    lambda1 = 0, lambda2= 0, positive = FALSE,fusedl = FALSE, data=lung,
    model = c("cox", "logistic", "linear", "poisson"),
    startbeta=0, startgamma=0, fold=8, epsilon = 1e-10, maxiter=25,
    standardize = FALSE, trace = TRUE, approximate = FALSE)

cv2<-optL1(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
      minlambda1=0, maxlambda1=3, base1=0, lambda2 = 0,
      fusedl = FALSE, positive = FALSE, data=lung,
      model = c("cox", "logistic", "linear", "poisson"),
      startbeta=0, startgamma=0, fold=8,epsilon = 1e-10, maxiter = Inf,
      standardize = FALSE, tol = .Machine$double.eps^0.25,trace = TRUE)

cv3<-optL2(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
      lambda1 = 0, minlambda2=0, maxlambda2=3, base2=0,fusedl = FALSE ,
      positive = FALSE, data=lung,
      model = c("cox", "logistic", "linear", "poisson"), startbeta=0,
      startgamma=0,fold=8, epsilon = 1e-10, maxiter=25, standardize = FALSE,
      tol = .Machine$double.eps^0.25,trace = TRUE, approximate = FALSE)

cv4<-profL1(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
       minlambda1=0, maxlambda1=0, base1=0, lambda2 = 0,fusedl = FALSE,
       positive = FALSE, data=lung,
       model = c("cox", "logistic", "linear", "poisson"), startbeta=0,
       startgamma=0, fold=8,epsilon = 1e-10, maxiter = Inf, standardize = FALSE,
       steps = 100, minsteps = 100/3,log = FALSE, save.predictions = FALSE,
       trace = TRUE, plot = FALSE)

cv5<-profL2(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=lung$inst,
       lambda1 = 0, minlambda2=5, maxlambda2=5, base2=0,fusedl = FALSE,
       positive = FALSE, data=lung,
       model = c("cox", "logistic", "linear", "poisson"), startbeta=0,
       startgamma=0, fold=8,epsilon = 1e-10, maxiter=25, standardize = FALSE,
       steps = 100, minsteps = 100/2,log = TRUE, save.predictions = FALSE,
       trace = TRUE, plot = FALSE, approximate = FALSE)

# Genelleştirilmiş doğrusal modellerin L1 (kement ve kaynaşmış kement) ve/veya L2 (sırt) cezaları veya ikisinin bir kombinasyonu ile takılması.
mdl<-penalized(response=S, penalized=lung$age+lung$sex+lung$ph.ecog+lung$ph.karno+
          lung$pat.karno+lung$meal.cal+lung$wt.loss, unpenalized=~inst,
           lambda1=0, lambda2=0, positive = FALSE, data=lung, fusedl=FALSE,
           model = c("cox", "logistic", "linear", "poisson"),
           startbeta=0, startgamma=0, steps =1, epsilon = 1e-10,maxiter=25,
           standardize = FALSE, trace = TRUE)

p<-predict(object=mdl, penalized=lung$age[1:50]+lung$sex[1:50]+lung$ph.ecog[1:50]+
          lung$ph.karno[1:50]+lung$pat.karno[1:50]+lung$meal.cal[1:50]+
          lung$wt.loss[1:50], unpenalized=~inst[1:50], data=lung[1:50,])
```

# 91. pls

Çok değişkenli regresyon yöntemleri, Kısmi En Küçük Kareler Regresyon
(PLSR), Temel Bileşen Regresyon (PCR) ve Kanonik Güçlendirilmiş Kısmi En
Küçük Kareler (CPPLS). NOT***PLS is a supervised procedure whereas PCA
is unsupervised.***

```{r}
library(pls)

#kısmi en küçük kareler regresyonu (PLSR), kanonik destekli kısmi en küçük kareler (CPPLS) veya temel bileşen regresyonu (PCR) gerçekleştirme işlevleri
plsr_mdl<-mvr(formula = beton$csMPa ~ msc(as.matrix(beton[, c(1:8)])),
    ncomp = 8, data = beton, validation = "CV", #"LOO""none"
    method = pls.options()$plsralg)
    #Y.add=NULL, subset=NULL, na.action=NULL,scale = FALSE, center = TRUE,
    #model = F,x = FALSE,y = FALSE)

pcr_mdl<-mvr(formula = beton$csMPa ~ msc(as.matrix(beton[, c(1:8)])),
    ncomp = 8, data = beton, validation = "CV", #"LOO""none"
    method = pls.options()$pcralg)

cppls_mdl<-mvr(formula = beton$csMPa ~ msc(as.matrix(beton[, c(1:8)])),
    ncomp = 8, data = beton, validation = "CV", #"LOO""none"
    #Y.add=rep(1,nrow(beton)), weights=NULL,
    method = pls.options()$cpplsalg)

##note
#pls.options(list(mvralg = "kernelpls", plsralg = "kernelpls",cpplsalg = "cppls",
#                 pcralg = "svdpc", parallel = NULL, w.tol = .Machine$double.eps,
#                 X.tol = 10^-12))


crossval(object=plsr_mdl,segments = 10, segment.type = "random",
         #"consecutive","interleaved"
         length.seg=1,jackknife = FALSE,trace = 15)


#---------------------
#Fits a PLS model using the CPPLS algorithm.
cppls.fit(X=as.matrix(beton[,-9]),Y=beton[,9],ncomp=8,Y.add = NULL,center = TRUE,
    stripped = FALSE,lower = 0.5, upper = 0.5,trunc.pow = FALSE,weights = NULL)

simpls.fit(X=as.matrix(beton[,-9]),Y=beton[,9],ncomp=8, center = TRUE,
           stripped = FALSE)
svdpc.fit(X=as.matrix(beton[,-9]),Y=beton[,9],ncomp=8, center = TRUE,
          stripped = FALSE)

#Fits a PLSR model with the kernel algorithm.
kernelpls.fit(X=as.matrix(beton[,-9]),Y=beton[,9], ncomp=8, center = TRUE,
              stripped = FALSE)
#------------------

#Prediction for mvr (PCR, PLSR) models.
predict(object=plsr_mdl, newdata=beton[1:25,-9], ncomp = 1:plsr_mdl$ncomp,
        comps=8, type = "response",na.action = na.pass)# "scores"
predict(object=pcr_mdl, newdata=beton[1:25,-9], ncomp = 1:pcr_mdl$ncomp,
        comps=8, type = "response",na.action = na.pass)# "scores"
predict(object=cppls_mdl, newdata=beton[1:25,-9], ncomp = 1:cppls_mdl$ncomp,
        comps=8, type = "response",na.action = na.pass)# "scores"
```

# 92. plsRglm

Genelleştirilmiş doğrusal modeller için (ağırlıklandırılmış) Kısmi En
Küçük Kareler Regresyonunu ve çeşitli kriterler kullanılarak bu tür
modellerin tekrarlanan k-katlı çapraz doğrulamasını sağlar. Açıklayıcı
değişkenlerdeki eksik verilere izin verir. Bootstrap güven aralıkları
yapıları da mevcuttur.

```{r}
library(plsRglm)
#install.package("plsdof")
#Bu işlev, tam veya eksik veri kümeleri için bir tanesini dışarıda bırakan çapraz doğrulamayla Kısmi En Küçük Kareler Regresyon modellerini uygular
m1<-plsR(csMPa~., beton,
         nt = 2, limQ2set = 0.0975, dataPredictY=beton[,-9], modele = "pls",
         family = NULL, typeVC = "none", EstimXNA = FALSE, scaleX = TRUE,
         scaleY = NULL, pvals.expli = FALSE, alpha.pvals.expli = 0.05,
         MClassed = FALSE, tol_Xi = 10^(-12), weights=NULL, subset=NULL,
         contrasts = NULL, sparse = FALSE, sparseStop = TRUE, naive = FALSE,
         verbose=TRUE)

#Bu işlev Kısmi En Küçük Kareler Regresyon genelleştirilmiş doğrusal modellerini tam veya eksik veri kümelerini uygular.
m2<-plsRglm(csMPa~., beton, nt=2, limQ2set=.0975, dataPredictY=beton[,-9],
            modele="pls-glm-gaussian", family=NULL, typeVC="none",
            EstimXNA=FALSE, scaleX=F, scaleY=NULL, pvals.expli=FALSE,
            alpha.pvals.expli=.05, MClassed=FALSE, tol_Xi=10^(-12),weights=NULL,
            sparse=FALSE, sparseStop=TRUE, naive=FALSE, verbose=TRUE)

p1<-predict(object=m1, newdata=beton[1:25,-9], comps = m1$computed_nt,
type = c("response", "scores"),
weights,methodNA = "adaptative",verbose = TRUE)

p2<-predict(object=m2, newdata=beton[1:25,-9], comps = m2$computed_nt,
type = "response", #"link", "response", "terms", "scores", "class", "probs"
se.fit = FALSE,weights,dispersion = NULL,methodNA = "adaptative",verbose = TRUE)

Metrics::mae(beton[1:25,9], p1)
Metrics::mse(beton[1:25,9], p1)

Metrics::mae(beton[1:25,9], p2)
Metrics::mse(beton[1:25,9], p2)
```

# 93. randomGLM

Genelleştirilmiş doğrusal modellere (GLM'ler) dayalı bir torbalama
tahmincisi uygulandı

```{r}
library(randomGLM)
x=train_adlt[,-13]; y=train_adlt[,13];xtest = NULL;type = "binary"
replace = TRUE;nBags = 100
mdl<-randomGLM(
# Input data
  x=x, y=y, xtest = NULL,
  weights = NULL,
# Which columns in x are categorical?
  categoricalColumns = NULL,
  maxCategoricalLevels = 2,
# Include interactions?
  maxInteractionOrder = 1,
  includeSelfinteractions = TRUE,
# Prediction type: type can be used to set
# the prediction type in a simplified way...
  type = type,#c("auto", "linear", "binary", "count", "general", "survival"),
# classify is retained mostly for backwards compatibility
  classify = switch(type,
    auto = !is.Surv(y) & (is.factor(y) | length(unique(y)) < 4),
  linear = FALSE,
  binary = TRUE ,
  count = FALSE,
  general = FALSE,
  survival = FALSE),
# family can be used to fine-tune the underlying regression model
  family = switch(type,
  auto = NULL,
  linear = gaussian(link="identity"),
  binary = binomial(link=logit),
  count = poisson(link = "log"),
  general = NULL,
  survival = NULL),
# Multi-level classification options - only apply to classification
# with multi-level response
  multiClass.global = TRUE,
  multiClass.pairwise = FALSE,
  multiClass.minObs = 1,
  multiClass.ignoreLevels = NULL,
# Sampling options
  nBags = 100,
  replace = TRUE,
  sampleBaggingWeights = NULL,
  nObsInBag = if (replace) nrow(x) else as.integer(0.632 * nrow(x)),
  nFeaturesInBag = ceiling(ifelse(ncol(x)<=10, ncol(x),
    ifelse(ncol(x)<=300, (1.0276-0.00276*ncol(x))*ncol(x), ncol(x)/5))),
  minInBagObs = min( max( nrow(x)/2, 5), 2*nrow(x)/3),
  maxBagAttempts = 100*nBags,
  replaceBadBagFeatures = TRUE,
# Individual ensemble member predictor options
  nCandidateCovariates=12,#50,
  corFncForCandidateCovariates= cor,
  corOptionsForCandidateCovariates = list(method = "pearson", use="p"),
  mandatoryCovariates = NULL,
  interactionsMandatory = FALSE,
  keepModels = is.null(xtest),
# Miscellaneous options
  thresholdClassProb = 0.5,
  interactionSeparatorForCoefNames = ".times.",
  randomSeed = 12345,
  nThreads = NULL,
  verbose =0 )



p<-predict(object=mdl, newdata=test_adlt[,-13], type="class",  #"response"
        thresholdClassProb = mdl$details$thresholdClassProb)
table(p,test_adlt[,13])
```

# 94. rFerns

Genel ve çok etiketli sınıflandırma için değiştirilmiş ve Kursa'da
tanıtıldığı gibi OOB hata yaklaşımı ve önem ölçüsünü içeren rastgele
eğrelti otları sınıflandırıcısını sağlar

```{r}
library(rFerns)
# Rastgele eğrelti otlarıyla sınıflandırma
m<-rFerns(x=train_adlt[,-13], y=factor(train_adlt[,13]), depth = 5, ferns = 1000,
       importance = "none", saveForest = TRUE, consistentSeed = NULL,
       threads = 0)

p<-predict(object=m, x=test_adlt[,-13], scores = FALSE)
table(p,test_adlt[,13])
```

# 95. robustDA

Bouveyron ve Girard, 2009'da önerilen sağlam karışım ayrıştırma analizi
(RMDA), etiket gürültüsü içeren verileri öğrenerek sağlam bir denetimli
sınıflandırıcı oluşturmaya olanak tanır. Önerilen yöntemin fikri,
tutarsızlıkları tespit etmek için öğrenme verilerinin etiketleri
tarafından taşınan denetimli bilgilerle verilerin denetimsiz
modellemesini karşılaştırmaktır. Yöntem daha sonra etiketlerde tespit
edilen tutarsızlıkları dikkate alarak sağlam bir sınıflandırıcı
oluşturabilir. MASS VE mclust çakması gibi

```{r}
library(robustDA)

m<-rmda(X=train_adlt[,c(1,11)],
     cls=train_adlt[,13], K = 3, model = "VEV")

p<-predict(object=m, X=test_adlt[,c(1,11)])
table(p$cls, test_adlt[,13])

#rmda(X=iris[,1:2], cls=as.numeric(factor(iris[,5])), K = 3, model = "VEV")
#lda(iris[,1:2],iris[,5])#MASS
#MclustDA(iris[,1:2],iris[,5])#mclust
```

# 96. mclust

Bayes düzenlemesi ve boyut azaltma dahil olmak üzere model tabanlı
kümeleme, sınıflandırma ve yoğunluk tahmini için EM algoritması
aracılığıyla tahmin edilen Gauss sonlu karışım modelleri

```{r}
library(mclust)
#Mclust for clustering;
#Parametrelendirilmiş sonlu Gauss karışım modellerine dayalı model tabanlı kümeleme. #Modeller, hiyerarşik model tabanlı toplayıcı kümeleme tarafından başlatılan EM #algoritmasıyla tahmin edilir. Daha sonra BIC'e göre en uygun model seçilir.
mdl<-Mclust(data=life[,-1], G = 1:9, modelNames = NULL, prior = NULL,
       control = emControl(), initialization = NULL,
       warn = mclust.options("warn"), x = NULL, verbose = interactive())
p<-predict(object=mdl, newdata=life[1:25,-1])


# MclustDA for supervised classification;
#Gauss sonlu karışım modellemesine dayalı diskriminant analizi.
mdl1<-MclustDA(data=train_adlt[,-13], class=train_adlt[,13], G = 1:12,
         modelNames = "VVV", modelType = c("MclustDA", "EDDA"), prior = NULL,
         control = emControl(),initialization = NULL,
         warn = mclust.options("warn"),verbose = interactive())
p1<-predict(object=mdl1, newdata=test_adlt[,-13], prop = mdl1$prop)
table(p1$classification,test_adlt[,13])

# MclustSSC for semi-supervised classification;
#Gauss sonlu karışım modellemesine dayalı yarı denetimli sınıflandırma.
mdl2<-MclustSSC(data=train_adlt[,-13], class=train_adlt[,13], G = 2,
          modelNames = "VVV", prior = NULL,
          control = emControl(), warn = mclust.options("warn"),
          verbose = interactive())
p2<-predict(object=mdl1, newdata=test_adlt[,-13])
table(p2$classification,test_adlt[,13])

# densityMclust for density estimation
#Mclust'un Gauss sonlu karışım modelini kullanarak her veri noktası için bir yoğunluk tahmini üretir.
mdl3<-densityMclust(life[,-1], plot = TRUE)
p3<-predict(object=mdl3, newdata=life[1:25,-1], what = c("dens", "cdens", "z"),
            logarithm = FALSE)

#Model tabanlı hiyerarşik kümeleme ile başlatılan EM algoritması ile uyumlu parametrelendirilmiş Gauss karışım modelleri için BIC.
mclustBIC(data=life[,-1], G = 1:6, modelNames = "EII", prior = NULL,
          control = emControl(),
          initialization = list(hcPairs = NULL,subset = NULL,noise = NULL),
          Vinv = NULL, warn = mclust.options("warn"), x = NULL,
          verbose = interactive())

#Bir Gauss karışım modelinin parametreleri için standart hataların ve yüzdelik önyükleme güven aralıklarının önyükleme veya jackknife tahmini.
MclustBootstrap(object=mdl3, nboot = 99, type = c("bs", "wlbs", "pb", "jk"),
                max.nonfit = 10*99, verbose = interactive())

#Belirli bir sonlu karışım modeli parametreleştirmesinde karışım bileşenlerinin sayısını değerlendirmek için olabilirlik oranı testini (LRT) gerçekleştirin. Gözlemlenen anlamlılığa, olabilirlik oranı testi istatistiği (LRTS) için (parametrik) önyükleme kullanılarak yaklaşık değer hesaplanır.
mclustBootstrapLRT(data=life[,-1], modelName = "EII", nboot = 999, level = 0.05,
                   maxG = NULL, verbose = interactive())
```

modelName :c("E", "V","EII", "VII", "EEI", "EVI", "VEI", "VVI")

# 97. rotationForest

İkili sınıflandırma için rotasyon ormanı modellerini yerleştirin ve
dağıtın. Döndürme ormanı, her temel sınıflandırıcının (ağaç), özellik
kümesinin rastgele bölümlerinin değişkenlerinin temel bileşenlerine
uyduğu bir topluluk yöntemidir.

```{r}
library(rotationForest)
mdl<-rotationForest(x=train_adlt[,-13], y=(train_adlt[,13]-1),
               K = round(ncol(train_adlt)/3, 0), L = 10, verbose = FALSE)

predict(object=mdl, newdata=test_adlt[,-13], all = FALSE)
```

## YARDIMCI VEYA SIKINTILI PAKETLER

# mice -yardımcı- --çok karışık, sonuç yok--

mice paketi, eksik verilerle başa çıkmak için bir yöntem uygular. Tam
Koşullu Belirtim (FCS) kullanılarak çoklu atama MICE algoritması
tarafından uygulanmıştır. Sürekli veriler (tahminli ortalama eşleştirme,
normal), ikili veriler (lojistik regresyon), sırasız kategorik veriler
(çok kategorili lojistik regresyon) ve sıralı kategorik veriler
(orantılı oranlar) için yerleşik atama modelleri sağlanır. MICE ayrıca
sürekli iki seviyeli verileri (normal model, pan, ikinci seviye
değişkenler) atayabilir. Değişkenler arasındaki tutarlılığı korumak için
pasif atama kullanılabilir.

```{r}
library('mice')

adult<-read.csv('adult.csv', sep = ',', fill = F, strip.white = T,
        col.names= c('age', 'workclass','fnlwgt', 'educatoin', 'educatoin_num',
                   'marital_status', 'occupation', 'relationship', 'race',
                   'sex', 'capital_gain', 'capital_loss', 'hours_per_week',
                   'native_country', 'income'),
                stringsAsFactors = T)

sapply(sapply(adult, grep, pattern = "\\?"),length)# nerede kaç tane ? var
# ? faktör içinde olduğu için bu şekilde değiştir
levels(adult$workclass)[levels(adult$workclass)=="?"] <- NA
levels(adult$occupation)[levels(adult$occupation)=="?"] <- NA
levels(adult$native_country)[levels(adult$native_country)=="?"] <- NA


#eksik değerleri bul
md.pattern(adult)

#algoritma sürekli, ikili, sırasız kategorik ve sıralı kategorik verilerin karışımlarını atayabilir. Ayrıca, sürekli iki seviyeli verileri atayabilir ve pasif atama yoluyla atamalar arasındaki tutarlılığı koruyabilir
imp<-mice(data = adult,m = 1,
      #method = make.method(data=adult,
      #                     where = make.where(adult),
      #                     blocks = make.blocks(adult),
      #                     defaultMethod = c("polyreg", "polyreg", "polyreg")
      #                     ),
     #predictorMatrix=make.predictorMatrix(adult), ignore = NULL,
     #where = make.where(adult),
     #blocks = make.blocks(adult), visitSequence = "monotone",
     #formulas = make.formulas(adult),
     #blots = make.blots(adult), post = 13,
     defaultMethod = c("polyreg","polyreg","polyreg"),
     maxit = 5,printFlag = TRUE,seed = NA, data.init = NULL
     )

complete(imp, "long", include = T)#tamamlananları gör
stripplot(imp, col=c("grey",mdc(2)),pch=c(1,20))#atama teşhisi


#Simülasyon amacıyla eksik verileri oluşturun
ampute(data=beton,prop = 0.5,patterns = NULL,freq = NULL,mech = "MAR",
       weights = NULL, std = TRUE,cont = TRUE,type = NULL,odds = NULL,
       bycases = TRUE,run = TRUE)


##PMM (Tahmine Dayalı Ortalama Eşleştirme) – Sayısal değişkenler için
##logreg(Lojistik Regresyon) – İkili Değişkenler için( 2 seviyeli)
##polyreg(Bayes polinom regresyon) – Faktör Değişkenleri için (>= 2 seviye)
##Proportional - Oransal oran modeli (sıralı, > = 2 seviye)

bwplot(imp)
```

# glmertree

(Genelleştirilmiş) doğrusal karma modellere dayalı özyinelemeli
bölümleme (GLMM'ler) 'lme4'ten lmer()/glmer() ve 'partykit'ten
lmtree()/glmtree()'yi birleştiriyor. ------BAŞKA PAKETLERDEN DERLEME
OLDUĞU İÇİN ÇOK PROBLEMLİ, ÇALIŞMIYOR

```{r}
library('glmertree')
#Karışık efektli beta regresyona dayalı model tabanlı özyinelemeli bölümleme.
betamertree(formula=csMPa~cement|water+slag+flyash+age, data=beton,
    family = binomial(link = "logit"), weights = NULL, cluster = NULL,
    ranefstart = NULL, offset = NULL, REML = TRUE, joint = TRUE,
    abstol = 0.001, maxit = 100, dfsplit = TRUE, verbose = FALSE,
    plot = FALSE, glmmTMB.control = glmmTMB::glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

#(Genelleştirilmiş) doğrusal karma modellere dayalı model tabanlı özyinelemeli bölümleme.1
lmertree(formula=csMPa~cement|water+slag+flyash+age, data=beton, weights = NULL,
         cluster = NULL, ranefstart = NULL, offset = NULL, joint = TRUE,
         abstol = 0.001, maxit = 100, dfsplit = TRUE, verbose = FALSE,
         plot = FALSE, REML = TRUE, lmer.control = lmerControl())

#(Genelleştirilmiş) doğrusal karma modellere dayalı model tabanlı özyinelemeli bölümleme.2
glmertree(formula=csMPa~cement|water+slag+flyash+age, data=beton,
          family = "binomial", weights = NULL, cluster = NULL,ranefstart = NULL,
          offset = NULL, joint = TRUE, abstol = 0.001, maxit = 100,
          dfsplit = TRUE, verbose = FALSE, plot = FALSE, nAGQ = 1L,
          glmer.control = glmerControl())

```

FAMILY: binomial(link = "logit") gaussian(link = "identity") Gamma(link
= "inverse") inverse.gaussian(link = "1/mu\^2") poisson(link = "log")
quasi(link = "identity", variance = "constant") quasibinomial(link =
"logit") quasipoisson(link = "log")

# mpath

Uygulamalar, sağlam (cezalandırılmış) genelleştirilmiş doğrusal
modelleri ve sağlam destek vektör makinelerini içerir. -----BİLGİSAYARI
KİLİTLİYOR ÇALIŞMIYOR-----

```{r}
library('mpath')
adlt[,14]<-adlt[,14]-1
#glmreg :fit a GLM with lasso (or elastic net), snet or mnet regularization
glmreg(formula=income~., data=data.frame(adlt), family="binomial", weights=NULL,
       offset=NULL, contrasts=NULL, x.keep=FALSE, y.keep=TRUE)

#glmregNB :fit a negative binomial model with lasso (or elastic net), snet and mnet regularization
#irglm :fit a robust generalized linear models
#irglmreg :Fit a robust penalized generalized linear models
#irsvm :fit case weighted support vector machines with robust loss functions
#loss2 :Composite Loss Value
#loss3 :Composite Loss Value for GLM
#ncl :fit a nonconvex loss based robust linear model
#zipath :Fit zero-inflated count data linear model with lasso (or elastic net),snet or mnet regularization

predict()
```

# RPMM

Beta ve Gauss Karışımları için Yinelemeli Bölümlenmiş Karışım Modeli.
Bu, hiyerarşik kümelemeye benzer ancak aynı zamanda sonlu karışım
modellerine benzer şekilde sınıfların hiyerarşisini döndüren model
tabanlı bir kümeleme algoritmasıdır. ---SORUNLU----

```{r}
library('RPMM')

#Özyinelemeli olarak bölümlenmiş karışım modelini kullanarak beta gizli sınıf modellemesi gerçekleştirir
mdl1 <- blcTree(x = scale(life[,-1]),
                initFunctions = list(blcInitializeSplitFanny()), weight = 10,
                index = 1:167, wthresh = 1e-08, nodename = "root",
                maxlevel = Inf, verbose = 2, nthresh = 5, level = 0, env = NULL,
                unsplit = NULL, splitCriterion = blcSplitCriterionBIC)

#Özyinelemeli olarak bölümlenmiş karışım modelini kullanarak Gauss gizli sınıf modellemesini gerçekleştirir
mdl2<-glcTree(x=life[,-1], initFunctions = list(glcInitializeSplitFanny(nu=1.5)),
        weight = NULL,index = NULL, wthresh = 1e-08,nodename = "root",
        maxlevel = Inf, verbose = 2, nthresh = 5, level = 0,env = NULL,
        unsplit = NULL, splitCriterion = glcSplitCriterionBIC)

predict(object=mdl1, newdata=life[1:5,-1], nodelist=NULL, type="weight")
predict(object=mdl2, newdata=life[1:5,-1], nodelist=NULL, type="weight")
```

# islasso

Model katsayıları üzerinde tahmin ve çıkarım yapılmasına olanak sağlamak
için kement düzenleme modellerine yönelik uyarılmış yumuşatma (IS)
fikrinin uygulanması (şu anda yalnızca hipotez testi). Çeşitli bağlantı
fonksiyonlarıyla doğrusal, lojistik, Poisson ve gama regresyonları
uygulanır.

----ÇALIŞMIYOR-----

```{r}
library('islasso')
z<-cv.glmnet(x=makeX(data.frame(train_adlt[,-14])),y=train_adlt[,14],
             alignment = "lambda")

mdl<-islasso(formula=income~., data = data.frame(train_adlt),
             family = gaussian(), lambda=mean(z$lambda), alpha = 1,
             weights=NULL, subset=NULL, offset=NULL, unpenalized=NULL,
             contrasts = NULL, control = is.control())

predict(object=mdl, newdata = data.frame(test_adlt[,-14]),
        type = "response",#c("link", "coefficients", "class", "terms"),
        se.fit = FALSE, ci = NULL, type.ci = "wald", level = .95,
        terms = NULL, na.action = na.pass)
```

# Rborist

Breiman tarafından tanımlandığı gibi sınıflandırma ve regresyon
ormanlarının ölçeklenebilir uygulaması ----HATA VERİYOR----

```{r}
library('Rborist')
df <- preformat(train_adlt[,-13])
target<-factor(train_adlt[,13])
# eski komut Rborist(x, y, ...)  Rborist(train_adlt[,-13],train_adlt[,13])

#rfArb:Rastgele Orman algoritmasının hızlandırılmış uygulaması. Çok çekirdekli ve GPU donanımı için ayarlandı.
mdl<-rfArb(x= df, y=target,autoCompress = 0.25,
      ctgCensus = "votes",classWeight = "balance",impPermute = 0,
      indexing = FALSE,maxLeaf = 0,minInfo = 0.01,
      minNode = 10,#if (is.factor(target)) 2 else 3,
      nLevel = 20,nSamp = 0,nThread = 0,nTree = 500,noValidate = FALSE,
      predFixed = 2,predProb = 0.0,predWeight = NULL,quantVec = NULL,
      quantiles = !is.null(NULL),#quantVec),
      regMono = NULL,rowWeight = NULL,splitQuant = NULL,
      thinLeaves = is.factor(target) && !FALSE,#indexing,
      trapUnobserved = FALSE,treeBlock = 1,verbose = FALSE,withRepl = TRUE)

p<-predict(object=mdl, newdata=test_adlt[,-13], yTest=NULL,
        keyedFrame = FALSE, quantVec=seq(0.1, 1.0, by = 0.10), quantiles = !is.null(NULL),
        ctgCensus = "votes", indexing = FALSE, trapUnobserved = FALSE,
        bagging = FALSE, nThread = 0, verbose = FALSE)
table(p$yPred, test_adlt$income)
```

# BDgraph

Sürekli, sıralı/ayrık/sayılı ve karma veriler için yönlendirilmemiş
grafik modellerde Bayes yapısının öğrenimine yönelik istatistiksel
araçlar. ---hata veriyor---

```{r}
library('BDgraph')
bdgraph( data=adlt, n = 10, method = "gcgm", #"ggm"
         algorithm = "bdmcmc", #rjmcmc
         iter = 5000, burnin = 5000 / 2, not.cont = NULL, g.prior = 0.2,
         df.prior = 3, g.start = "empty", jump = NULL, save = FALSE,
         cores = NULL, threshold = 1e-8, verbose = TRUE, nu = 1 )

# bdgraph.dw :Search algorithm for Gaussian copula graphical models for count data
# bdgraph.mpl :Search algorithm in graphical models using marginal pseudo likehlihood
# bdgraph.npn :Nonparametric transfer
# bdw.reg :Bayesian estimation of (zero-inflated) Discrete Weibull regression
```

# joinet

R paket birleşimi, yığılmış genellemeyi kullanarak çok değişkenli sırt
ve kement regresyonunu uygular. Bu çok değişkenli regresyon, ilişkili
sonuçları tahmin etmede tipik olarak tek değişkenli regresyondan daha
iyi performans gösterir. Yüksek boyutlu ortamlarda tahmine dayalı ve
yorumlanabilir modeller sağlar. ---GLMNET ALT KOPYASI, HATA VERİYOR----

```{r}
library('joinet')
#Multivariate Elastic Net Regression
joinet(Y=beton[,9],X=beton[,-9],
       family = "gaussian",nfolds = 8,foldid = NULL, type.measure = "mse",
       alpha.base = 1,alpha.meta = 1,weight = NULL, sign = NULL,)
predict(object, newx, type = "response")
```

# plsdof

PLS için model seçimi çeşitli bilgi kriterlerine (aic, bic, gmdl) veya
çapraz doğrulamaya dayanmaktadır. PLS regresyon katsayılarının
ortalaması ve kovaryansına ilişkin tahminler mevcuttur. Yaklaşık güven
aralıklarının oluşturulmasına ve test prosedürlerinin uygulanmasına izin
verirler. Ayrıca Ridge Regresyonu ve Temel Bileşenler Regresyonuna
yönelik çapraz doğrulama prosedürleri mevcut. ----TAHMİN UYGULAMASI
YOK----

```{r}
library(plsdof)
#Bu fonksiyon Kısmi En Küçük Kareler uyumunu hesaplar. Bu algoritma esas olarak gözlem sayısına göre ölçeklenir
kernel.pls.fit(X=as.matrix(beton[,-9]),y=beton[,9],m = ncol(beton[,-9]),
               compute.jacobian = FALSE, DoF.max = min(ncol(beton[,-9]) + 1,
                                                      nrow(beton[,-9]) - 1)
               )

#Bu fonksiyon Kısmi En Küçük Kareler çözümünü ve regresyon katsayılarının birinci türevini hesaplar. Bu uygulama çoğunlukla değişken sayısına göre ölçeklenir
linear.pls.fit(X=as.matrix(beton[,-9]),y=beton[,9],m = ncol(beton[,-9]),
               compute.jacobian = FALSE, DoF.max = min(ncol(beton[,-9]) + 1,
                                                      nrow(beton[,-9]) - 1)
               )

#Bu fonksiyon Temel Bileşenler Regresyon (PCR) uyumunu hesaplar.
pcr(X=as.matrix(beton[,-9]),y=beton[,9], scale = TRUE,
    m = min(ncol(beton[,-9]), nrow(beton[,-9]) - 1),
    eps = 1e-06, supervised = FALSE)

#Bu fonksiyon Kısmi En Küçük Kareler uyumunu hesaplar.
pls.model(X=as.matrix(beton[,-9]), y=beton[,9], m = ncol(beton[,-9]),
             Xtest = NULL, ytest = NULL, compute.DoF = FALSE,
             compute.jacobian = FALSE,use.kernel = FALSE,method.cor = "pearson")

#Bu işlev, çapraz doğrulamaya dayalı olarak optimal sırt regresyon modelini hesaplar.
ridge.cv(X=as.matrix(beton[,-9]),y=beton[,9], lambda = NULL, scale = TRUE,
         k = 10, plot.it = FALSE, groups = NULL, method.cor = "pearson",
         compute.jackknife = TRUE)
```

YORULUP TAMAMLAYAMADIKLARIM

-   rpartScore
-   rqPen
-   rrcov
-   rrcovHD
-   RRF
-   rrlda
-   sda
-   sdwd
-   snn
-   sparsediscrim
-   sparseLDA
-   spikeslab
-   spls
-   stepPlr
-   superpc
-   supervisedPRIM
-   vbmp
-   wsrf
